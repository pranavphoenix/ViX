{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"ViN.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-26T20:08:26.958404Z","iopub.execute_input":"2021-07-26T20:08:26.958813Z","iopub.status.idle":"2021-07-26T20:08:55.227337Z","shell.execute_reply.started":"2021-07-26T20:08:26.958724Z","shell.execute_reply":"2021-07-26T20:08:55.226437Z"},"trusted":true,"colab":{"referenced_widgets":["1ad5a8bbce4e445ebf9a0d5f1abca2f3"]},"id":"rFNPvicnONEZ","outputId":"b2bdb795-ed46-47a1-9188-b35585b2821f"},"source":["import torch, math\n","import torchvision\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from PIL import Image\n","!pip install torchsummary\n","from torchsummary import summary\n","import time\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","import torch.optim as optim\n","!pip install torchsummary\n","from torchsummary import summary\n","!pip install einops\n","from math import ceil\n","# !pip install nystrom-attention\n","# !pip install performer_pytorch\n","\n","from torch import nn, einsum\n","from einops import rearrange, repeat\n","from einops.layers.torch import Rearrange\n","\n","from einops import rearrange, reduce\n","\n","# helpers\n","from einops import reduce\n","\n","transform = transforms.Compose(\n","        [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","batch_size = 128\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","                                          shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","                                         shuffle=False, num_workers=2)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","def accuracy(output, target, topk=(1,5)):\n","    \"\"\"Computes the precision@k for the specified values of k\n","    prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n","    \"\"\"\n","    maxk = max(topk)\n","         # sizefunction: the number of total elements\n","    batch_size = target.size(0) \n"," \n","         # topk function selects the number of k before output\n","    _, pred = output.topk(maxk, 1, True, True)\n","         ##########Do not understand t()k\n","    pred = pred.t()\n","    correct = pred.eq(target.view(1, -1).expand_as(pred))   \n","    res = []\n","    for k in topk:\n","        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n","        res.append(correct_k.mul_(100.0 / batch_size))\n","    return res"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting torchsummary\n","  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n","Installing collected packages: torchsummary\n","Successfully installed torchsummary-1.5.1\n","\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","Requirement already satisfied: torchsummary in /opt/conda/lib/python3.7/site-packages (1.5.1)\n","\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","Collecting einops\n","  Downloading einops-0.3.0-py2.py3-none-any.whl (25 kB)\n","Installing collected packages: einops\n","Successfully installed einops-0.3.0\n","\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ad5a8bbce4e445ebf9a0d5f1abca2f3"}},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-26T20:08:55.230867Z","iopub.execute_input":"2021-07-26T20:08:55.231127Z","iopub.status.idle":"2021-07-26T20:08:55.255921Z","shell.execute_reply.started":"2021-07-26T20:08:55.231099Z","shell.execute_reply":"2021-07-26T20:08:55.254997Z"},"trusted":true,"id":"gHmPVXLhONEl"},"source":["from math import ceil\n","import torch\n","from torch import nn, einsum\n","import torch.nn.functional as F\n","\n","from einops import rearrange, reduce\n","\n","# helper functions\n","\n","def exists(val):\n","    return val is not None\n","\n","def moore_penrose_iter_pinv(x, iters = 6):\n","    device = x.device\n","\n","    abs_x = torch.abs(x)\n","    col = abs_x.sum(dim = -1)\n","    row = abs_x.sum(dim = -2)\n","    z = rearrange(x, '... i j -> ... j i') / (torch.max(col) * torch.max(row))\n","\n","    I = torch.eye(x.shape[-1], device = device)\n","    I = rearrange(I, 'i j -> () i j')\n","\n","    for _ in range(iters):\n","        xz = x @ z\n","        z = 0.25 * z @ (13 * I - (xz @ (15 * I - (xz @ (7 * I - xz)))))\n","\n","    return z\n","# main attention class\n","\n","class NystromAttention(nn.Module):\n","    def __init__(\n","        self,\n","        dim,\n","        dim_head = 32,\n","        heads = 4,\n","        num_landmarks = 256,\n","        pinv_iterations = 6,\n","        residual = True,\n","        residual_conv_kernel = 33,\n","        eps = 1e-8,\n","        dropout = 0.\n","    ):\n","        super().__init__()\n","        self.eps = eps\n","        inner_dim = heads * dim_head\n","\n","        self.num_landmarks = num_landmarks\n","        self.pinv_iterations = pinv_iterations\n","\n","        self.heads = heads\n","        self.scale = dim_head ** -0.5\n","        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n","\n","        self.to_out = nn.Sequential(\n","            nn.Linear(inner_dim, dim),\n","            nn.Dropout(dropout)\n","        )\n","\n","        self.residual = residual\n","        if residual:\n","            kernel_size = residual_conv_kernel\n","            padding = residual_conv_kernel // 2\n","            self.res_conv = nn.Conv2d(heads, heads, (kernel_size, 1), padding = (padding, 0), groups = heads, bias = False)\n","\n","    def forward(self, x, mask = None, return_attn = False):\n","        b, n, _, h, m, iters, eps = *x.shape, self.heads, self.num_landmarks, self.pinv_iterations, self.eps\n","\n","        # pad so that sequence can be evenly divided into m landmarks\n","\n","        remainder = n % m\n","        if remainder > 0:\n","            padding = m - (n % m)\n","            x = F.pad(x, (0, 0, padding, 0), value = 0)\n","\n","        # derive query, keys, values\n","\n","        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n","        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n","\n","        q = q * self.scale\n","\n","        # generate landmarks by sum reduction, and then calculate mean using the mask\n","\n","        l = ceil(n / m)\n","        landmark_einops_eq = '... (n l) d -> ... n d'\n","        q_landmarks = reduce(q, landmark_einops_eq, 'sum', l = l)\n","        k_landmarks = reduce(k, landmark_einops_eq, 'sum', l = l)\n","\n","        # calculate landmark mask, and also get sum of non-masked elements in preparation for masked mean\n","\n","        divisor = l\n","\n","\n","        # masked mean (if mask exists)\n","\n","        q_landmarks /= divisor\n","        k_landmarks /= divisor\n","\n","        # similarities\n","\n","        einops_eq = '... i d, ... j d -> ... i j'\n","        sim1 = einsum(einops_eq, q, k_landmarks)\n","        sim2 = einsum(einops_eq, q_landmarks, k_landmarks)\n","        sim3 = einsum(einops_eq, q_landmarks, k)\n","\n","        # masking\n","\n","        if exists(mask):\n","            mask_value = -torch.finfo(q.dtype).max\n","            sim1.masked_fill_(~(mask[..., None] * mask_landmarks[..., None, :]), mask_value)\n","            sim2.masked_fill_(~(mask_landmarks[..., None] * mask_landmarks[..., None, :]), mask_value)\n","            sim3.masked_fill_(~(mask_landmarks[..., None] * mask[..., None, :]), mask_value)\n","\n","        # eq (15) in the paper and aggregate values\n","\n","        attn1, attn2, attn3 = map(lambda t: t.softmax(dim = -1), (sim1, sim2, sim3))\n","        attn2_inv = moore_penrose_iter_pinv(attn2, iters)\n","\n","        out = (attn1 @ attn2_inv) @ (attn3 @ v)\n","\n","        # add depth-wise conv residual of values\n","\n","        if self.residual:\n","            out += self.res_conv(v)\n","\n","        # merge and combine heads\n","\n","        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n","        out = self.to_out(out)\n","        out = out[:, -n:]\n","\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-26T20:08:55.260200Z","iopub.execute_input":"2021-07-26T20:08:55.260603Z","iopub.status.idle":"2021-07-26T20:08:55.284385Z","shell.execute_reply.started":"2021-07-26T20:08:55.260572Z","shell.execute_reply":"2021-07-26T20:08:55.283371Z"},"trusted":true,"id":"4A-iQZLjONEu"},"source":["import torch\n","from torch import nn, einsum\n","import torch.nn.functional as F\n","\n","from einops import rearrange, repeat\n","from einops.layers.torch import Rearrange\n","\n","# helpers\n","\n","def pair(t):\n","    return t if isinstance(t, tuple) else (t, t)\n","\n","# classes\n","\n","class PreNorm(nn.Module):\n","    def __init__(self, dim, fn):\n","        super().__init__()\n","        self.norm = nn.LayerNorm(dim)\n","        self.fn = fn\n","    def forward(self, x, **kwargs):\n","        return self.fn(self.norm(x), **kwargs)\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, dim, hidden_dim, dropout = 0.):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_dim, dim),\n","            nn.Dropout(dropout)\n","        )\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class Transformer(nn.Module):\n","    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n","        super().__init__()\n","        self.layers = nn.ModuleList([])\n","        num_landmarks = 64\n","        pinv_iterations = 6\n","        residual = True\n","        residual_conv_kernel = 33\n","        eps = 1e-8\n","        for _ in range(depth):\n","            self.layers.append(nn.ModuleList([\n","                PreNorm(dim, NystromAttention(dim, dim_head = dim_head, num_landmarks =num_landmarks, pinv_iterations =pinv_iterations, residual = residual, residual_conv_kernel = residual_conv_kernel, eps = eps)),\n","                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n","            ]))\n","    def forward(self, x):\n","        for attn, ff in self.layers:\n","            x = attn(x) + x\n","            x = ff(x) + x\n","        return x\n","\n","class ViT(nn.Module):\n","    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n","        super().__init__()\n","        image_height, image_width = pair(image_size)\n","        patch_height, patch_width = pair(patch_size)\n","\n","        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n","\n","        num_patches = (image_height // patch_height) * (image_width // patch_width)\n","        patch_dim = channels * patch_height * patch_width\n","        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n","\n","        self.to_patch_embedding = nn.Sequential(\n","            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n","            nn.Linear(patch_dim, dim),\n","        )\n","\n","        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n","        self.dropout = nn.Dropout(emb_dropout)\n","\n","        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n","\n","        self.pool = pool\n","        self.to_latent = nn.Identity()\n","\n","        self.mlp_head = nn.Sequential(\n","            nn.LayerNorm(dim),\n","            nn.Linear(dim, num_classes)\n","        )\n","\n","    def forward(self, img):\n","        x = self.to_patch_embedding(img)\n","        b, n, _ = x.shape\n","\n","        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n","        x = torch.cat((cls_tokens, x), dim=1)\n","        x += self.pos_embedding[:, :(n + 1)]\n","        x = self.dropout(x)\n","\n","        x = self.transformer(x)\n","\n","        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n","\n","        x = self.to_latent(x)\n","        return self.mlp_head(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-26T20:08:55.285933Z","iopub.execute_input":"2021-07-26T20:08:55.286324Z","iopub.status.idle":"2021-07-26T20:09:00.325372Z","shell.execute_reply.started":"2021-07-26T20:08:55.286283Z","shell.execute_reply":"2021-07-26T20:09:00.324515Z"},"trusted":true,"id":"7AMuRodMONEw","outputId":"fe0a80d0-31b2-4fd6-c980-95e1fda1e19a"},"source":["\n","model = ViT(\n","    image_size = 32,\n","    patch_size = 1,\n","    num_classes = 10,             # number of stages\n","    dim = 128,  # dimensions at each stage\n","    depth = 4,              # transformer of depth 4 at each stage\n","    heads = 4,      # heads at each stage\n","    mlp_dim = 256,\n","    dropout = 0.,\n","    dim_head = 32\n",")\n","\n","\n","model.to(device)\n","print(summary(model, (3,32,32)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","         Rearrange-1              [-1, 1024, 3]               0\n","            Linear-2            [-1, 1024, 128]             512\n","           Dropout-3            [-1, 1025, 128]               0\n","         LayerNorm-4            [-1, 1025, 128]             256\n","            Linear-5            [-1, 1088, 384]          49,152\n","            Conv2d-6          [-1, 4, 1088, 32]             132\n","            Linear-7            [-1, 1088, 128]          16,512\n","           Dropout-8            [-1, 1088, 128]               0\n","  NystromAttention-9            [-1, 1025, 128]               0\n","          PreNorm-10            [-1, 1025, 128]               0\n","        LayerNorm-11            [-1, 1025, 128]             256\n","           Linear-12            [-1, 1025, 256]          33,024\n","             GELU-13            [-1, 1025, 256]               0\n","          Dropout-14            [-1, 1025, 256]               0\n","           Linear-15            [-1, 1025, 128]          32,896\n","          Dropout-16            [-1, 1025, 128]               0\n","      FeedForward-17            [-1, 1025, 128]               0\n","          PreNorm-18            [-1, 1025, 128]               0\n","        LayerNorm-19            [-1, 1025, 128]             256\n","           Linear-20            [-1, 1088, 384]          49,152\n","           Conv2d-21          [-1, 4, 1088, 32]             132\n","           Linear-22            [-1, 1088, 128]          16,512\n","          Dropout-23            [-1, 1088, 128]               0\n"," NystromAttention-24            [-1, 1025, 128]               0\n","          PreNorm-25            [-1, 1025, 128]               0\n","        LayerNorm-26            [-1, 1025, 128]             256\n","           Linear-27            [-1, 1025, 256]          33,024\n","             GELU-28            [-1, 1025, 256]               0\n","          Dropout-29            [-1, 1025, 256]               0\n","           Linear-30            [-1, 1025, 128]          32,896\n","          Dropout-31            [-1, 1025, 128]               0\n","      FeedForward-32            [-1, 1025, 128]               0\n","          PreNorm-33            [-1, 1025, 128]               0\n","        LayerNorm-34            [-1, 1025, 128]             256\n","           Linear-35            [-1, 1088, 384]          49,152\n","           Conv2d-36          [-1, 4, 1088, 32]             132\n","           Linear-37            [-1, 1088, 128]          16,512\n","          Dropout-38            [-1, 1088, 128]               0\n"," NystromAttention-39            [-1, 1025, 128]               0\n","          PreNorm-40            [-1, 1025, 128]               0\n","        LayerNorm-41            [-1, 1025, 128]             256\n","           Linear-42            [-1, 1025, 256]          33,024\n","             GELU-43            [-1, 1025, 256]               0\n","          Dropout-44            [-1, 1025, 256]               0\n","           Linear-45            [-1, 1025, 128]          32,896\n","          Dropout-46            [-1, 1025, 128]               0\n","      FeedForward-47            [-1, 1025, 128]               0\n","          PreNorm-48            [-1, 1025, 128]               0\n","        LayerNorm-49            [-1, 1025, 128]             256\n","           Linear-50            [-1, 1088, 384]          49,152\n","           Conv2d-51          [-1, 4, 1088, 32]             132\n","           Linear-52            [-1, 1088, 128]          16,512\n","          Dropout-53            [-1, 1088, 128]               0\n"," NystromAttention-54            [-1, 1025, 128]               0\n","          PreNorm-55            [-1, 1025, 128]               0\n","        LayerNorm-56            [-1, 1025, 128]             256\n","           Linear-57            [-1, 1025, 256]          33,024\n","             GELU-58            [-1, 1025, 256]               0\n","          Dropout-59            [-1, 1025, 256]               0\n","           Linear-60            [-1, 1025, 128]          32,896\n","          Dropout-61            [-1, 1025, 128]               0\n","      FeedForward-62            [-1, 1025, 128]               0\n","          PreNorm-63            [-1, 1025, 128]               0\n","      Transformer-64            [-1, 1025, 128]               0\n","         Identity-65                  [-1, 128]               0\n","        LayerNorm-66                  [-1, 128]             256\n","           Linear-67                   [-1, 10]           1,290\n","================================================================\n","Total params: 530,970\n","Trainable params: 530,970\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 84.58\n","Params size (MB): 2.03\n","Estimated Total Size (MB): 86.62\n","----------------------------------------------------------------\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-26T20:09:00.326953Z","iopub.execute_input":"2021-07-26T20:09:00.327306Z"},"trusted":true,"id":"hDRKZdMBONE0"},"source":["criterion = nn.CrossEntropyLoss()\n","scaler = torch.cuda.amp.GradScaler()\n","# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n","top1 = []\n","top5 = []\n","optimizer = optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n","for epoch in range(40):  # loop over the dataset multiple times\n","    t0 = time.time()\n","    epoch_accuracy = 0\n","    epoch_loss = 0\n","    running_loss = 0.0\n","\n","    for i, data in enumerate(trainloader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        with torch.cuda.amp.autocast():\n","            loss = criterion(outputs, labels)\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        acc = (outputs.argmax(dim=1) == labels).float().mean()\n","        epoch_accuracy += acc / len(trainloader)\n","        epoch_loss += loss / len(trainloader)\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        if i % 200 == 199:    # print every 2000 mini-batches\n","            print('[%d, %5d] loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / 2000))\n","            running_loss = 0.0\n","    correct = 0\n","    total = 0\n","    correct_1=0\n","    correct_5=0\n","    c = 0\n","    with torch.no_grad():\n","        for data in testloader:\n","            images, labels = data[0].to(device), data[1].to(device)\n","            outputs = model(images)\n","#         outputs = net(images)\n","\n","            _, predicted = torch.max(outputs.data, 1)\n","            res = accuracy(outputs, labels)\n","            correct_1 += res[0][0].float()\n","            correct_5 += res[1][0].float()\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","            c += 1\n","        \n","    print(f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - Top 1: {correct_1/c} - Top 5: {correct_5/c} - Time: {time.time() - t0}\\n\")\n","    top1.append(correct_1/c)\n","    top5.append(correct_5/c)\n","    if float(correct_1/c) >= float(max(top1)):\n","        PATH = 'ViP.pth'\n","        torch.save(model.state_dict(), PATH)\n","        print(1)\n","print('Finished Training')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"JHQ0R_m-ONE2"},"source":["plt.subplot(2, 1, 1)\n","plt.plot(top1[0:14], '.-')\n","plt.title('Accuracy')\n","plt.ylabel('Top 1 accuracy')\n","\n","plt.subplot(2, 1, 2)\n","plt.plot(top5[0:14], '.-')\n","plt.xlabel('epochs')\n","plt.ylabel('Top 5 accuracy')\n","\n","plt.show()\n","np.shape(top1)"],"execution_count":null,"outputs":[]}]}