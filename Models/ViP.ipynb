{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch, math\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\n!pip install torchsummary\nfrom torchsummary import summary\nimport time\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nimport torch.optim as optim\n!pip install torchsummary\nfrom torchsummary import summary\n!pip install einops\nfrom math import ceil\n# !pip install nystrom-attention\n!pip install performer_pytorch\n\nfrom torch import nn, einsum\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\n\nfrom einops import rearrange, reduce\n\n# helpers\nfrom einops import reduce\n\ntransform = transforms.Compose(\n        [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\nbatch_size = 128\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\ndef accuracy(output, target, topk=(1,5)):\n    \"\"\"Computes the precision@k for the specified values of k\n    prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n    \"\"\"\n    maxk = max(topk)\n         # sizefunction: the number of total elements\n    batch_size = target.size(0) \n \n         # topk function selects the number of k before output\n    _, pred = output.topk(maxk, 1, True, True)\n         ##########Do not understand t()k\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))   \n    res = []\n    for k in topk:\n        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-26T17:03:17.965333Z","iopub.execute_input":"2021-07-26T17:03:17.965767Z","iopub.status.idle":"2021-07-26T17:03:56.719898Z","shell.execute_reply.started":"2021-07-26T17:03:17.965677Z","shell.execute_reply":"2021-07-26T17:03:56.718847Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nRequirement already satisfied: torchsummary in /opt/conda/lib/python3.7/site-packages (1.5.1)\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nCollecting einops\n  Downloading einops-0.3.0-py2.py3-none-any.whl (25 kB)\nInstalling collected packages: einops\nSuccessfully installed einops-0.3.0\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nCollecting performer_pytorch\n  Downloading performer_pytorch-1.0.11-py3-none-any.whl (12 kB)\nRequirement already satisfied: einops>=0.3 in /opt/conda/lib/python3.7/site-packages (from performer_pytorch) (0.3.0)\nCollecting local-attention>=1.1.1\n  Downloading local_attention-1.4.3-py3-none-any.whl (5.0 kB)\nRequirement already satisfied: torch>=1.6 in /opt/conda/lib/python3.7/site-packages (from performer_pytorch) (1.7.0)\nCollecting axial-positional-embedding>=0.1.0\n  Downloading axial_positional_embedding-0.2.1.tar.gz (2.6 kB)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.6->performer_pytorch) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.6->performer_pytorch) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch>=1.6->performer_pytorch) (0.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch>=1.6->performer_pytorch) (1.19.5)\nBuilding wheels for collected packages: axial-positional-embedding\n  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-py3-none-any.whl size=2902 sha256=cd76631643e839b8f240d00677e931abfad43fe6c99b83b256ca0704a2c9387c\n  Stored in directory: /root/.cache/pip/wheels/4a/2c/c3/9a1cb267c0d0d9b6eeba7952addb32b17857d1f799690c27a8\nSuccessfully built axial-positional-embedding\nInstalling collected packages: local-attention, axial-positional-embedding, performer-pytorch\nSuccessfully installed axial-positional-embedding-0.2.1 local-attention-1.4.3 performer-pytorch-1.0.11\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24ed05a7afb349469134d124df12c9b9"}},"metadata":{}},{"name":"stdout","text":"Extracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.cuda.amp import autocast\nfrom functools import partial\nfrom contextlib import contextmanager\nfrom local_attention import LocalAttention\nfrom axial_positional_embedding import AxialPositionalEmbedding\nfrom performer_pytorch.reversible import ReversibleSequence, SequentialSequence\n\ndef exists(val):\n    return val is not None\n\ndef empty(tensor):\n    return tensor.numel() == 0\n\ndef default(val, d):\n    return val if exists(val) else d\n\n@contextmanager\ndef null_context():\n    yield\n\n# def cast_tuple(val):\n#     return (val,) if not isinstance(val, tuple) else val\n\ndef get_module_device(module):\n    return next(module.parameters()).device\n\ndef find_modules(nn_module, type):\n    return [module for module in nn_module.modules() if isinstance(module, type)]\n\nclass Always(nn.Module):\n    def __init__(self, val):\n        super().__init__()\n        self.val = val\n\n    def forward(self, *args, **kwargs):\n        return self.val\n\n# kernel functions\n\n# transcribed from jax to pytorch from\n# https://github.com/google-research/google-research/blob/master/performer/fast_attention/jax/fast_attention.py\n\ndef softmax_kernel(data, *, projection_matrix, is_query, normalize_data=True, eps=1e-4, device = None):\n    b, h, *_ = data.shape\n\n    data_normalizer = (data.shape[-1] ** -0.25) if normalize_data else 1.\n\n    ratio = (projection_matrix.shape[0] ** -0.5)\n\n    projection = repeat(projection_matrix, 'j d -> b h j d', b = b, h = h)\n    projection = projection.type_as(data)\n\n    data_dash = torch.einsum('...id,...jd->...ij', (data_normalizer * data), projection)\n\n    diag_data = data ** 2\n    diag_data = torch.sum(diag_data, dim=-1)\n    diag_data = (diag_data / 2.0) * (data_normalizer ** 2)\n    diag_data = diag_data.unsqueeze(dim=-1)\n\n    if is_query:\n        data_dash = ratio * (\n            torch.exp(data_dash - diag_data -\n                    torch.max(data_dash, dim=-1, keepdim=True).values) + eps)\n    else:\n        data_dash = ratio * (\n            torch.exp(data_dash - diag_data - torch.max(data_dash)) + eps)\n\n    return data_dash.type_as(data)\n\ndef generalized_kernel(data, *, projection_matrix, kernel_fn = nn.ReLU(), kernel_epsilon = 0.001, normalize_data = True, device = None):\n    b, h, *_ = data.shape\n\n    data_normalizer = (data.shape[-1] ** -0.25) if normalize_data else 1.\n\n    if projection_matrix is None:\n        return kernel_fn(data_normalizer * data) + kernel_epsilon\n\n    projection = repeat(projection_matrix, 'j d -> b h j d', b = b, h = h)\n    projection = projection.type_as(data)\n\n    data_dash = torch.einsum('...id,...jd->...ij', (data_normalizer * data), projection)\n\n    data_prime = kernel_fn(data_dash) + kernel_epsilon\n    return data_prime.type_as(data)\n\ndef orthogonal_matrix_chunk(cols, device = None):\n    unstructured_block = torch.randn((cols, cols), device = device)\n    q, r = torch.qr(unstructured_block.cpu(), some = True)\n    q, r = map(lambda t: t.to(device), (q, r))\n    return q.t()\n\ndef gaussian_orthogonal_random_matrix(nb_rows, nb_columns, scaling = 0, device = None):\n    nb_full_blocks = int(nb_rows / nb_columns)\n\n    block_list = []\n\n    for _ in range(nb_full_blocks):\n        q = orthogonal_matrix_chunk(nb_columns, device = device)\n        block_list.append(q)\n\n    remaining_rows = nb_rows - nb_full_blocks * nb_columns\n    if remaining_rows > 0:\n        q = orthogonal_matrix_chunk(nb_columns, device = device)\n        block_list.append(q[:remaining_rows])\n\n    final_matrix = torch.cat(block_list)\n\n    if scaling == 0:\n        multiplier = torch.randn((nb_rows, nb_columns), device = device).norm(dim = 1)\n    elif scaling == 1:\n        multiplier = math.sqrt((float(nb_columns))) * torch.ones((nb_rows,), device = device)\n    else:\n        raise ValueError(f'Invalid scaling {scaling}')\n\n    return torch.diag(multiplier) @ final_matrix\n\n# linear attention classes with softmax kernel\n\n# non-causal linear attention\ndef linear_attention(q, k, v):\n    k_cumsum = k.sum(dim = -2)\n    D_inv = 1. / torch.einsum('...nd,...d->...n', q, k_cumsum.type_as(q))\n    context = torch.einsum('...nd,...ne->...de', k, v)\n    out = torch.einsum('...de,...nd,...n->...ne', context, q, D_inv)\n#     print(\"linear attention\", out.size)\n    return out\n\nclass FastAttention(nn.Module):\n    def __init__(self, dim_heads, nb_features = None, ortho_scaling = 0, causal = False, generalized_attention = False, kernel_fn = nn.ReLU(), no_projection = False):\n        super().__init__()\n        nb_features = default(nb_features, int(dim_heads * math.log(dim_heads)))\n\n        self.dim_heads = dim_heads\n        self.nb_features = nb_features\n        self.ortho_scaling = ortho_scaling\n\n        self.create_projection = partial(gaussian_orthogonal_random_matrix, nb_rows = self.nb_features, nb_columns = dim_heads, scaling = ortho_scaling)\n        projection_matrix = self.create_projection()\n        self.register_buffer('projection_matrix', projection_matrix)\n\n        self.generalized_attention = generalized_attention\n        self.kernel_fn = kernel_fn\n\n        # if this is turned on, no projection will be used\n        # queries and keys will be softmax-ed as in the original efficient attention paper\n        self.no_projection = no_projection\n\n        self.causal = causal\n        \n    @torch.no_grad()\n    def redraw_projection_matrix(self, device):\n        projections = self.create_projection(device = device)\n        self.projection_matrix.copy_(projections)\n        del projections\n\n    def forward(self, q, k, v):\n        device = q.device\n        \n        if self.no_projection:\n            q = q.softmax(dim = -1)\n            k = torch.exp(k) if self.causal else k.softmax(dim = -2)\n\n        elif self.generalized_attention:\n            create_kernel = partial(generalized_kernel, kernel_fn = self.kernel_fn, projection_matrix = self.projection_matrix, device = device)\n            q, k = map(create_kernel, (q, k))\n\n        else:\n            create_kernel = partial(softmax_kernel, projection_matrix = self.projection_matrix, device = device)\n            q = create_kernel(q, is_query = True)\n            k = create_kernel(k, is_query = False)\n\n        attn_fn = linear_attention if not self.causal else self.causal_linear_fn\n        out = attn_fn(q, k, v)\n#         print('fastattention', out.size())\n        return out\n\n# a module for keeping track of when to update the projections\n\nclass ProjectionUpdater(nn.Module):\n    def __init__(self, instance, feature_redraw_interval):\n        super().__init__()\n        self.instance = instance\n        self.feature_redraw_interval = feature_redraw_interval\n        self.register_buffer('calls_since_last_redraw', torch.tensor(0))\n\n    def fix_projections_(self):\n        self.feature_redraw_interval = None\n\n    def redraw_projections(self):\n        model = self.instance\n\n        if not self.training:\n            return\n\n        if exists(self.feature_redraw_interval) and self.calls_since_last_redraw >= self.feature_redraw_interval:\n            device = get_module_device(model)\n\n            fast_attentions = find_modules(model, FastAttention)\n            for fast_attention in fast_attentions:\n                fast_attention.redraw_projection_matrix(device)\n\n            self.calls_since_last_redraw.zero_()\n            return\n\n        self.calls_since_last_redraw += 1\n\n    def forward(self, x):\n        raise NotImplemented\n\n# classes\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        causal = False,\n        heads = 4,\n        dim_head = 32,\n        local_heads = 0,\n        local_window_size = 256,\n        nb_features = None,\n        feature_redraw_interval = 1000,\n        generalized_attention = False,\n        kernel_fn = nn.ReLU(),\n        dropout = 0.,\n        no_projection = False,\n        qkv_bias = False,\n        attn_out_bias = True\n    ):\n        super().__init__()\n        assert dim % heads == 0, 'dimension must be divisible by number of heads'\n        dim_head = default(dim_head, dim // heads)\n        inner_dim = dim_head * heads\n        self.fast_attention = FastAttention(dim_head, nb_features, causal = causal, generalized_attention = generalized_attention, kernel_fn = kernel_fn, no_projection = no_projection)\n\n        self.heads = heads\n        self.global_heads = heads - local_heads\n        self.local_attn = LocalAttention(window_size = local_window_size, causal = causal, autopad = True, dropout = dropout, look_forward = int(not causal), rel_pos_emb_config = (dim_head, local_heads)) if local_heads > 0 else None\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = qkv_bias)\n        self.to_k = nn.Linear(dim, inner_dim, bias = qkv_bias)\n        self.to_v = nn.Linear(dim, inner_dim, bias = qkv_bias)\n        self.to_out = nn.Linear(inner_dim, dim, bias = attn_out_bias)\n        self.dropout = nn.Dropout(dropout)\n        \n\n    def forward(self, x, pos_emb = None, context = None, mask = None, context_mask = None, **kwargs):\n        \n        b, n, _, h, gh = *x.shape, self.heads, self.global_heads\n\n        cross_attend = exists(context)\n\n        context = default(context, x)\n        context_mask = default(context_mask, mask) if not cross_attend else context_mask\n\n        q, k, v = self.to_q(x), self.to_k(context), self.to_v(context)\n\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n        (q, lq), (k, lk), (v, lv) = map(lambda t: (t[:, :gh], t[:, gh:]), (q, k, v))\n\n        attn_outs = []\n\n        if not empty(q):\n            if exists(context_mask):\n                global_mask = context_mask[:, None, :, None]\n                v.masked_fill_(~global_mask, 0.)\n\n            if exists(pos_emb) and not cross_attend:\n                q, k = apply_rotary_pos_emb(q, k, pos_emb)\n\n            out = self.fast_attention(q, k, v)\n            attn_outs.append(out)\n\n        if not empty(lq):\n            assert not cross_attend, 'local attention is not compatible with cross attention'\n            out = self.local_attn(lq, lk, lv, input_mask = mask)\n            attn_outs.append(out)\n\n        out = torch.cat(attn_outs, dim = 1)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n#         print(\"Attention\", out.size())\n        out =  self.to_out(out)\n        out = self.dropout(out)\n        return out\n\n\nclass SelfAttention(Attention):\n    def forward(self, *args, context = None, **kwargs):\n        assert not exists(context), 'self attention should not receive context'\n#         print(1, \"self attention module\")\n        return super().forward(*args, **kwargs)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T17:03:56.721996Z","iopub.execute_input":"2021-07-26T17:03:56.722422Z","iopub.status.idle":"2021-07-26T17:03:56.779704Z","shell.execute_reply.started":"2021-07-26T17:03:56.722378Z","shell.execute_reply":"2021-07-26T17:03:56.778798Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn, einsum\nimport torch.nn.functional as F\n\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\n\n# helpers\n\ndef pair(t):\n    return t if isinstance(t, tuple) else (t, t)\n\n# classes\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs)\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout = 0.):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass Transformer(nn.Module):\n    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        local_attn_heads = 0\n        local_window_size = 256\n        causal = False\n        nb_features = None\n        generalized_attention = False\n        kernel_fn = nn.ReLU()\n        attn_dropout = 0.\n        no_projection = False\n        qkv_bias = True\n        attn_out_bias = True\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PreNorm(dim, SelfAttention(dim, causal = causal, heads = heads, dim_head = dim_head, local_heads = local_attn_heads, local_window_size = local_window_size, nb_features = nb_features, generalized_attention = generalized_attention, kernel_fn = kernel_fn, dropout = attn_dropout, no_projection = no_projection, qkv_bias = qkv_bias, attn_out_bias = attn_out_bias)),\n                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n            ]))\n    def forward(self, x):\n        for attn, ff in self.layers:\n            x = attn(x) + x\n            x = ff(x) + x\n        return x\n\nclass ViT(nn.Module):\n    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n        super().__init__()\n        image_height, image_width = pair(image_size)\n        patch_height, patch_width = pair(patch_size)\n\n        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n\n        num_patches = (image_height // patch_height) * (image_width // patch_width)\n        patch_dim = channels * patch_height * patch_width\n        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n\n        self.to_patch_embedding = nn.Sequential(\n            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n            nn.Linear(patch_dim, dim),\n        )\n\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n        self.dropout = nn.Dropout(emb_dropout)\n\n        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n\n        self.pool = pool\n        self.to_latent = nn.Identity()\n\n        self.mlp_head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_classes)\n        )\n\n    def forward(self, img):\n        x = self.to_patch_embedding(img)\n        b, n, _ = x.shape\n\n        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x += self.pos_embedding[:, :(n + 1)]\n        x = self.dropout(x)\n\n        x = self.transformer(x)\n\n        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n\n        x = self.to_latent(x)\n        return self.mlp_head(x)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T17:03:56.781992Z","iopub.execute_input":"2021-07-26T17:03:56.782387Z","iopub.status.idle":"2021-07-26T17:03:56.804918Z","shell.execute_reply.started":"2021-07-26T17:03:56.782344Z","shell.execute_reply":"2021-07-26T17:03:56.803527Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\nmodel = ViT(\n    image_size = 32,\n    patch_size = 1,\n    num_classes = 10,             # number of stages\n    dim = 128,  # dimensions at each stage\n    depth = 4,              # transformer of depth 4 at each stage\n    heads = 4,      # heads at each stage\n    mlp_dim = 256,\n    dropout = 0.,\n    dim_head = 32\n)\n\n\nmodel.to(device)\nprint(summary(model, (3,32,32)))","metadata":{"execution":{"iopub.status.busy":"2021-07-26T17:03:56.806674Z","iopub.execute_input":"2021-07-26T17:03:56.807044Z","iopub.status.idle":"2021-07-26T17:04:02.759067Z","shell.execute_reply.started":"2021-07-26T17:03:56.807007Z","shell.execute_reply":"2021-07-26T17:04:02.758231Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n         Rearrange-1              [-1, 1024, 3]               0\n            Linear-2            [-1, 1024, 128]             512\n           Dropout-3            [-1, 1025, 128]               0\n         LayerNorm-4            [-1, 1025, 128]             256\n            Linear-5            [-1, 1025, 128]          16,512\n            Linear-6            [-1, 1025, 128]          16,512\n            Linear-7            [-1, 1025, 128]          16,512\n     FastAttention-8          [-1, 4, 1025, 32]               0\n            Linear-9            [-1, 1025, 128]          16,512\n          Dropout-10            [-1, 1025, 128]               0\n    SelfAttention-11            [-1, 1025, 128]               0\n          PreNorm-12            [-1, 1025, 128]               0\n        LayerNorm-13            [-1, 1025, 128]             256\n           Linear-14            [-1, 1025, 256]          33,024\n             GELU-15            [-1, 1025, 256]               0\n          Dropout-16            [-1, 1025, 256]               0\n           Linear-17            [-1, 1025, 128]          32,896\n          Dropout-18            [-1, 1025, 128]               0\n      FeedForward-19            [-1, 1025, 128]               0\n          PreNorm-20            [-1, 1025, 128]               0\n        LayerNorm-21            [-1, 1025, 128]             256\n           Linear-22            [-1, 1025, 128]          16,512\n           Linear-23            [-1, 1025, 128]          16,512\n           Linear-24            [-1, 1025, 128]          16,512\n    FastAttention-25          [-1, 4, 1025, 32]               0\n           Linear-26            [-1, 1025, 128]          16,512\n          Dropout-27            [-1, 1025, 128]               0\n    SelfAttention-28            [-1, 1025, 128]               0\n          PreNorm-29            [-1, 1025, 128]               0\n        LayerNorm-30            [-1, 1025, 128]             256\n           Linear-31            [-1, 1025, 256]          33,024\n             GELU-32            [-1, 1025, 256]               0\n          Dropout-33            [-1, 1025, 256]               0\n           Linear-34            [-1, 1025, 128]          32,896\n          Dropout-35            [-1, 1025, 128]               0\n      FeedForward-36            [-1, 1025, 128]               0\n          PreNorm-37            [-1, 1025, 128]               0\n        LayerNorm-38            [-1, 1025, 128]             256\n           Linear-39            [-1, 1025, 128]          16,512\n           Linear-40            [-1, 1025, 128]          16,512\n           Linear-41            [-1, 1025, 128]          16,512\n    FastAttention-42          [-1, 4, 1025, 32]               0\n           Linear-43            [-1, 1025, 128]          16,512\n          Dropout-44            [-1, 1025, 128]               0\n    SelfAttention-45            [-1, 1025, 128]               0\n          PreNorm-46            [-1, 1025, 128]               0\n        LayerNorm-47            [-1, 1025, 128]             256\n           Linear-48            [-1, 1025, 256]          33,024\n             GELU-49            [-1, 1025, 256]               0\n          Dropout-50            [-1, 1025, 256]               0\n           Linear-51            [-1, 1025, 128]          32,896\n          Dropout-52            [-1, 1025, 128]               0\n      FeedForward-53            [-1, 1025, 128]               0\n          PreNorm-54            [-1, 1025, 128]               0\n        LayerNorm-55            [-1, 1025, 128]             256\n           Linear-56            [-1, 1025, 128]          16,512\n           Linear-57            [-1, 1025, 128]          16,512\n           Linear-58            [-1, 1025, 128]          16,512\n    FastAttention-59          [-1, 4, 1025, 32]               0\n           Linear-60            [-1, 1025, 128]          16,512\n          Dropout-61            [-1, 1025, 128]               0\n    SelfAttention-62            [-1, 1025, 128]               0\n          PreNorm-63            [-1, 1025, 128]               0\n        LayerNorm-64            [-1, 1025, 128]             256\n           Linear-65            [-1, 1025, 256]          33,024\n             GELU-66            [-1, 1025, 256]               0\n          Dropout-67            [-1, 1025, 256]               0\n           Linear-68            [-1, 1025, 128]          32,896\n          Dropout-69            [-1, 1025, 128]               0\n      FeedForward-70            [-1, 1025, 128]               0\n          PreNorm-71            [-1, 1025, 128]               0\n      Transformer-72            [-1, 1025, 128]               0\n         Identity-73                  [-1, 128]               0\n        LayerNorm-74                  [-1, 128]             256\n           Linear-75                   [-1, 10]           1,290\n================================================================\nTotal params: 531,978\nTrainable params: 531,978\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.01\nForward/backward pass size (MB): 83.11\nParams size (MB): 2.03\nEstimated Total Size (MB): 85.15\n----------------------------------------------------------------\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\nscaler = torch.cuda.amp.GradScaler()\n# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\ntop1 = []\ntop5 = []\noptimizer = optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\nfor epoch in range(40):  # loop over the dataset multiple times\n    t0 = time.time()\n    epoch_accuracy = 0\n    epoch_loss = 0\n    running_loss = 0.0\n\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data[0].to(device), data[1].to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        with torch.cuda.amp.autocast():\n            loss = criterion(outputs, labels)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        acc = (outputs.argmax(dim=1) == labels).float().mean()\n        epoch_accuracy += acc / len(trainloader)\n        epoch_loss += loss / len(trainloader)\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 200 == 199:    # print every 2000 mini-batches\n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss / 2000))\n            running_loss = 0.0\n    correct = 0\n    total = 0\n    correct_1=0\n    correct_5=0\n    c = 0\n    with torch.no_grad():\n        for data in testloader:\n            images, labels = data[0].to(device), data[1].to(device)\n            outputs = model(images)\n#         outputs = net(images)\n\n            _, predicted = torch.max(outputs.data, 1)\n            res = accuracy(outputs, labels)\n            correct_1 += res[0][0].float()\n            correct_5 += res[1][0].float()\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            c += 1\n        \n    print(f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - Top 1: {correct_1/c} - Top 5: {correct_5/c} - Time: {time.time() - t0}\\n\")\n    top1.append(correct_1/c)\n    top5.append(correct_5/c)\n    if float(correct_1/c) >= float(max(top1)):\n        PATH = 'ViP.pth'\n        torch.save(model.state_dict(), PATH)\n        print(1)\nprint('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2021-07-26T19:33:25.729061Z","iopub.execute_input":"2021-07-26T19:33:25.729410Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"[1,   200] loss: 0.047\nEpoch : 1 - loss : 0.4954 - acc: 0.8183 - Top 1: 53.520572662353516 - Top 5: 92.91930389404297 - Time: 219.76932668685913\n\n1\n[2,   200] loss: 0.041\nEpoch : 2 - loss : 0.4521 - acc: 0.8374 - Top 1: 53.34256362915039 - Top 5: 93.0874252319336 - Time: 219.98375177383423\n\n[3,   200] loss: 0.040\nEpoch : 3 - loss : 0.4445 - acc: 0.8379 - Top 1: 53.62935256958008 - Top 5: 92.96875 - Time: 219.72788405418396\n\n1\n[4,   200] loss: 0.038\nEpoch : 4 - loss : 0.4175 - acc: 0.8483 - Top 1: 53.698577880859375 - Top 5: 92.7907485961914 - Time: 219.84369373321533\n\n1\n[5,   200] loss: 0.037\nEpoch : 5 - loss : 0.4041 - acc: 0.8536 - Top 1: 52.34375 - Top 5: 92.49407196044922 - Time: 219.92320156097412\n\n[6,   200] loss: 0.034\nEpoch : 6 - loss : 0.3817 - acc: 0.8595 - Top 1: 53.62935256958008 - Top 5: 92.56329345703125 - Time: 219.87092351913452\n\n[7,   200] loss: 0.033\nEpoch : 7 - loss : 0.3676 - acc: 0.8662 - Top 1: 53.1151123046875 - Top 5: 92.65229797363281 - Time: 219.91751074790955\n\n[8,   200] loss: 0.032\n","output_type":"stream"}]},{"cell_type":"code","source":"plt.subplot(2, 1, 1)\nplt.plot(top1[0:14], '.-')\nplt.title('Accuracy')\nplt.ylabel('Top 1 accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(top5[0:14], '.-')\nplt.xlabel('epochs')\nplt.ylabel('Top 5 accuracy')\n\nplt.show()\nnp.shape(top1)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T19:30:40.432582Z","iopub.execute_input":"2021-07-26T19:30:40.433291Z","iopub.status.idle":"2021-07-26T19:30:40.798201Z","shell.execute_reply.started":"2021-07-26T19:30:40.433249Z","shell.execute_reply":"2021-07-26T19:30:40.797273Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(40,)"},"metadata":{}}]}]}