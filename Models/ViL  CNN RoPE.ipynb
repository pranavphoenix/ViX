{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch, math\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\n!pip install torchsummary\nfrom torchsummary import summary\nimport time\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nimport torch.optim as optim\n!pip install torchsummary\nfrom torchsummary import summary\n!pip install einops\nfrom math import ceil\n# !pip install nystrom-attention\n# !pip install performer_pytorch\n!pip install linformer\nfrom torch import nn, einsum\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\n\nfrom einops import rearrange, reduce\n\n# helpers\nfrom einops import reduce\n\ntransform = transforms.Compose(\n        [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\nbatch_size = 256\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\ndef accuracy(output, target, topk=(1,5)):\n    \"\"\"Computes the precision@k for the specified values of k\n    prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n    \"\"\"\n    maxk = max(topk)\n         # sizefunction: the number of total elements\n    batch_size = target.size(0) \n \n         # topk function selects the number of k before output\n    _, pred = output.topk(maxk, 1, True, True)\n         ##########Do not understand t()k\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))   \n    res = []\n    for k in topk:\n        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"BmgrxU_RFfaL","outputId":"45a30594-520c-461d-87d2-679f8ad3d496","execution":{"iopub.status.busy":"2021-07-28T19:46:36.050756Z","iopub.execute_input":"2021-07-28T19:46:36.051118Z","iopub.status.idle":"2021-07-28T19:47:14.500669Z","shell.execute_reply.started":"2021-07-28T19:46:36.051040Z","shell.execute_reply":"2021-07-28T19:47:14.499685Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nRequirement already satisfied: torchsummary in /opt/conda/lib/python3.7/site-packages (1.5.1)\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nCollecting einops\n  Downloading einops-0.3.0-py2.py3-none-any.whl (25 kB)\nInstalling collected packages: einops\nSuccessfully installed einops-0.3.0\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nCollecting linformer\n  Downloading linformer-0.2.1-py3-none-any.whl (6.1 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from linformer) (1.7.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->linformer) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch->linformer) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch->linformer) (0.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch->linformer) (1.19.5)\nInstalling collected packages: linformer\nSuccessfully installed linformer-0.2.1\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"285b8c7c3e154e3592a49303f9024921"}},"metadata":{}},{"name":"stdout","text":"Extracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"import math\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom linformer.reversible import ReversibleSequence, SequentialSequence\n\ndef rotate_every_two(x):\n    x = rearrange(x, '... (d j) -> ... d j', j = 2)\n    x1, x2 = x.unbind(dim = -1)\n    x = torch.stack((-x2, x1), dim = -1)\n    return rearrange(x, '... d j -> ... (d j)')\n\ndef apply_rotary_pos_emb(q, k, sinu_pos):\n    sinu_pos = rearrange(sinu_pos, '() n (j d) -> n j d', j = 2)\n    sin, cos = sinu_pos.unbind(dim = -2)\n\n    sin, cos = map(lambda t: repeat(t, 'b n -> b (n j)', j = 2), (sin, cos))\n    q, k = map(lambda t: (t * cos) + (rotate_every_two(t) * sin), (q, k))\n    return q, k\n\nclass FixedPositionalEmbedding(nn.Module):\n    def __init__(self, dim, max_seq_len):\n        super().__init__()\n        inv_freq = 1. / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n        position = torch.arange(0, max_seq_len, dtype=torch.float)\n        sinusoid_inp = torch.einsum(\"i,j->ij\", position, inv_freq)\n        emb = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n        self.register_buffer('emb', emb)\n\n    def forward(self, x):\n        return self.emb[None, :x.shape[1], :].to(x)\n\ndef exists(val):\n    return val is not None\n\ndef default(val, default_val):\n    return val if val is not None else default_val\n\ndef init_(tensor):\n    dim = tensor.shape[-1]\n    std = 1 / math.sqrt(dim)\n    tensor.uniform_(-std, std)\n    return tensor\n\nclass LinformerSelfAttention(nn.Module):\n    def __init__(self, dim, seq_len, k = 256, heads = 8, dim_head = None, one_kv_head = False, share_kv = False, dropout = 0.):\n        super().__init__()\n        assert (dim % heads) == 0, 'dimension must be divisible by the number of heads'\n\n        self.seq_len = seq_len\n        self.k = k\n\n        self.heads = heads\n\n        dim_head = default(dim_head, dim // heads)\n        self.dim_head = dim_head\n\n        self.to_q = nn.Linear(dim, dim_head * heads, bias = False)\n\n        kv_dim = dim_head if one_kv_head else (dim_head * heads)\n        self.to_k = nn.Linear(dim, kv_dim, bias = False)\n        self.proj_k = nn.Parameter(init_(torch.zeros(seq_len, k)))\n\n        self.share_kv = share_kv\n        if not share_kv:\n            self.to_v = nn.Linear(dim, kv_dim, bias = False)\n            self.proj_v = nn.Parameter(init_(torch.zeros(seq_len, k)))\n\n        self.dropout = nn.Dropout(dropout)\n        self.to_out = nn.Linear(dim_head * heads, dim)\n\n    def forward(self, x, pos_emb = None, context = None, **kwargs):\n        b, n, d, d_h, h, k = *x.shape, self.dim_head, self.heads, self.k\n\n        kv_len = n if context is None else context.shape[1]\n        assert kv_len == self.seq_len, f'the sequence length of the key / values must be {self.seq_len} - {kv_len} given'\n\n        queries = self.to_q(x)\n\n        proj_seq_len = lambda args: torch.einsum('bnd,nk->bkd', *args)\n\n        kv_input = x if context is None else context\n\n        keys = self.to_k(kv_input)\n        values = self.to_v(kv_input) if not self.share_kv else keys\n\n        queries, keys = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (queries, keys))\n\n        if exists(pos_emb): \n            queries, keys = apply_rotary_pos_emb(queries, keys, pos_emb)\n            queries, keys = map(lambda t: rearrange(t, 'b h n d -> b n (h d)', h = h), (queries, keys))\n        \n\n        kv_projs = (self.proj_k, self.proj_v if not self.share_kv else self.proj_k)\n\n        # project keys and values along the sequence length dimension to k\n\n        keys, values = map(proj_seq_len, zip((keys, values), kv_projs))\n\n        # merge head into batch for queries and key / values\n\n        queries = queries.reshape(b, n, h, -1).transpose(1, 2)\n\n        merge_key_values = lambda t: t.reshape(b, k, -1, d_h).transpose(1, 2).expand(-1, h, -1, -1)\n        keys, values = map(merge_key_values, (keys, values))\n        \n        # attention\n\n        dots = torch.einsum('bhnd,bhkd->bhnk', queries, keys) * (d_h ** -0.5)\n        attn = dots.softmax(dim=-1)\n        attn = self.dropout(attn)\n        out = torch.einsum('bhnk,bhkd->bhnd', attn, values)\n\n        # split heads\n        out = out.transpose(1, 2).reshape(b, n, -1)\n        return self.to_out(out)","metadata":{"id":"FOiFLNi6FfaR","execution":{"iopub.status.busy":"2021-07-28T19:47:14.502597Z","iopub.execute_input":"2021-07-28T19:47:14.503024Z","iopub.status.idle":"2021-07-28T19:47:14.537223Z","shell.execute_reply.started":"2021-07-28T19:47:14.502983Z","shell.execute_reply":"2021-07-28T19:47:14.536270Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn, einsum\nimport torch.nn.functional as F\n\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\n\n# helpers\n\ndef pair(t):\n    return t if isinstance(t, tuple) else (t, t)\n\n# classes\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs)\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout = 0.):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass Transformer(nn.Module):\n    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        k = 64\n        seq_len = 1025\n        one_kv_head = False #use False for RoPE\n        share_kv = True\n        reversible = False\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PreNorm(dim, LinformerSelfAttention(dim, seq_len, k = k, heads = heads, dim_head = dim_head, one_kv_head = one_kv_head, share_kv = share_kv, dropout = dropout)),\n                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n            ]))\n    def forward(self, x, pos_emb):\n        for attn, ff in self.layers:\n            x = attn(x, pos_emb = pos_emb) + x\n            x = ff(x) + x\n        return x\n\nclass ViT(nn.Module):\n    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0., rotary_position_emb = True):\n        super().__init__()\n        image_height, image_width = pair(image_size)\n        patch_height, patch_width = pair(patch_size)\n\n        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n\n        num_patches = (image_height // patch_height) * (image_width // patch_width)\n        patch_dim = channels * patch_height * patch_width\n        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n\n        self.to_patch_embedding = nn.Sequential(\n            nn.Conv2d(3, 32, 3, stride = 1, padding = 1),\n            nn.Conv2d(32, 64, 3, stride = 1, padding = 1),\n            nn.Conv2d(64, dim, 3, stride = 1, padding = 1),\n            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width)\n            \n        )\n\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n        self.dropout = nn.Dropout(emb_dropout)\n\n        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n\n        self.pool = pool\n        self.to_latent = nn.Identity()\n\n        self.mlp_head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_classes)\n        )\n        max_seq_len = 1025\n        if rotary_position_emb:\n            self.layer_pos_emb = FixedPositionalEmbedding(dim_head, max_seq_len)\n\n    def forward(self, img):\n        x = self.to_patch_embedding(img)\n        b, n, _ = x.shape\n\n        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n        x = torch.cat((cls_tokens, x), dim=1)\n#         x += self.pos_embedding[:, :(n + 1)] #1D learnable PE\n        x = self.dropout(x)\n\n        layer_pos_emb = self.layer_pos_emb(x)\n        x = self.transformer(x, pos_emb = layer_pos_emb)\n\n        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n\n        x = self.to_latent(x)\n        return self.mlp_head(x)","metadata":{"id":"94EEKIehFfaU","execution":{"iopub.status.busy":"2021-07-28T19:47:14.539875Z","iopub.execute_input":"2021-07-28T19:47:14.540287Z","iopub.status.idle":"2021-07-28T19:47:14.566086Z","shell.execute_reply.started":"2021-07-28T19:47:14.540248Z","shell.execute_reply":"2021-07-28T19:47:14.564878Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"model = ViT(\n    image_size = 32,\n    patch_size = 1,\n    num_classes = 10,             # number of stages\n    dim = 128,  # dimensions at each stage\n    depth = 4,              # transformer of depth 4 at each stage\n    heads = 4,      # heads at each stage\n    mlp_dim = 256,\n    dropout = 0.,\n    dim_head = 32\n)\n\n\nmodel.to(device)\nprint(summary(model, (3,32,32)))","metadata":{"id":"WoOl842NFfaX","outputId":"c9630a92-ad98-4274-dd20-0def392d91c9","execution":{"iopub.status.busy":"2021-07-28T19:47:14.568083Z","iopub.execute_input":"2021-07-28T19:47:14.568516Z","iopub.status.idle":"2021-07-28T19:47:19.953775Z","shell.execute_reply.started":"2021-07-28T19:47:14.568473Z","shell.execute_reply":"2021-07-28T19:47:19.952956Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 32, 32, 32]             896\n            Conv2d-2           [-1, 64, 32, 32]          18,496\n            Conv2d-3          [-1, 128, 32, 32]          73,856\n         Rearrange-4            [-1, 1024, 128]               0\n           Dropout-5            [-1, 1025, 128]               0\nFixedPositionalEmbedding-6             [-1, 1025, 32]               0\n         LayerNorm-7            [-1, 1025, 128]             256\n            Linear-8            [-1, 1025, 128]          16,384\n            Linear-9            [-1, 1025, 128]          16,384\n          Dropout-10          [-1, 4, 1025, 64]               0\n           Linear-11            [-1, 1025, 128]          16,512\nLinformerSelfAttention-12            [-1, 1025, 128]               0\n          PreNorm-13            [-1, 1025, 128]               0\n        LayerNorm-14            [-1, 1025, 128]             256\n           Linear-15            [-1, 1025, 256]          33,024\n             GELU-16            [-1, 1025, 256]               0\n          Dropout-17            [-1, 1025, 256]               0\n           Linear-18            [-1, 1025, 128]          32,896\n          Dropout-19            [-1, 1025, 128]               0\n      FeedForward-20            [-1, 1025, 128]               0\n          PreNorm-21            [-1, 1025, 128]               0\n        LayerNorm-22            [-1, 1025, 128]             256\n           Linear-23            [-1, 1025, 128]          16,384\n           Linear-24            [-1, 1025, 128]          16,384\n          Dropout-25          [-1, 4, 1025, 64]               0\n           Linear-26            [-1, 1025, 128]          16,512\nLinformerSelfAttention-27            [-1, 1025, 128]               0\n          PreNorm-28            [-1, 1025, 128]               0\n        LayerNorm-29            [-1, 1025, 128]             256\n           Linear-30            [-1, 1025, 256]          33,024\n             GELU-31            [-1, 1025, 256]               0\n          Dropout-32            [-1, 1025, 256]               0\n           Linear-33            [-1, 1025, 128]          32,896\n          Dropout-34            [-1, 1025, 128]               0\n      FeedForward-35            [-1, 1025, 128]               0\n          PreNorm-36            [-1, 1025, 128]               0\n        LayerNorm-37            [-1, 1025, 128]             256\n           Linear-38            [-1, 1025, 128]          16,384\n           Linear-39            [-1, 1025, 128]          16,384\n          Dropout-40          [-1, 4, 1025, 64]               0\n           Linear-41            [-1, 1025, 128]          16,512\nLinformerSelfAttention-42            [-1, 1025, 128]               0\n          PreNorm-43            [-1, 1025, 128]               0\n        LayerNorm-44            [-1, 1025, 128]             256\n           Linear-45            [-1, 1025, 256]          33,024\n             GELU-46            [-1, 1025, 256]               0\n          Dropout-47            [-1, 1025, 256]               0\n           Linear-48            [-1, 1025, 128]          32,896\n          Dropout-49            [-1, 1025, 128]               0\n      FeedForward-50            [-1, 1025, 128]               0\n          PreNorm-51            [-1, 1025, 128]               0\n        LayerNorm-52            [-1, 1025, 128]             256\n           Linear-53            [-1, 1025, 128]          16,384\n           Linear-54            [-1, 1025, 128]          16,384\n          Dropout-55          [-1, 4, 1025, 64]               0\n           Linear-56            [-1, 1025, 128]          16,512\nLinformerSelfAttention-57            [-1, 1025, 128]               0\n          PreNorm-58            [-1, 1025, 128]               0\n        LayerNorm-59            [-1, 1025, 128]             256\n           Linear-60            [-1, 1025, 256]          33,024\n             GELU-61            [-1, 1025, 256]               0\n          Dropout-62            [-1, 1025, 256]               0\n           Linear-63            [-1, 1025, 128]          32,896\n          Dropout-64            [-1, 1025, 128]               0\n      FeedForward-65            [-1, 1025, 128]               0\n          PreNorm-66            [-1, 1025, 128]               0\n      Transformer-67            [-1, 1025, 128]               0\n         Identity-68                  [-1, 128]               0\n        LayerNorm-69                  [-1, 128]             256\n           Linear-70                   [-1, 10]           1,290\n================================================================\nTotal params: 557,642\nTrainable params: 557,642\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.01\nForward/backward pass size (MB): 81.08\nParams size (MB): 2.13\nEstimated Total Size (MB): 83.22\n----------------------------------------------------------------\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\nscaler = torch.cuda.amp.GradScaler()\n# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\ntop1 = []\ntop5 = []\noptimizer = optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\nfor epoch in range(40):  # loop over the dataset multiple times\n    t0 = time.time()\n    epoch_accuracy = 0\n    epoch_loss = 0\n    running_loss = 0.0\n\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data[0].to(device), data[1].to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        with torch.cuda.amp.autocast():\n            loss = criterion(outputs, labels)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        acc = (outputs.argmax(dim=1) == labels).float().mean()\n        epoch_accuracy += acc / len(trainloader)\n        epoch_loss += loss / len(trainloader)\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 200 == 199:    # print every 2000 mini-batches\n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss / 2000))\n            running_loss = 0.0\n    correct = 0\n    total = 0\n    correct_1=0\n    correct_5=0\n    c = 0\n    with torch.no_grad():\n        for data in testloader:\n            images, labels = data[0].to(device), data[1].to(device)\n            outputs = model(images)\n#         outputs = net(images)\n\n            _, predicted = torch.max(outputs.data, 1)\n            res = accuracy(outputs, labels)\n            correct_1 += res[0][0].float()\n            correct_5 += res[1][0].float()\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            c += 1\n        \n    print(f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - Top 1: {correct_1/c} - Top 5: {correct_5/c} - Time: {time.time() - t0}\\n\")\n    top1.append(correct_1/c)\n    top5.append(correct_5/c)\n    if float(correct_1/c) >= float(max(top1)):\n        PATH = 'ViLCNNRoPE.pth'\n        torch.save(model.state_dict(), PATH)\n        print(1)\nprint('Finished Training')","metadata":{"id":"yZAB3FhZFfaX","outputId":"c4b7d913-a3fd-4e2a-e84e-c04d92f37817","execution":{"iopub.status.busy":"2021-07-28T19:47:19.955209Z","iopub.execute_input":"2021-07-28T19:47:19.955582Z","iopub.status.idle":"2021-07-28T20:32:07.505024Z","shell.execute_reply.started":"2021-07-28T19:47:19.955545Z","shell.execute_reply":"2021-07-28T20:32:07.502833Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Epoch : 1 - loss : 1.6135 - acc: 0.4112 - Top 1: 49.453125 - Top 5: 93.1640625 - Time: 133.58766794204712\n\n1\nEpoch : 2 - loss : 1.3192 - acc: 0.5204 - Top 1: 54.404296875 - Top 5: 94.8046875 - Time: 133.83519315719604\n\n1\nEpoch : 3 - loss : 1.1732 - acc: 0.5783 - Top 1: 57.5390625 - Top 5: 95.576171875 - Time: 133.75986671447754\n\n1\nEpoch : 4 - loss : 1.0655 - acc: 0.6172 - Top 1: 59.70703125 - Top 5: 96.103515625 - Time: 133.9139428138733\n\n1\nEpoch : 5 - loss : 0.9645 - acc: 0.6558 - Top 1: 60.95703125 - Top 5: 96.337890625 - Time: 133.80614686012268\n\n1\nEpoch : 6 - loss : 0.8673 - acc: 0.6904 - Top 1: 61.806640625 - Top 5: 96.494140625 - Time: 133.80714511871338\n\n1\nEpoch : 7 - loss : 0.7745 - acc: 0.7235 - Top 1: 63.14453125 - Top 5: 96.6796875 - Time: 133.76443529129028\n\n1\nEpoch : 8 - loss : 0.6833 - acc: 0.7564 - Top 1: 63.271484375 - Top 5: 96.640625 - Time: 133.71886563301086\n\n1\nEpoch : 9 - loss : 0.6075 - acc: 0.7840 - Top 1: 63.408203125 - Top 5: 96.81640625 - Time: 133.84882187843323\n\n1\nEpoch : 10 - loss : 0.5328 - acc: 0.8103 - Top 1: 63.896484375 - Top 5: 96.669921875 - Time: 133.78736186027527\n\n1\nEpoch : 11 - loss : 0.4471 - acc: 0.8421 - Top 1: 64.052734375 - Top 5: 96.7578125 - Time: 133.86109948158264\n\n1\nEpoch : 12 - loss : 0.3890 - acc: 0.8627 - Top 1: 63.837890625 - Top 5: 96.77734375 - Time: 133.821063041687\n\nEpoch : 13 - loss : 0.3316 - acc: 0.8816 - Top 1: 64.169921875 - Top 5: 96.474609375 - Time: 133.74534225463867\n\n1\nEpoch : 14 - loss : 0.2841 - acc: 0.8984 - Top 1: 64.404296875 - Top 5: 96.630859375 - Time: 133.7377324104309\n\n1\nEpoch : 15 - loss : 0.2367 - acc: 0.9179 - Top 1: 63.515625 - Top 5: 96.34765625 - Time: 133.77208590507507\n\nEpoch : 16 - loss : 0.2037 - acc: 0.9272 - Top 1: 64.033203125 - Top 5: 96.337890625 - Time: 133.85373449325562\n\nEpoch : 17 - loss : 0.1760 - acc: 0.9381 - Top 1: 63.759765625 - Top 5: 96.26953125 - Time: 133.95656657218933\n\nEpoch : 18 - loss : 0.1650 - acc: 0.9415 - Top 1: 62.666015625 - Top 5: 96.1328125 - Time: 133.80550360679626\n\nEpoch : 19 - loss : 0.1484 - acc: 0.9475 - Top 1: 64.111328125 - Top 5: 96.23046875 - Time: 133.85560941696167\n\nEpoch : 20 - loss : 0.1096 - acc: 0.9618 - Top 1: 63.427734375 - Top 5: 96.162109375 - Time: 133.76998448371887\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-d4ca5e2415c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No inf checks were recorded for this optimizer.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No inf checks were recorded for this optimizer.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"plt.subplot(2, 1, 1)\nplt.plot(top1, '.-')\nplt.title('Accuracy')\nplt.ylabel('Top 1 accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(top5, '.-')\nplt.xlabel('epochs')\nplt.ylabel('Top 5 accuracy')\n\nplt.show()\n","metadata":{"id":"suqt55zpFfaZ","execution":{"iopub.status.busy":"2021-07-28T20:32:22.079062Z","iopub.execute_input":"2021-07-28T20:32:22.079403Z","iopub.status.idle":"2021-07-28T20:32:22.320662Z","shell.execute_reply.started":"2021-07-28T20:32:22.079360Z","shell.execute_reply":"2021-07-28T20:32:22.319733Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(20,)"},"metadata":{}}]}]}