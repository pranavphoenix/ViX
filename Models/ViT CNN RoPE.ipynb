{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch, math\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\n!pip install torchsummary\nfrom torchsummary import summary\nimport time\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nimport torch.optim as optim\n!pip install torchsummary\nfrom torchsummary import summary\n!pip install einops\nfrom math import ceil\n# !pip install nystrom-attention\n\nfrom torch import nn, einsum\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\n\nfrom einops import rearrange, reduce\n\n# helpers\nfrom einops import reduce\n\ntransform = transforms.Compose(\n        [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\nbatch_size = 64\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\ndef accuracy(output, target, topk=(1,5)):\n    \"\"\"Computes the precision@k for the specified values of k\n    prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n    \"\"\"\n    maxk = max(topk)\n         # sizefunction: the number of total elements\n    batch_size = target.size(0) \n \n         # topk function selects the number of k before output\n    _, pred = output.topk(maxk, 1, True, True)\n         ##########Do not understand t()k\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))   \n    res = []\n    for k in topk:\n        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"X0phoSUyE6SI","outputId":"3974b74a-be7a-4d7a-bd74-36e59f35ec74","execution":{"iopub.status.busy":"2021-07-29T13:48:36.548640Z","iopub.execute_input":"2021-07-29T13:48:36.549278Z","iopub.status.idle":"2021-07-29T13:49:04.777664Z","shell.execute_reply.started":"2021-07-29T13:48:36.549065Z","shell.execute_reply":"2021-07-29T13:49:04.776684Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nRequirement already satisfied: torchsummary in /opt/conda/lib/python3.7/site-packages (1.5.1)\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nCollecting einops\n  Downloading einops-0.3.0-py2.py3-none-any.whl (25 kB)\nInstalling collected packages: einops\nSuccessfully installed einops-0.3.0\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c952de7b6430487db0f5de630a4e8125"}},"metadata":{}},{"name":"stdout","text":"Extracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"def exists(val):\n    return val is not None\n\ndef rotate_every_two(x):\n    x = rearrange(x, '... (d j) -> ... d j', j = 2)\n    x1, x2 = x.unbind(dim = -1)\n    x = torch.stack((-x2, x1), dim = -1)\n    return rearrange(x, '... d j -> ... (d j)')\n\ndef apply_rotary_pos_emb(q, k, sinu_pos):\n    sinu_pos = rearrange(sinu_pos, '() n (j d) -> n j d', j = 2)\n    sin, cos = sinu_pos.unbind(dim = -2)\n\n    sin, cos = map(lambda t: repeat(t, 'b n -> b (n j)', j = 2), (sin, cos))\n    q, k = map(lambda t: (t * cos) + (rotate_every_two(t) * sin), (q, k))\n    return q, k\n\nclass FixedPositionalEmbedding(nn.Module):\n    def __init__(self, dim, max_seq_len):\n        super().__init__()\n        inv_freq = 1. / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n        position = torch.arange(0, max_seq_len, dtype=torch.float)\n        sinusoid_inp = torch.einsum(\"i,j->ij\", position, inv_freq)\n        emb = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n        self.register_buffer('emb', emb)\n\n    def forward(self, x):\n        return self.emb[None, :x.shape[1], :].to(x)\n","metadata":{"id":"wyDFkX7yFJfW","execution":{"iopub.status.busy":"2021-07-29T13:49:04.779394Z","iopub.execute_input":"2021-07-29T13:49:04.779741Z","iopub.status.idle":"2021-07-29T13:49:04.790895Z","shell.execute_reply.started":"2021-07-29T13:49:04.779703Z","shell.execute_reply":"2021-07-29T13:49:04.789883Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn, einsum\nimport torch.nn.functional as F\n\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\n\n# helpers\n\ndef pair(t):\n    return t if isinstance(t, tuple) else (t, t)\n\n# classes\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs)\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout = 0.):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass Attention(nn.Module):\n    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n        super().__init__()\n        inner_dim = dim_head *  heads\n        project_out = not (heads == 1 and dim_head == dim)\n\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n\n        self.attend = nn.Softmax(dim = -1)\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim),\n            nn.Dropout(dropout)\n        ) if project_out else nn.Identity()\n\n    def forward(self, x, pos_emb = None):\n        b, n, _, h = *x.shape, self.heads\n        qkv = self.to_qkv(x).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n        if exists(pos_emb):\n            q, k = apply_rotary_pos_emb(q, k, pos_emb)\n\n        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        attn = self.attend(dots)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass Transformer(nn.Module):\n    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n            ]))\n    def forward(self, x, pos_emb):\n        for attn, ff in self.layers:\n            x = attn(x, pos_emb = pos_emb) + x\n            x = ff(x) + x\n        return x\n\nclass ViT(nn.Module):\n    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0., rotary_position_emb = True):\n        super().__init__()\n        image_height, image_width = pair(image_size)\n        patch_height, patch_width = pair(patch_size)\n\n        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n\n        num_patches = (image_height // patch_height) * (image_width // patch_width)\n        patch_dim = channels * patch_height * patch_width\n        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n\n        self.to_patch_embedding = nn.Sequential(\n            nn.Conv2d(3, 32, 3, stride = 1, padding = 1),\n            nn.Conv2d(32, 64, 3, stride = 1, padding = 1),\n            nn.Conv2d(64, dim, 3, stride = 1, padding = 1),\n            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width)\n        )\n\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n        self.dropout = nn.Dropout(emb_dropout)\n\n        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n\n        self.pool = pool\n        self.to_latent = nn.Identity()\n\n        self.mlp_head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_classes)\n        )\n        max_seq_len = 1025\n        if rotary_position_emb:\n            self.layer_pos_emb = FixedPositionalEmbedding(dim_head, max_seq_len)\n\n\n    def forward(self, img):\n        x = self.to_patch_embedding(img)\n        b, n, _ = x.shape\n\n        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n        x = torch.cat((cls_tokens, x), dim=1)\n#         x += self.pos_embedding[:, :(n + 1)]\n        x = self.dropout(x)\n        layer_pos_emb = self.layer_pos_emb(x)\n        x = self.transformer(x, pos_emb = layer_pos_emb)\n\n        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n\n        x = self.to_latent(x)\n        return self.mlp_head(x)","metadata":{"id":"X-jpMGt0E6SR","execution":{"iopub.status.busy":"2021-07-29T13:49:04.793216Z","iopub.execute_input":"2021-07-29T13:49:04.793828Z","iopub.status.idle":"2021-07-29T13:49:04.826816Z","shell.execute_reply.started":"2021-07-29T13:49:04.793774Z","shell.execute_reply":"2021-07-29T13:49:04.826037Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\nmodel = ViT(\n    image_size = 32,\n    patch_size = 1,\n    num_classes = 10,             # number of stages\n    dim = 128,  # dimensions at each stage\n    depth = 4,              # transformer of depth 4 at each stage\n    heads = 4,      # heads at each stage\n    mlp_dim = 256,\n    dropout = 0.,\n    dim_head = 32\n)\n\n\nmodel.to(device)\nprint(summary(model, (3,32,32)))","metadata":{"id":"mwo5-wfqE6SV","outputId":"f45163f2-ad1f-4be4-abb8-e99b7c3a0083","execution":{"iopub.status.busy":"2021-07-29T13:49:04.828949Z","iopub.execute_input":"2021-07-29T13:49:04.829677Z","iopub.status.idle":"2021-07-29T13:49:10.116733Z","shell.execute_reply.started":"2021-07-29T13:49:04.829635Z","shell.execute_reply":"2021-07-29T13:49:10.115949Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 32, 32, 32]             896\n            Conv2d-2           [-1, 64, 32, 32]          18,496\n            Conv2d-3          [-1, 128, 32, 32]          73,856\n         Rearrange-4            [-1, 1024, 128]               0\n           Dropout-5            [-1, 1025, 128]               0\nFixedPositionalEmbedding-6             [-1, 1025, 32]               0\n         LayerNorm-7            [-1, 1025, 128]             256\n            Linear-8            [-1, 1025, 384]          49,152\n           Softmax-9        [-1, 4, 1025, 1025]               0\n           Linear-10            [-1, 1025, 128]          16,512\n          Dropout-11            [-1, 1025, 128]               0\n        Attention-12            [-1, 1025, 128]               0\n          PreNorm-13            [-1, 1025, 128]               0\n        LayerNorm-14            [-1, 1025, 128]             256\n           Linear-15            [-1, 1025, 256]          33,024\n             GELU-16            [-1, 1025, 256]               0\n          Dropout-17            [-1, 1025, 256]               0\n           Linear-18            [-1, 1025, 128]          32,896\n          Dropout-19            [-1, 1025, 128]               0\n      FeedForward-20            [-1, 1025, 128]               0\n          PreNorm-21            [-1, 1025, 128]               0\n        LayerNorm-22            [-1, 1025, 128]             256\n           Linear-23            [-1, 1025, 384]          49,152\n          Softmax-24        [-1, 4, 1025, 1025]               0\n           Linear-25            [-1, 1025, 128]          16,512\n          Dropout-26            [-1, 1025, 128]               0\n        Attention-27            [-1, 1025, 128]               0\n          PreNorm-28            [-1, 1025, 128]               0\n        LayerNorm-29            [-1, 1025, 128]             256\n           Linear-30            [-1, 1025, 256]          33,024\n             GELU-31            [-1, 1025, 256]               0\n          Dropout-32            [-1, 1025, 256]               0\n           Linear-33            [-1, 1025, 128]          32,896\n          Dropout-34            [-1, 1025, 128]               0\n      FeedForward-35            [-1, 1025, 128]               0\n          PreNorm-36            [-1, 1025, 128]               0\n        LayerNorm-37            [-1, 1025, 128]             256\n           Linear-38            [-1, 1025, 384]          49,152\n          Softmax-39        [-1, 4, 1025, 1025]               0\n           Linear-40            [-1, 1025, 128]          16,512\n          Dropout-41            [-1, 1025, 128]               0\n        Attention-42            [-1, 1025, 128]               0\n          PreNorm-43            [-1, 1025, 128]               0\n        LayerNorm-44            [-1, 1025, 128]             256\n           Linear-45            [-1, 1025, 256]          33,024\n             GELU-46            [-1, 1025, 256]               0\n          Dropout-47            [-1, 1025, 256]               0\n           Linear-48            [-1, 1025, 128]          32,896\n          Dropout-49            [-1, 1025, 128]               0\n      FeedForward-50            [-1, 1025, 128]               0\n          PreNorm-51            [-1, 1025, 128]               0\n        LayerNorm-52            [-1, 1025, 128]             256\n           Linear-53            [-1, 1025, 384]          49,152\n          Softmax-54        [-1, 4, 1025, 1025]               0\n           Linear-55            [-1, 1025, 128]          16,512\n          Dropout-56            [-1, 1025, 128]               0\n        Attention-57            [-1, 1025, 128]               0\n          PreNorm-58            [-1, 1025, 128]               0\n        LayerNorm-59            [-1, 1025, 128]             256\n           Linear-60            [-1, 1025, 256]          33,024\n             GELU-61            [-1, 1025, 256]               0\n          Dropout-62            [-1, 1025, 256]               0\n           Linear-63            [-1, 1025, 128]          32,896\n          Dropout-64            [-1, 1025, 128]               0\n      FeedForward-65            [-1, 1025, 128]               0\n          PreNorm-66            [-1, 1025, 128]               0\n      Transformer-67            [-1, 1025, 128]               0\n         Identity-68                  [-1, 128]               0\n        LayerNorm-69                  [-1, 128]             256\n           Linear-70                   [-1, 10]           1,290\n================================================================\nTotal params: 623,178\nTrainable params: 623,178\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.01\nForward/backward pass size (MB): 209.33\nParams size (MB): 2.38\nEstimated Total Size (MB): 211.72\n----------------------------------------------------------------\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\nscaler = torch.cuda.amp.GradScaler()\n# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\ntop1 = []\ntop5 = []\noptimizer = optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\nfor epoch in range(40):  # loop over the dataset multiple times\n    t0 = time.time()\n    epoch_accuracy = 0\n    epoch_loss = 0\n    running_loss = 0.0\n\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data[0].to(device), data[1].to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        with torch.cuda.amp.autocast():\n            loss = criterion(outputs, labels)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        acc = (outputs.argmax(dim=1) == labels).float().mean()\n        epoch_accuracy += acc / len(trainloader)\n        epoch_loss += loss / len(trainloader)\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 200 == 199:    # print every 2000 mini-batches\n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss / 2000))\n            running_loss = 0.0\n    correct = 0\n    total = 0\n    correct_1=0\n    correct_5=0\n    c = 0\n    with torch.no_grad():\n        for data in testloader:\n            images, labels = data[0].to(device), data[1].to(device)\n            outputs = model(images)\n#         outputs = net(images)\n\n            _, predicted = torch.max(outputs.data, 1)\n            res = accuracy(outputs, labels)\n            correct_1 += res[0][0].float()\n            correct_5 += res[1][0].float()\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            c += 1\n        \n    print(f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - Top 1: {correct_1/c} - Top 5: {correct_5/c} - Time: {time.time() - t0}\\n\")\n    top1.append(correct_1/c)\n    top5.append(correct_5/c)\n    if float(correct_1/c) >= float(max(top1)):\n        PATH = 'ViTCNNRoPE.pth'\n        torch.save(model.state_dict(), PATH)\n        print(1)\nprint('Finished Training')","metadata":{"id":"wCbaCFu8E6SW","outputId":"a8e84724-b3d0-4fad-c76c-6e591ea66621","execution":{"iopub.status.busy":"2021-07-29T13:49:10.117980Z","iopub.execute_input":"2021-07-29T13:49:10.118263Z","iopub.status.idle":"2021-07-29T16:27:37.827430Z","shell.execute_reply.started":"2021-07-29T13:49:10.118237Z","shell.execute_reply":"2021-07-29T16:27:37.825656Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[1,   200] loss: 0.192\n[1,   400] loss: 0.161\n[1,   600] loss: 0.147\nEpoch : 1 - loss : 1.5983 - acc: 0.4150 - Top 1: 52.54777145385742 - Top 5: 93.95899963378906 - Time: 345.748432636261\n\n1\n[2,   200] loss: 0.129\n[2,   400] loss: 0.124\n[2,   600] loss: 0.118\nEpoch : 2 - loss : 1.2146 - acc: 0.5646 - Top 1: 61.40525436401367 - Top 5: 96.3475341796875 - Time: 345.7273392677307\n\n1\n[3,   200] loss: 0.105\n[3,   400] loss: 0.104\n[3,   600] loss: 0.100\nEpoch : 3 - loss : 1.0142 - acc: 0.6383 - Top 1: 65.2070083618164 - Top 5: 97.23328399658203 - Time: 346.49928617477417\n\n1\n[4,   200] loss: 0.091\n[4,   400] loss: 0.089\n[4,   600] loss: 0.087\nEpoch : 4 - loss : 0.8851 - acc: 0.6853 - Top 1: 68.5011978149414 - Top 5: 97.64132690429688 - Time: 346.31526350975037\n\n1\n[5,   200] loss: 0.078\n[5,   400] loss: 0.079\n[5,   600] loss: 0.077\nEpoch : 5 - loss : 0.7818 - acc: 0.7221 - Top 1: 71.25796508789062 - Top 5: 97.7408447265625 - Time: 346.6030752658844\n\n1\n[6,   200] loss: 0.070\n[6,   400] loss: 0.071\n[6,   600] loss: 0.071\nEpoch : 6 - loss : 0.7056 - acc: 0.7511 - Top 1: 71.56648254394531 - Top 5: 97.97969818115234 - Time: 346.47144198417664\n\n1\n[7,   200] loss: 0.063\n[7,   400] loss: 0.063\n[7,   600] loss: 0.065\nEpoch : 7 - loss : 0.6412 - acc: 0.7731 - Top 1: 73.3777847290039 - Top 5: 98.28821563720703 - Time: 346.45924139022827\n\n1\n[8,   200] loss: 0.055\n[8,   400] loss: 0.061\n[8,   600] loss: 0.059\nEpoch : 8 - loss : 0.5863 - acc: 0.7932 - Top 1: 72.36266326904297 - Top 5: 98.14888763427734 - Time: 346.48152351379395\n\n[9,   200] loss: 0.050\n[9,   400] loss: 0.054\n[9,   600] loss: 0.055\nEpoch : 9 - loss : 0.5375 - acc: 0.8096 - Top 1: 73.70621490478516 - Top 5: 98.3777847290039 - Time: 346.67427229881287\n\n1\n[10,   200] loss: 0.046\n[10,   400] loss: 0.049\n[10,   600] loss: 0.050\nEpoch : 10 - loss : 0.4898 - acc: 0.8255 - Top 1: 74.28343963623047 - Top 5: 98.4375 - Time: 346.39405488967896\n\n1\n[11,   200] loss: 0.041\n[11,   400] loss: 0.044\n[11,   600] loss: 0.047\nEpoch : 11 - loss : 0.4441 - acc: 0.8427 - Top 1: 74.59195709228516 - Top 5: 98.0195083618164 - Time: 346.7232720851898\n\n1\n[12,   200] loss: 0.038\n[12,   400] loss: 0.039\n[12,   600] loss: 0.042\nEpoch : 12 - loss : 0.4009 - acc: 0.8578 - Top 1: 74.3630599975586 - Top 5: 98.3280258178711 - Time: 346.4922306537628\n\n[13,   200] loss: 0.033\n[13,   400] loss: 0.035\n[13,   600] loss: 0.038\nEpoch : 13 - loss : 0.3658 - acc: 0.8692 - Top 1: 73.65644836425781 - Top 5: 98.09912872314453 - Time: 346.50104546546936\n\n[14,   200] loss: 0.029\n[14,   400] loss: 0.033\n[14,   600] loss: 0.033\nEpoch : 14 - loss : 0.3266 - acc: 0.8831 - Top 1: 74.5820083618164 - Top 5: 98.44745635986328 - Time: 346.4842417240143\n\n[15,   200] loss: 0.025\n[15,   400] loss: 0.030\n[15,   600] loss: 0.031\nEpoch : 15 - loss : 0.2994 - acc: 0.8931 - Top 1: 74.7511978149414 - Top 5: 98.41759490966797 - Time: 346.61402916908264\n\n1\n[16,   200] loss: 0.024\n[16,   400] loss: 0.027\n[16,   600] loss: 0.029\nEpoch : 16 - loss : 0.2749 - acc: 0.9010 - Top 1: 74.96018981933594 - Top 5: 98.3280258178711 - Time: 346.5341782569885\n\n1\n[17,   200] loss: 0.021\n[17,   400] loss: 0.025\n[17,   600] loss: 0.027\nEpoch : 17 - loss : 0.2486 - acc: 0.9091 - Top 1: 75.0597152709961 - Top 5: 97.93988800048828 - Time: 346.6512460708618\n\n1\n[18,   200] loss: 0.018\n[18,   400] loss: 0.022\n[18,   600] loss: 0.024\nEpoch : 18 - loss : 0.2261 - acc: 0.9174 - Top 1: 73.82563781738281 - Top 5: 98.33798217773438 - Time: 346.5256621837616\n\n[19,   200] loss: 0.017\n[19,   400] loss: 0.020\n[19,   600] loss: 0.023\nEpoch : 19 - loss : 0.2082 - acc: 0.9244 - Top 1: 74.3630599975586 - Top 5: 98.08917236328125 - Time: 346.69697737693787\n\n[20,   200] loss: 0.016\n[20,   400] loss: 0.019\n[20,   600] loss: 0.020\nEpoch : 20 - loss : 0.1909 - acc: 0.9310 - Top 1: 74.93033599853516 - Top 5: 98.33798217773438 - Time: 346.3755669593811\n\n[21,   200] loss: 0.014\n[21,   400] loss: 0.017\n[21,   600] loss: 0.019\nEpoch : 21 - loss : 0.1770 - acc: 0.9357 - Top 1: 75.6966552734375 - Top 5: 98.21855163574219 - Time: 346.3247330188751\n\n1\n[22,   200] loss: 0.013\n[22,   400] loss: 0.015\n[22,   600] loss: 0.019\nEpoch : 22 - loss : 0.1624 - acc: 0.9421 - Top 1: 75.20899963378906 - Top 5: 98.21855163574219 - Time: 346.1016380786896\n\n[23,   200] loss: 0.012\n[23,   400] loss: 0.015\n[23,   600] loss: 0.016\nEpoch : 23 - loss : 0.1530 - acc: 0.9447 - Top 1: 75.23885345458984 - Top 5: 98.28821563720703 - Time: 346.4702296257019\n\n[24,   200] loss: 0.010\n[24,   400] loss: 0.014\n[24,   600] loss: 0.017\nEpoch : 24 - loss : 0.1415 - acc: 0.9494 - Top 1: 74.28343963623047 - Top 5: 98.1588363647461 - Time: 346.77336025238037\n\n[25,   200] loss: 0.010\n[25,   400] loss: 0.013\n[25,   600] loss: 0.016\nEpoch : 25 - loss : 0.1368 - acc: 0.9502 - Top 1: 74.76114654541016 - Top 5: 98.04936218261719 - Time: 346.35316944122314\n\n[26,   200] loss: 0.011\n[26,   400] loss: 0.013\n[26,   600] loss: 0.016\nEpoch : 26 - loss : 0.1338 - acc: 0.9514 - Top 1: 75.12937927246094 - Top 5: 98.38774108886719 - Time: 346.3864850997925\n\n[27,   200] loss: 0.010\n[27,   400] loss: 0.013\n[27,   600] loss: 0.013\nEpoch : 27 - loss : 0.1266 - acc: 0.9552 - Top 1: 74.65167236328125 - Top 5: 98.25836181640625 - Time: 346.3477249145508\n\n[28,   200] loss: 0.008\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-6b5d52defa9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"plt.subplot(2, 1, 1)\nplt.plot(top1, '.-')\nplt.title('Accuracy')\nplt.ylabel('Top 1 accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(top5, '.-')\nplt.xlabel('epochs')\nplt.ylabel('Top 5 accuracy')\n\nplt.show()\n","metadata":{"id":"6EprELVcE6SX","execution":{"iopub.status.busy":"2021-07-29T16:27:45.951630Z","iopub.execute_input":"2021-07-29T16:27:45.952005Z","iopub.status.idle":"2021-07-29T16:27:46.179082Z","shell.execute_reply.started":"2021-07-29T16:27:45.951966Z","shell.execute_reply":"2021-07-29T16:27:46.178038Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]}]}