{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch, math\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\n!pip install torchsummary\nfrom torchsummary import summary\nimport time\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nimport torch.optim as optim\n!pip install torchsummary\nfrom torchsummary import summary\n!pip install einops\nfrom math import ceil\n!pip install nystrom-attention\n# !pip install performer_pytorch\n\nfrom torch import nn, einsum\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\n\nfrom einops import rearrange, reduce\n\n# helpers\nfrom einops import reduce\n\ntransform = transforms.Compose(\n        [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\nbatch_size = 192\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\ndef accuracy(output, target, topk=(1,5)):\n    \"\"\"Computes the precision@k for the specified values of k\n    prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n    \"\"\"\n    maxk = max(topk)\n         # sizefunction: the number of total elements\n    batch_size = target.size(0) \n \n         # topk function selects the number of k before output\n    _, pred = output.topk(maxk, 1, True, True)\n         ##########Do not understand t()k\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))   \n    res = []\n    for k in topk:\n        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-28T13:49:40.534433Z","iopub.execute_input":"2021-07-28T13:49:40.534799Z","iopub.status.idle":"2021-07-28T13:50:26.747081Z","shell.execute_reply.started":"2021-07-28T13:49:40.534721Z","shell.execute_reply":"2021-07-28T13:50:26.746123Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nRequirement already satisfied: torchsummary in /opt/conda/lib/python3.7/site-packages (1.5.1)\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nCollecting einops\n  Downloading einops-0.3.0-py2.py3-none-any.whl (25 kB)\nInstalling collected packages: einops\nSuccessfully installed einops-0.3.0\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nCollecting nystrom-attention\n  Downloading nystrom_attention-0.0.11-py3-none-any.whl (4.5 kB)\nRequirement already satisfied: torch>=1.6 in /opt/conda/lib/python3.7/site-packages (from nystrom-attention) (1.7.0)\nRequirement already satisfied: einops>=0.3 in /opt/conda/lib/python3.7/site-packages (from nystrom-attention) (0.3.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.6->nystrom-attention) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.6->nystrom-attention) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch>=1.6->nystrom-attention) (0.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch>=1.6->nystrom-attention) (1.19.5)\nInstalling collected packages: nystrom-attention\nSuccessfully installed nystrom-attention-0.0.11\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"044388b93bd643cfbb70ad156fa5eb53"}},"metadata":{}},{"name":"stdout","text":"Extracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"from math import ceil\nimport torch\nfrom torch import nn, einsum\nimport torch.nn.functional as F\n\nfrom einops import rearrange, reduce\n\n# helper functions\n\ndef rotate_every_two(x):\n    x = rearrange(x, '... (d j) -> ... d j', j = 2)\n    x1, x2 = x.unbind(dim = -1)\n    x = torch.stack((-x2, x1), dim = -1)\n    return rearrange(x, '... d j -> ... (d j)')\n\ndef apply_rotary_pos_emb(q, k, sinu_pos):\n    sinu_pos = rearrange(sinu_pos, '() n (j d) -> n j d', j = 2)\n    sin, cos = sinu_pos.unbind(dim = -2)\n\n    sin, cos = map(lambda t: repeat(t, 'b n -> b (n j)', j = 2), (sin, cos))\n    q, k = map(lambda t: (t * cos) + (rotate_every_two(t) * sin), (q, k))\n    return q, k\n\nclass FixedPositionalEmbedding(nn.Module):\n    def __init__(self, dim, max_seq_len):\n        super().__init__()\n        inv_freq = 1. / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n        position = torch.arange(0, max_seq_len, dtype=torch.float)\n        sinusoid_inp = torch.einsum(\"i,j->ij\", position, inv_freq)\n        emb = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n        self.register_buffer('emb', emb)\n\n    def forward(self, x):\n        return self.emb[None, :1088, :].to(x)\n\ndef exists(val):\n    return val is not None\n\ndef moore_penrose_iter_pinv(x, iters = 6):\n    device = x.device\n\n    abs_x = torch.abs(x)\n    col = abs_x.sum(dim = -1)\n    row = abs_x.sum(dim = -2)\n    z = rearrange(x, '... i j -> ... j i') / (torch.max(col) * torch.max(row))\n\n    I = torch.eye(x.shape[-1], device = device)\n    I = rearrange(I, 'i j -> () i j')\n\n    for _ in range(iters):\n        xz = x @ z\n        z = 0.25 * z @ (13 * I - (xz @ (15 * I - (xz @ (7 * I - xz)))))\n\n    return z\n# main attention class\n\nclass NystromAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 4,\n        num_landmarks = 256,\n        pinv_iterations = 6,\n        residual = True,\n        residual_conv_kernel = 33,\n        eps = 1e-8,\n        dropout = 0.\n    ):\n        super().__init__()\n        self.eps = eps\n        inner_dim = heads * dim_head\n\n        self.num_landmarks = num_landmarks\n        self.pinv_iterations = pinv_iterations\n\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim),\n            nn.Dropout(dropout)\n        )\n\n        self.residual = residual\n        if residual:\n            kernel_size = residual_conv_kernel\n            padding = residual_conv_kernel // 2\n            self.res_conv = nn.Conv2d(heads, heads, (kernel_size, 1), padding = (padding, 0), groups = heads, bias = False)\n\n    def forward(self, x, pos_emb = None, mask = None, return_attn = False):\n        b, n, _, h, m, iters, eps = *x.shape, self.heads, self.num_landmarks, self.pinv_iterations, self.eps\n\n        # pad so that sequence can be evenly divided into m landmarks\n\n        remainder = n % m\n        if remainder > 0:\n            padding = m - (n % m)\n            x = F.pad(x, (0, 0, padding, 0), value = 0)\n\n        # derive query, keys, values\n\n        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n\n        \n        if exists(pos_emb):\n            q, k = apply_rotary_pos_emb(q, k, pos_emb)\n\n\n        q = q * self.scale\n\n        # generate landmarks by sum reduction, and then calculate mean using the mask\n\n        l = ceil(n / m)\n        landmark_einops_eq = '... (n l) d -> ... n d'\n        q_landmarks = reduce(q, landmark_einops_eq, 'sum', l = l)\n        k_landmarks = reduce(k, landmark_einops_eq, 'sum', l = l)\n\n        # calculate landmark mask, and also get sum of non-masked elements in preparation for masked mean\n\n        divisor = l\n\n\n        # masked mean (if mask exists)\n\n        q_landmarks /= divisor\n        k_landmarks /= divisor\n\n        # similarities\n\n        einops_eq = '... i d, ... j d -> ... i j'\n        sim1 = einsum(einops_eq, q, k_landmarks)\n        sim2 = einsum(einops_eq, q_landmarks, k_landmarks)\n        sim3 = einsum(einops_eq, q_landmarks, k)\n\n        # masking\n\n        if exists(mask):\n            mask_value = -torch.finfo(q.dtype).max\n            sim1.masked_fill_(~(mask[..., None] * mask_landmarks[..., None, :]), mask_value)\n            sim2.masked_fill_(~(mask_landmarks[..., None] * mask_landmarks[..., None, :]), mask_value)\n            sim3.masked_fill_(~(mask_landmarks[..., None] * mask[..., None, :]), mask_value)\n\n        # eq (15) in the paper and aggregate values\n\n        attn1, attn2, attn3 = map(lambda t: t.softmax(dim = -1), (sim1, sim2, sim3))\n        attn2_inv = moore_penrose_iter_pinv(attn2, iters)\n\n        out = (attn1 @ attn2_inv) @ (attn3 @ v)\n\n        # add depth-wise conv residual of values\n\n        if self.residual:\n            out += self.res_conv(v)\n\n        # merge and combine heads\n\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        out = self.to_out(out)\n        out = out[:, -n:]\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2021-07-28T13:50:26.748853Z","iopub.execute_input":"2021-07-28T13:50:26.749231Z","iopub.status.idle":"2021-07-28T13:50:26.779670Z","shell.execute_reply.started":"2021-07-28T13:50:26.749192Z","shell.execute_reply":"2021-07-28T13:50:26.778674Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn, einsum\nimport torch.nn.functional as F\n\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\n\n# helpers\n\ndef pair(t):\n    return t if isinstance(t, tuple) else (t, t)\n\n# classes\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs)\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout = 0.):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\n    def forward(self, x):\n        return self.net(x)\n\n\nclass Transformer(nn.Module):\n    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        num_landmarks = 64\n        pinv_iterations = 6\n        residual = True\n        residual_conv_kernel = 33\n        eps = 1e-8\n#           self.pos_emb = AxialRotaryEmbedding(dim_head)\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PreNorm(dim, NystromAttention(dim, dim_head = dim_head, num_landmarks =num_landmarks, pinv_iterations =pinv_iterations, residual = residual, residual_conv_kernel = residual_conv_kernel, eps = eps)),\n                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n            ]))\n    def forward(self, x, pos_emb):\n#         pos_emb = self.pos_emb(x[:, 1:])\n        for attn, ff in self.layers:\n            x = attn(x, pos_emb = pos_emb) + x\n            x = ff(x) + x\n        return x\n\nclass ViT(nn.Module):\n    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0., rotary_position_emb = True):\n        super().__init__()\n        image_height, image_width = pair(image_size)\n        patch_height, patch_width = pair(patch_size)\n        self.patch_size = patch_size\n\n        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n\n        num_patches = (image_height // patch_height) * (image_width // patch_width)\n        patch_dim = channels * patch_height * patch_width\n        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n\n        self.to_patch_embedding = nn.Sequential(\n            nn.Conv2d(3, 32, 3, stride = 1, padding = 1),\n            nn.Conv2d(32, 64, 3, stride = 1, padding = 1),\n            nn.Conv2d(64, dim, 3, stride = 1, padding = 1),\n            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width)\n\n        )\n\n#         self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n        self.dropout = nn.Dropout(emb_dropout)\n\n        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n\n        self.pool = pool\n        self.to_latent = nn.Identity()\n\n        self.mlp_head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_classes)\n        )\n        max_seq_len = 1088\n        if rotary_position_emb:\n            self.layer_pos_emb = FixedPositionalEmbedding(dim_head, max_seq_len)\n\n    def forward(self, img):\n        b, _, h, w, p = *img.shape, self.patch_size\n        x = self.to_patch_embedding(img)\n#         b, n, _ = x.shape\n        n = x.shape[1]\n\n        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n        \n        x = torch.cat((cls_tokens, x), dim=1)\n#         x += self.pos_embedding[:, :(n + 1)]\n        x = self.dropout(x)\n#         fmap_dims = {'h': h // p, 'w': w // p}\n        layer_pos_emb = self.layer_pos_emb(x)\n        x = self.transformer(x, pos_emb = layer_pos_emb)\n\n        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n\n        x = self.to_latent(x)\n        return self.mlp_head(x)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T13:50:26.781999Z","iopub.execute_input":"2021-07-28T13:50:26.782380Z","iopub.status.idle":"2021-07-28T13:50:26.807576Z","shell.execute_reply.started":"2021-07-28T13:50:26.782344Z","shell.execute_reply":"2021-07-28T13:50:26.806649Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\nmodel = ViT(\n    image_size = 32,\n    patch_size = 1,\n    num_classes = 10,             # number of stages\n    dim = 128,  # dimensions at each stage\n    depth = 4,              # transformer of depth 4 at each stage\n    heads = 4,      # heads at each stage\n    mlp_dim = 256,\n    dropout = 0.,\n    dim_head = 32\n)\n\n\nmodel.to(device)\nprint(summary(model, (3,32,32)))","metadata":{"execution":{"iopub.status.busy":"2021-07-28T13:50:26.809218Z","iopub.execute_input":"2021-07-28T13:50:26.809795Z","iopub.status.idle":"2021-07-28T13:50:32.189119Z","shell.execute_reply.started":"2021-07-28T13:50:26.809751Z","shell.execute_reply":"2021-07-28T13:50:32.188179Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 32, 32, 32]             896\n            Conv2d-2           [-1, 64, 32, 32]          18,496\n            Conv2d-3          [-1, 128, 32, 32]          73,856\n         Rearrange-4            [-1, 1024, 128]               0\n           Dropout-5            [-1, 1025, 128]               0\nFixedPositionalEmbedding-6             [-1, 1088, 32]               0\n         LayerNorm-7            [-1, 1025, 128]             256\n            Linear-8            [-1, 1088, 384]          49,152\n            Conv2d-9          [-1, 4, 1088, 32]             132\n           Linear-10            [-1, 1088, 128]          16,512\n          Dropout-11            [-1, 1088, 128]               0\n NystromAttention-12            [-1, 1025, 128]               0\n          PreNorm-13            [-1, 1025, 128]               0\n        LayerNorm-14            [-1, 1025, 128]             256\n           Linear-15            [-1, 1025, 256]          33,024\n             GELU-16            [-1, 1025, 256]               0\n          Dropout-17            [-1, 1025, 256]               0\n           Linear-18            [-1, 1025, 128]          32,896\n          Dropout-19            [-1, 1025, 128]               0\n      FeedForward-20            [-1, 1025, 128]               0\n          PreNorm-21            [-1, 1025, 128]               0\n        LayerNorm-22            [-1, 1025, 128]             256\n           Linear-23            [-1, 1088, 384]          49,152\n           Conv2d-24          [-1, 4, 1088, 32]             132\n           Linear-25            [-1, 1088, 128]          16,512\n          Dropout-26            [-1, 1088, 128]               0\n NystromAttention-27            [-1, 1025, 128]               0\n          PreNorm-28            [-1, 1025, 128]               0\n        LayerNorm-29            [-1, 1025, 128]             256\n           Linear-30            [-1, 1025, 256]          33,024\n             GELU-31            [-1, 1025, 256]               0\n          Dropout-32            [-1, 1025, 256]               0\n           Linear-33            [-1, 1025, 128]          32,896\n          Dropout-34            [-1, 1025, 128]               0\n      FeedForward-35            [-1, 1025, 128]               0\n          PreNorm-36            [-1, 1025, 128]               0\n        LayerNorm-37            [-1, 1025, 128]             256\n           Linear-38            [-1, 1088, 384]          49,152\n           Conv2d-39          [-1, 4, 1088, 32]             132\n           Linear-40            [-1, 1088, 128]          16,512\n          Dropout-41            [-1, 1088, 128]               0\n NystromAttention-42            [-1, 1025, 128]               0\n          PreNorm-43            [-1, 1025, 128]               0\n        LayerNorm-44            [-1, 1025, 128]             256\n           Linear-45            [-1, 1025, 256]          33,024\n             GELU-46            [-1, 1025, 256]               0\n          Dropout-47            [-1, 1025, 256]               0\n           Linear-48            [-1, 1025, 128]          32,896\n          Dropout-49            [-1, 1025, 128]               0\n      FeedForward-50            [-1, 1025, 128]               0\n          PreNorm-51            [-1, 1025, 128]               0\n        LayerNorm-52            [-1, 1025, 128]             256\n           Linear-53            [-1, 1088, 384]          49,152\n           Conv2d-54          [-1, 4, 1088, 32]             132\n           Linear-55            [-1, 1088, 128]          16,512\n          Dropout-56            [-1, 1088, 128]               0\n NystromAttention-57            [-1, 1025, 128]               0\n          PreNorm-58            [-1, 1025, 128]               0\n        LayerNorm-59            [-1, 1025, 128]             256\n           Linear-60            [-1, 1025, 256]          33,024\n             GELU-61            [-1, 1025, 256]               0\n          Dropout-62            [-1, 1025, 256]               0\n           Linear-63            [-1, 1025, 128]          32,896\n          Dropout-64            [-1, 1025, 128]               0\n      FeedForward-65            [-1, 1025, 128]               0\n          PreNorm-66            [-1, 1025, 128]               0\n      Transformer-67            [-1, 1025, 128]               0\n         Identity-68                  [-1, 128]               0\n        LayerNorm-69                  [-1, 128]             256\n           Linear-70                   [-1, 10]           1,290\n================================================================\nTotal params: 623,706\nTrainable params: 623,706\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.01\nForward/backward pass size (MB): 86.57\nParams size (MB): 2.38\nEstimated Total Size (MB): 88.97\n----------------------------------------------------------------\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\nscaler = torch.cuda.amp.GradScaler()\n# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\ntop1 = []\ntop5 = []\noptimizer = optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\nfor epoch in range(40):  # loop over the dataset multiple times\n    t0 = time.time()\n    epoch_accuracy = 0\n    epoch_loss = 0\n    running_loss = 0.0\n\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data[0].to(device), data[1].to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        with torch.cuda.amp.autocast():\n            loss = criterion(outputs, labels)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        acc = (outputs.argmax(dim=1) == labels).float().mean()\n        epoch_accuracy += acc / len(trainloader)\n        epoch_loss += loss / len(trainloader)\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 200 == 199:    # print every 2000 mini-batches\n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss / 2000))\n            running_loss = 0.0\n    correct = 0\n    total = 0\n    correct_1=0\n    correct_5=0\n    c = 0\n    with torch.no_grad():\n        for data in testloader:\n            images, labels = data[0].to(device), data[1].to(device)\n            outputs = model(images)\n#         outputs = net(images)\n\n            _, predicted = torch.max(outputs.data, 1)\n            res = accuracy(outputs, labels)\n            correct_1 += res[0][0].float()\n            correct_5 += res[1][0].float()\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            c += 1\n        \n    print(f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - Top 1: {correct_1/c} - Top 5: {correct_5/c} - Time: {time.time() - t0}\\n\")\n    top1.append(correct_1/c)\n    top5.append(correct_5/c)\n    if float(correct_1/c) >= float(max(top1)):\n        PATH = 'ViNCNNRoPE.pth'\n        torch.save(model.state_dict(), PATH)\n        print(1)\nprint('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2021-07-28T13:50:32.191005Z","iopub.execute_input":"2021-07-28T13:50:32.191432Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"[1,   200] loss: 0.166\nEpoch : 1 - loss : 1.5908 - acc: 0.4193 - Top 1: 52.0538444519043 - Top 5: 93.91706848144531 - Time: 229.1003885269165\n\n1\n[2,   200] loss: 0.122\nEpoch : 2 - loss : 1.1980 - acc: 0.5664 - Top 1: 60.043235778808594 - Top 5: 95.80384826660156 - Time: 229.30970287322998\n\n1\n[3,   200] loss: 0.101\nEpoch : 3 - loss : 1.0018 - acc: 0.6430 - Top 1: 63.40408706665039 - Top 5: 96.47207641601562 - Time: 229.60699439048767\n\n1\n[4,   200] loss: 0.088\nEpoch : 4 - loss : 0.8767 - acc: 0.6875 - Top 1: 66.49960327148438 - Top 5: 97.15998077392578 - Time: 229.65705633163452\n\n1\n[5,   200] loss: 0.078\nEpoch : 5 - loss : 0.7828 - acc: 0.7230 - Top 1: 68.50431823730469 - Top 5: 97.5432357788086 - Time: 229.71101689338684\n\n1\n[6,   200] loss: 0.071\nEpoch : 6 - loss : 0.7090 - acc: 0.7494 - Top 1: 70.49921417236328 - Top 5: 97.9363021850586 - Time: 229.26595854759216\n\n1\n[7,   200] loss: 0.064\nEpoch : 7 - loss : 0.6493 - acc: 0.7690 - Top 1: 71.2657241821289 - Top 5: 98.11322021484375 - Time: 229.34841775894165\n\n1\n[8,   200] loss: 0.058\nEpoch : 8 - loss : 0.5869 - acc: 0.7902 - Top 1: 70.73507690429688 - Top 5: 98.00511169433594 - Time: 229.6662278175354\n\n[9,   200] loss: 0.053\nEpoch : 9 - loss : 0.5338 - acc: 0.8108 - Top 1: 72.21894073486328 - Top 5: 97.79875946044922 - Time: 229.6003212928772\n\n1\n[10,   200] loss: 0.047\nEpoch : 10 - loss : 0.4832 - acc: 0.8287 - Top 1: 72.8085708618164 - Top 5: 98.19181060791016 - Time: 229.6566038131714\n\n1\n[11,   200] loss: 0.042\nEpoch : 11 - loss : 0.4342 - acc: 0.8447 - Top 1: 72.80857849121094 - Top 5: 98.21147918701172 - Time: 229.5496530532837\n\n1\n[12,   200] loss: 0.038\nEpoch : 12 - loss : 0.3923 - acc: 0.8601 - Top 1: 71.77672576904297 - Top 5: 97.82821655273438 - Time: 229.8141508102417\n\n[13,   200] loss: 0.034\nEpoch : 13 - loss : 0.3483 - acc: 0.8765 - Top 1: 72.13050842285156 - Top 5: 98.0444107055664 - Time: 229.6297345161438\n\n[14,   200] loss: 0.030\nEpoch : 14 - loss : 0.3094 - acc: 0.8895 - Top 1: 71.4229507446289 - Top 5: 97.75944519042969 - Time: 229.56687140464783\n\n[15,   200] loss: 0.026\nEpoch : 15 - loss : 0.2722 - acc: 0.9036 - Top 1: 72.93630981445312 - Top 5: 97.8478775024414 - Time: 229.58213686943054\n\n1\n[16,   200] loss: 0.024\nEpoch : 16 - loss : 0.2505 - acc: 0.9102 - Top 1: 72.3663558959961 - Top 5: 97.78892517089844 - Time: 229.53164219856262\n\n[17,   200] loss: 0.021\nEpoch : 17 - loss : 0.2189 - acc: 0.9219 - Top 1: 72.50393676757812 - Top 5: 97.73977661132812 - Time: 229.28740072250366\n\n[18,   200] loss: 0.019\nEpoch : 18 - loss : 0.2028 - acc: 0.9285 - Top 1: 72.52359771728516 - Top 5: 97.86752319335938 - Time: 229.59633469581604\n\n[19,   200] loss: 0.017\nEpoch : 19 - loss : 0.1790 - acc: 0.9352 - Top 1: 72.4056625366211 - Top 5: 97.62184143066406 - Time: 229.68560194969177\n\n[20,   200] loss: 0.015\nEpoch : 20 - loss : 0.1549 - acc: 0.9444 - Top 1: 72.6906509399414 - Top 5: 97.85769653320312 - Time: 229.75966930389404\n\n","output_type":"stream"}]},{"cell_type":"code","source":"plt.subplot(2, 1, 1)\nplt.plot(top1[0:14], '.-')\nplt.title('Accuracy')\nplt.ylabel('Top 1 accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(top5[0:14], '.-')\nplt.xlabel('epochs')\nplt.ylabel('Top 5 accuracy')\n\nplt.show()\nnp.shape(top1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}