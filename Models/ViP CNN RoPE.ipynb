{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch, math\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\n!pip install torchsummary\nfrom torchsummary import summary\nimport time\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nimport torch.optim as optim\n!pip install torchsummary\nfrom torchsummary import summary\n!pip install einops\nfrom math import ceil\n# !pip install nystrom-attention\n!pip install performer_pytorch\n\nfrom torch import nn, einsum\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\n\nfrom einops import rearrange, reduce\n\n# helpers\nfrom einops import reduce\n\ntransform = transforms.Compose(\n        [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\nbatch_size = 192\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\ndef accuracy(output, target, topk=(1,5)):\n    \"\"\"Computes the precision@k for the specified values of k\n    prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n    \"\"\"\n    maxk = max(topk)\n         # sizefunction: the number of total elements\n    batch_size = target.size(0) \n \n         # topk function selects the number of k before output\n    _, pred = output.topk(maxk, 1, True, True)\n         ##########Do not understand t()k\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))   \n    res = []\n    for k in topk:\n        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-28T05:39:08.606460Z","iopub.execute_input":"2021-07-28T05:39:08.606817Z","iopub.status.idle":"2021-07-28T05:39:45.719038Z","shell.execute_reply.started":"2021-07-28T05:39:08.606740Z","shell.execute_reply":"2021-07-28T05:39:45.718040Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nRequirement already satisfied: torchsummary in /opt/conda/lib/python3.7/site-packages (1.5.1)\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nCollecting einops\n  Downloading einops-0.3.0-py2.py3-none-any.whl (25 kB)\nInstalling collected packages: einops\nSuccessfully installed einops-0.3.0\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nCollecting performer_pytorch\n  Downloading performer_pytorch-1.0.11-py3-none-any.whl (12 kB)\nCollecting axial-positional-embedding>=0.1.0\n  Downloading axial_positional_embedding-0.2.1.tar.gz (2.6 kB)\nRequirement already satisfied: torch>=1.6 in /opt/conda/lib/python3.7/site-packages (from performer_pytorch) (1.7.0)\nRequirement already satisfied: einops>=0.3 in /opt/conda/lib/python3.7/site-packages (from performer_pytorch) (0.3.0)\nCollecting local-attention>=1.1.1\n  Downloading local_attention-1.4.3-py3-none-any.whl (5.0 kB)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.6->performer_pytorch) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.6->performer_pytorch) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch>=1.6->performer_pytorch) (0.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch>=1.6->performer_pytorch) (1.19.5)\nBuilding wheels for collected packages: axial-positional-embedding\n  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-py3-none-any.whl size=2902 sha256=b52d49a3afed2bfa612c3b60d45bfceabc583bfd5cc2751896b01805551431f3\n  Stored in directory: /root/.cache/pip/wheels/4a/2c/c3/9a1cb267c0d0d9b6eeba7952addb32b17857d1f799690c27a8\nSuccessfully built axial-positional-embedding\nInstalling collected packages: local-attention, axial-positional-embedding, performer-pytorch\nSuccessfully installed axial-positional-embedding-0.2.1 local-attention-1.4.3 performer-pytorch-1.0.11\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad32ac74801d41d8a28a3c62866b6827"}},"metadata":{}},{"name":"stdout","text":"Extracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.cuda.amp import autocast\nfrom functools import partial\nfrom contextlib import contextmanager\nfrom local_attention import LocalAttention\nfrom axial_positional_embedding import AxialPositionalEmbedding\nfrom performer_pytorch.reversible import ReversibleSequence, SequentialSequence\n\ndef exists(val):\n    return val is not None\n\ndef empty(tensor):\n    return tensor.numel() == 0\n\ndef default(val, d):\n    return val if exists(val) else d\n\n@contextmanager\ndef null_context():\n    yield\n\ndef rotate_every_two(x):\n    x = rearrange(x, '... (d j) -> ... d j', j = 2)\n    x1, x2 = x.unbind(dim = -1)\n    x = torch.stack((-x2, x1), dim = -1)\n    return rearrange(x, '... d j -> ... (d j)')\n\ndef apply_rotary_pos_emb(q, k, sinu_pos):\n    sinu_pos = rearrange(sinu_pos, '() n (j d) -> n j d', j = 2)\n    sin, cos = sinu_pos.unbind(dim = -2)\n\n    sin, cos = map(lambda t: repeat(t, 'b n -> b (n j)', j = 2), (sin, cos))\n    q, k = map(lambda t: (t * cos) + (rotate_every_two(t) * sin), (q, k))\n    return q, k\n\nclass FixedPositionalEmbedding(nn.Module):\n    def __init__(self, dim, max_seq_len):\n        super().__init__()\n        inv_freq = 1. / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n        position = torch.arange(0, max_seq_len, dtype=torch.float)\n        sinusoid_inp = torch.einsum(\"i,j->ij\", position, inv_freq)\n        emb = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n        self.register_buffer('emb', emb)\n\n    def forward(self, x):\n        return self.emb[None, :x.shape[1], :].to(x)\n# def cast_tuple(val):\n#     return (val,) if not isinstance(val, tuple) else val\n\ndef get_module_device(module):\n    return next(module.parameters()).device\n\ndef find_modules(nn_module, type):\n    return [module for module in nn_module.modules() if isinstance(module, type)]\n\nclass Always(nn.Module):\n    def __init__(self, val):\n        super().__init__()\n        self.val = val\n\n    def forward(self, *args, **kwargs):\n        return self.val\n\n# kernel functions\n\n# transcribed from jax to pytorch from\n# https://github.com/google-research/google-research/blob/master/performer/fast_attention/jax/fast_attention.py\n\ndef softmax_kernel(data, *, projection_matrix, is_query, normalize_data=True, eps=1e-4, device = None):\n    b, h, *_ = data.shape\n\n    data_normalizer = (data.shape[-1] ** -0.25) if normalize_data else 1.\n\n    ratio = (projection_matrix.shape[0] ** -0.5)\n\n    projection = repeat(projection_matrix, 'j d -> b h j d', b = b, h = h)\n    projection = projection.type_as(data)\n\n    data_dash = torch.einsum('...id,...jd->...ij', (data_normalizer * data), projection)\n\n    diag_data = data ** 2\n    diag_data = torch.sum(diag_data, dim=-1)\n    diag_data = (diag_data / 2.0) * (data_normalizer ** 2)\n    diag_data = diag_data.unsqueeze(dim=-1)\n\n    if is_query:\n        data_dash = ratio * (\n            torch.exp(data_dash - diag_data -\n                    torch.max(data_dash, dim=-1, keepdim=True).values) + eps)\n    else:\n        data_dash = ratio * (\n            torch.exp(data_dash - diag_data - torch.max(data_dash)) + eps)\n\n    return data_dash.type_as(data)\n\ndef generalized_kernel(data, *, projection_matrix, kernel_fn = nn.ReLU(), kernel_epsilon = 0.001, normalize_data = True, device = None):\n    b, h, *_ = data.shape\n\n    data_normalizer = (data.shape[-1] ** -0.25) if normalize_data else 1.\n\n    if projection_matrix is None:\n        return kernel_fn(data_normalizer * data) + kernel_epsilon\n\n    projection = repeat(projection_matrix, 'j d -> b h j d', b = b, h = h)\n    projection = projection.type_as(data)\n\n    data_dash = torch.einsum('...id,...jd->...ij', (data_normalizer * data), projection)\n\n    data_prime = kernel_fn(data_dash) + kernel_epsilon\n    return data_prime.type_as(data)\n\ndef orthogonal_matrix_chunk(cols, device = None):\n    unstructured_block = torch.randn((cols, cols), device = device)\n    q, r = torch.qr(unstructured_block.cpu(), some = True)\n    q, r = map(lambda t: t.to(device), (q, r))\n    return q.t()\n\ndef gaussian_orthogonal_random_matrix(nb_rows, nb_columns, scaling = 0, device = None):\n    nb_full_blocks = int(nb_rows / nb_columns)\n\n    block_list = []\n\n    for _ in range(nb_full_blocks):\n        q = orthogonal_matrix_chunk(nb_columns, device = device)\n        block_list.append(q)\n\n    remaining_rows = nb_rows - nb_full_blocks * nb_columns\n    if remaining_rows > 0:\n        q = orthogonal_matrix_chunk(nb_columns, device = device)\n        block_list.append(q[:remaining_rows])\n\n    final_matrix = torch.cat(block_list)\n\n    if scaling == 0:\n        multiplier = torch.randn((nb_rows, nb_columns), device = device).norm(dim = 1)\n    elif scaling == 1:\n        multiplier = math.sqrt((float(nb_columns))) * torch.ones((nb_rows,), device = device)\n    else:\n        raise ValueError(f'Invalid scaling {scaling}')\n\n    return torch.diag(multiplier) @ final_matrix\n\n# linear attention classes with softmax kernel\n\n# non-causal linear attention\ndef linear_attention(q, k, v):\n    k_cumsum = k.sum(dim = -2)\n    D_inv = 1. / torch.einsum('...nd,...d->...n', q, k_cumsum.type_as(q))\n    context = torch.einsum('...nd,...ne->...de', k, v)\n    out = torch.einsum('...de,...nd,...n->...ne', context, q, D_inv)\n#     print(\"linear attention\", out.size)\n    return out\n\nclass FastAttention(nn.Module):\n    def __init__(self, dim_heads, nb_features = None, ortho_scaling = 0, causal = False, generalized_attention = False, kernel_fn = nn.ReLU(), no_projection = False):\n        super().__init__()\n        nb_features = default(nb_features, int(dim_heads * math.log(dim_heads)))\n\n        self.dim_heads = dim_heads\n        self.nb_features = nb_features\n        self.ortho_scaling = ortho_scaling\n\n        self.create_projection = partial(gaussian_orthogonal_random_matrix, nb_rows = self.nb_features, nb_columns = dim_heads, scaling = ortho_scaling)\n        projection_matrix = self.create_projection()\n        self.register_buffer('projection_matrix', projection_matrix)\n\n        self.generalized_attention = generalized_attention\n        self.kernel_fn = kernel_fn\n\n        # if this is turned on, no projection will be used\n        # queries and keys will be softmax-ed as in the original efficient attention paper\n        self.no_projection = no_projection\n\n        self.causal = causal\n        \n    @torch.no_grad()\n    def redraw_projection_matrix(self, device):\n        projections = self.create_projection(device = device)\n        self.projection_matrix.copy_(projections)\n        del projections\n\n    def forward(self, q, k, v):\n        device = q.device\n        \n        if self.no_projection:\n            q = q.softmax(dim = -1)\n            k = torch.exp(k) if self.causal else k.softmax(dim = -2)\n\n        elif self.generalized_attention:\n            create_kernel = partial(generalized_kernel, kernel_fn = self.kernel_fn, projection_matrix = self.projection_matrix, device = device)\n            q, k = map(create_kernel, (q, k))\n\n        else:\n            create_kernel = partial(softmax_kernel, projection_matrix = self.projection_matrix, device = device)\n            q = create_kernel(q, is_query = True)\n            k = create_kernel(k, is_query = False)\n\n        attn_fn = linear_attention if not self.causal else self.causal_linear_fn\n        out = attn_fn(q, k, v)\n#         print('fastattention', out.size())\n        return out\n\n# a module for keeping track of when to update the projections\n\nclass ProjectionUpdater(nn.Module):\n    def __init__(self, instance, feature_redraw_interval):\n        super().__init__()\n        self.instance = instance\n        self.feature_redraw_interval = feature_redraw_interval\n        self.register_buffer('calls_since_last_redraw', torch.tensor(0))\n\n    def fix_projections_(self):\n        self.feature_redraw_interval = None\n\n    def redraw_projections(self):\n        model = self.instance\n\n        if not self.training:\n            return\n\n        if exists(self.feature_redraw_interval) and self.calls_since_last_redraw >= self.feature_redraw_interval:\n            device = get_module_device(model)\n\n            fast_attentions = find_modules(model, FastAttention)\n            for fast_attention in fast_attentions:\n                fast_attention.redraw_projection_matrix(device)\n\n            self.calls_since_last_redraw.zero_()\n            return\n\n        self.calls_since_last_redraw += 1\n\n    def forward(self, x):\n        raise NotImplemented\n\n# classes\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        causal = False,\n        heads = 4,\n        dim_head = 32,\n        local_heads = 0,\n        local_window_size = 256,\n        nb_features = None,\n        feature_redraw_interval = 1000,\n        generalized_attention = False,\n        kernel_fn = nn.ReLU(),\n        dropout = 0.,\n        no_projection = False,\n        qkv_bias = False,\n        attn_out_bias = True\n    ):\n        super().__init__()\n        assert dim % heads == 0, 'dimension must be divisible by number of heads'\n        dim_head = default(dim_head, dim // heads)\n        inner_dim = dim_head * heads\n        self.fast_attention = FastAttention(dim_head, nb_features, causal = causal, generalized_attention = generalized_attention, kernel_fn = kernel_fn, no_projection = no_projection)\n\n        self.heads = heads\n        self.global_heads = heads - local_heads\n        self.local_attn = LocalAttention(window_size = local_window_size, causal = causal, autopad = True, dropout = dropout, look_forward = int(not causal), rel_pos_emb_config = (dim_head, local_heads)) if local_heads > 0 else None\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = qkv_bias)\n        self.to_k = nn.Linear(dim, inner_dim, bias = qkv_bias)\n        self.to_v = nn.Linear(dim, inner_dim, bias = qkv_bias)\n        self.to_out = nn.Linear(inner_dim, dim, bias = attn_out_bias)\n        self.dropout = nn.Dropout(dropout)\n        \n\n    def forward(self, x, pos_emb = None, context = None, mask = None, context_mask = None, **kwargs):\n        \n        b, n, _, h, gh = *x.shape, self.heads, self.global_heads\n\n        cross_attend = exists(context)\n\n        context = default(context, x)\n        context_mask = default(context_mask, mask) if not cross_attend else context_mask\n\n        q, k, v = self.to_q(x), self.to_k(context), self.to_v(context)\n\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n        (q, lq), (k, lk), (v, lv) = map(lambda t: (t[:, :gh], t[:, gh:]), (q, k, v))\n\n        attn_outs = []\n\n        if not empty(q):\n            if exists(context_mask):\n                global_mask = context_mask[:, None, :, None]\n                v.masked_fill_(~global_mask, 0.)\n\n            if exists(pos_emb) and not cross_attend:\n                q, k = apply_rotary_pos_emb(q, k, pos_emb)\n                \n\n            out = self.fast_attention(q, k, v)\n            attn_outs.append(out)\n\n\n        if not empty(lq):\n            assert not cross_attend, 'local attention is not compatible with cross attention'\n            out = self.local_attn(lq, lk, lv, input_mask = mask)\n            attn_outs.append(out)\n\n        out = torch.cat(attn_outs, dim = 1)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n#         print(\"Attention\", out.size())\n        out =  self.to_out(out)\n        out = self.dropout(out)\n        return out\n\n\nclass SelfAttention(Attention):\n    def forward(self, *args, pos_emb, context = None, **kwargs):\n        assert not exists(context), 'self attention should not receive context'\n#         print(1, \"self attention module\")\n        return super().forward(*args, pos_emb, **kwargs)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T05:39:45.721040Z","iopub.execute_input":"2021-07-28T05:39:45.721519Z","iopub.status.idle":"2021-07-28T05:39:45.814984Z","shell.execute_reply.started":"2021-07-28T05:39:45.721482Z","shell.execute_reply":"2021-07-28T05:39:45.814239Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn, einsum\nimport torch.nn.functional as F\n\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\n\n# helpers\n\ndef pair(t):\n    return t if isinstance(t, tuple) else (t, t)\n\n# classes\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs)\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout = 0.):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\n    def forward(self, x):\n        return self.net(x)\n\n\nclass Transformer(nn.Module):\n    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        local_attn_heads = 0\n        local_window_size = 256\n        causal = False\n        nb_features = None\n        generalized_attention = False\n        kernel_fn = nn.ReLU()\n        attn_dropout = 0.\n        no_projection = False\n        qkv_bias = True\n        attn_out_bias = True\n#           self.pos_emb = AxialRotaryEmbedding(dim_head)\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PreNorm(dim, SelfAttention(dim, causal = causal, heads = heads, dim_head = dim_head, local_heads = local_attn_heads, local_window_size = local_window_size, nb_features = nb_features, generalized_attention = generalized_attention, kernel_fn = kernel_fn, dropout = attn_dropout, no_projection = no_projection, qkv_bias = qkv_bias, attn_out_bias = attn_out_bias)),\n                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n            ]))\n    def forward(self, x, pos_emb):\n#         pos_emb = self.pos_emb(x[:, 1:])\n        for attn, ff in self.layers:\n            x = attn(x, pos_emb = pos_emb) + x\n            x = ff(x) + x\n        return x\n\nclass ViT(nn.Module):\n    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0., rotary_position_emb = True):\n        super().__init__()\n        image_height, image_width = pair(image_size)\n        patch_height, patch_width = pair(patch_size)\n        self.patch_size = patch_size\n\n        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n\n        num_patches = (image_height // patch_height) * (image_width // patch_width)\n        patch_dim = channels * patch_height * patch_width\n        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n\n        self.to_patch_embedding = nn.Sequential(\n            nn.Conv2d(3, 32, 3, stride = 1, padding = 1),\n            nn.Conv2d(32, 64, 3, stride = 1, padding = 1),\n            nn.Conv2d(64, dim, 3, stride = 1, padding = 1),\n            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width)\n\n        )\n\n#         self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n        self.dropout = nn.Dropout(emb_dropout)\n\n        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n\n        self.pool = pool\n        self.to_latent = nn.Identity()\n\n        self.mlp_head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_classes)\n        )\n        max_seq_len = 1025\n        if rotary_position_emb:\n            self.layer_pos_emb = FixedPositionalEmbedding(dim_head, max_seq_len)\n\n    def forward(self, img):\n        b, _, h, w, p = *img.shape, self.patch_size\n        x = self.to_patch_embedding(img)\n#         b, n, _ = x.shape\n        n = x.shape[1]\n\n        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n        \n        x = torch.cat((cls_tokens, x), dim=1)\n#         x += self.pos_embedding[:, :(n + 1)]\n        x = self.dropout(x)\n#         fmap_dims = {'h': h // p, 'w': w // p}\n        layer_pos_emb = self.layer_pos_emb(x)\n        x = self.transformer(x, pos_emb = layer_pos_emb)\n\n        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n\n        x = self.to_latent(x)\n        return self.mlp_head(x)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T05:39:45.817524Z","iopub.execute_input":"2021-07-28T05:39:45.819595Z","iopub.status.idle":"2021-07-28T05:39:45.862501Z","shell.execute_reply.started":"2021-07-28T05:39:45.819557Z","shell.execute_reply":"2021-07-28T05:39:45.861413Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\nmodel = ViT(\n    image_size = 32,\n    patch_size = 1,\n    num_classes = 10,             # number of stages\n    dim = 128,  # dimensions at each stage\n    depth = 4,              # transformer of depth 4 at each stage\n    heads = 4,      # heads at each stage\n    mlp_dim = 256,\n    dropout = 0.,\n    dim_head = 32\n)\n\n\nmodel.to(device)\nprint(summary(model, (3,32,32)))","metadata":{"execution":{"iopub.status.busy":"2021-07-28T05:39:45.870035Z","iopub.execute_input":"2021-07-28T05:39:45.872304Z","iopub.status.idle":"2021-07-28T05:39:51.258878Z","shell.execute_reply.started":"2021-07-28T05:39:45.872223Z","shell.execute_reply":"2021-07-28T05:39:51.257945Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 32, 32, 32]             896\n            Conv2d-2           [-1, 64, 32, 32]          18,496\n            Conv2d-3          [-1, 128, 32, 32]          73,856\n         Rearrange-4            [-1, 1024, 128]               0\n           Dropout-5            [-1, 1025, 128]               0\nFixedPositionalEmbedding-6             [-1, 1025, 32]               0\n         LayerNorm-7            [-1, 1025, 128]             256\n            Linear-8            [-1, 1025, 128]          16,512\n            Linear-9            [-1, 1025, 128]          16,512\n           Linear-10            [-1, 1025, 128]          16,512\n    FastAttention-11          [-1, 4, 1025, 32]               0\n           Linear-12            [-1, 1025, 128]          16,512\n          Dropout-13            [-1, 1025, 128]               0\n    SelfAttention-14            [-1, 1025, 128]               0\n          PreNorm-15            [-1, 1025, 128]               0\n        LayerNorm-16            [-1, 1025, 128]             256\n           Linear-17            [-1, 1025, 256]          33,024\n             GELU-18            [-1, 1025, 256]               0\n          Dropout-19            [-1, 1025, 256]               0\n           Linear-20            [-1, 1025, 128]          32,896\n          Dropout-21            [-1, 1025, 128]               0\n      FeedForward-22            [-1, 1025, 128]               0\n          PreNorm-23            [-1, 1025, 128]               0\n        LayerNorm-24            [-1, 1025, 128]             256\n           Linear-25            [-1, 1025, 128]          16,512\n           Linear-26            [-1, 1025, 128]          16,512\n           Linear-27            [-1, 1025, 128]          16,512\n    FastAttention-28          [-1, 4, 1025, 32]               0\n           Linear-29            [-1, 1025, 128]          16,512\n          Dropout-30            [-1, 1025, 128]               0\n    SelfAttention-31            [-1, 1025, 128]               0\n          PreNorm-32            [-1, 1025, 128]               0\n        LayerNorm-33            [-1, 1025, 128]             256\n           Linear-34            [-1, 1025, 256]          33,024\n             GELU-35            [-1, 1025, 256]               0\n          Dropout-36            [-1, 1025, 256]               0\n           Linear-37            [-1, 1025, 128]          32,896\n          Dropout-38            [-1, 1025, 128]               0\n      FeedForward-39            [-1, 1025, 128]               0\n          PreNorm-40            [-1, 1025, 128]               0\n        LayerNorm-41            [-1, 1025, 128]             256\n           Linear-42            [-1, 1025, 128]          16,512\n           Linear-43            [-1, 1025, 128]          16,512\n           Linear-44            [-1, 1025, 128]          16,512\n    FastAttention-45          [-1, 4, 1025, 32]               0\n           Linear-46            [-1, 1025, 128]          16,512\n          Dropout-47            [-1, 1025, 128]               0\n    SelfAttention-48            [-1, 1025, 128]               0\n          PreNorm-49            [-1, 1025, 128]               0\n        LayerNorm-50            [-1, 1025, 128]             256\n           Linear-51            [-1, 1025, 256]          33,024\n             GELU-52            [-1, 1025, 256]               0\n          Dropout-53            [-1, 1025, 256]               0\n           Linear-54            [-1, 1025, 128]          32,896\n          Dropout-55            [-1, 1025, 128]               0\n      FeedForward-56            [-1, 1025, 128]               0\n          PreNorm-57            [-1, 1025, 128]               0\n        LayerNorm-58            [-1, 1025, 128]             256\n           Linear-59            [-1, 1025, 128]          16,512\n           Linear-60            [-1, 1025, 128]          16,512\n           Linear-61            [-1, 1025, 128]          16,512\n    FastAttention-62          [-1, 4, 1025, 32]               0\n           Linear-63            [-1, 1025, 128]          16,512\n          Dropout-64            [-1, 1025, 128]               0\n    SelfAttention-65            [-1, 1025, 128]               0\n          PreNorm-66            [-1, 1025, 128]               0\n        LayerNorm-67            [-1, 1025, 128]             256\n           Linear-68            [-1, 1025, 256]          33,024\n             GELU-69            [-1, 1025, 256]               0\n          Dropout-70            [-1, 1025, 256]               0\n           Linear-71            [-1, 1025, 128]          32,896\n          Dropout-72            [-1, 1025, 128]               0\n      FeedForward-73            [-1, 1025, 128]               0\n          PreNorm-74            [-1, 1025, 128]               0\n      Transformer-75            [-1, 1025, 128]               0\n         Identity-76                  [-1, 128]               0\n        LayerNorm-77                  [-1, 128]             256\n           Linear-78                   [-1, 10]           1,290\n================================================================\nTotal params: 624,714\nTrainable params: 624,714\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.01\nForward/backward pass size (MB): 85.08\nParams size (MB): 2.38\nEstimated Total Size (MB): 87.48\n----------------------------------------------------------------\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\nscaler = torch.cuda.amp.GradScaler()\n# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\ntop1 = []\ntop5 = []\noptimizer = optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\nfor epoch in range(40):  # loop over the dataset multiple times\n    t0 = time.time()\n    epoch_accuracy = 0\n    epoch_loss = 0\n    running_loss = 0.0\n\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data[0].to(device), data[1].to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        with torch.cuda.amp.autocast():\n            loss = criterion(outputs, labels)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        acc = (outputs.argmax(dim=1) == labels).float().mean()\n        epoch_accuracy += acc / len(trainloader)\n        epoch_loss += loss / len(trainloader)\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 200 == 199:    # print every 2000 mini-batches\n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss / 2000))\n            running_loss = 0.0\n    correct = 0\n    total = 0\n    correct_1=0\n    correct_5=0\n    c = 0\n    with torch.no_grad():\n        for data in testloader:\n            images, labels = data[0].to(device), data[1].to(device)\n            outputs = model(images)\n#         outputs = net(images)\n\n            _, predicted = torch.max(outputs.data, 1)\n            res = accuracy(outputs, labels)\n            correct_1 += res[0][0].float()\n            correct_5 += res[1][0].float()\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            c += 1\n        \n    print(f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - Top 1: {correct_1/c} - Top 5: {correct_5/c} - Time: {time.time() - t0}\\n\")\n    top1.append(correct_1/c)\n    top5.append(correct_5/c)\n    if float(correct_1/c) >= float(max(top1)):\n        PATH = 'ViPCNNRoPE.pth'\n        torch.save(model.state_dict(), PATH)\n        print(1)\nprint('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:33:29.512865Z","iopub.execute_input":"2021-07-28T08:33:29.513498Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"[1,   200] loss: 0.010\nEpoch : 1 - loss : 0.1038 - acc: 0.9628 - Top 1: 75.61910247802734 - Top 5: 98.2213134765625 - Time: 239.60823917388916\n\n1\n[2,   200] loss: 0.010\nEpoch : 2 - loss : 0.1042 - acc: 0.9630 - Top 1: 75.76651763916016 - Top 5: 98.123046875 - Time: 239.53121662139893\n\n1\n[3,   200] loss: 0.008\nEpoch : 3 - loss : 0.0880 - acc: 0.9688 - Top 1: 76.55267333984375 - Top 5: 98.37854766845703 - Time: 239.38190150260925\n\n1\n[4,   200] loss: 0.007\nEpoch : 4 - loss : 0.0748 - acc: 0.9735 - Top 1: 75.99252319335938 - Top 5: 98.30974578857422 - Time: 239.4769036769867\n\n","output_type":"stream"}]},{"cell_type":"code","source":"plt.subplot(2, 1, 1)\nplt.plot(top1[0:14], '.-')\nplt.title('Accuracy')\nplt.ylabel('Top 1 accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(top5[0:14], '.-')\nplt.xlabel('epochs')\nplt.ylabel('Top 5 accuracy')\n\nplt.show()\nnp.shape(top1)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:19:26.991626Z","iopub.execute_input":"2021-07-28T08:19:26.992187Z","iopub.status.idle":"2021-07-28T08:19:27.311426Z","shell.execute_reply.started":"2021-07-28T08:19:26.992144Z","shell.execute_reply":"2021-07-28T08:19:27.310485Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(40,)"},"metadata":{}}]}]}