{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch, math\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\n!pip install torchsummary\nfrom torchsummary import summary\nimport time\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nimport torch.optim as optim\n!pip install torchsummary\nfrom torchsummary import summary\n!pip install einops\nfrom math import ceil\n# !pip install nystrom-attention\n# !pip install performer_pytorch\n!pip install linformer\nfrom torch import nn, einsum\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\n\nfrom einops import rearrange, reduce\n\n# helpers\nfrom einops import reduce\n\ntransform = transforms.Compose(\n        [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\nbatch_size = 256\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\ndef accuracy(output, target, topk=(1,5)):\n    \"\"\"Computes the precision@k for the specified values of k\n    prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n    \"\"\"\n    maxk = max(topk)\n         # sizefunction: the number of total elements\n    batch_size = target.size(0) \n \n         # topk function selects the number of k before output\n    _, pred = output.topk(maxk, 1, True, True)\n         ##########Do not understand t()k\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))   \n    res = []\n    for k in topk:\n        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"BmgrxU_RFfaL","outputId":"45a30594-520c-461d-87d2-679f8ad3d496","execution":{"iopub.status.busy":"2021-07-28T20:42:45.903439Z","iopub.execute_input":"2021-07-28T20:42:45.903815Z","iopub.status.idle":"2021-07-28T20:43:25.313073Z","shell.execute_reply.started":"2021-07-28T20:42:45.903730Z","shell.execute_reply":"2021-07-28T20:43:25.312105Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nRequirement already satisfied: torchsummary in /opt/conda/lib/python3.7/site-packages (1.5.1)\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nCollecting einops\n  Downloading einops-0.3.0-py2.py3-none-any.whl (25 kB)\nInstalling collected packages: einops\nSuccessfully installed einops-0.3.0\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nCollecting linformer\n  Downloading linformer-0.2.1-py3-none-any.whl (6.1 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from linformer) (1.7.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->linformer) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch->linformer) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch->linformer) (0.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch->linformer) (1.19.5)\nInstalling collected packages: linformer\nSuccessfully installed linformer-0.2.1\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cfed545e5db4729893e124e0f37b32d"}},"metadata":{}},{"name":"stdout","text":"Extracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"import math\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom linformer.reversible import ReversibleSequence, SequentialSequence\n\n\ndef exists(val):\n    return val is not None\n\ndef default(val, default_val):\n    return val if val is not None else default_val\n\ndef init_(tensor):\n    dim = tensor.shape[-1]\n    std = 1 / math.sqrt(dim)\n    tensor.uniform_(-std, std)\n    return tensor\n\nclass LinformerSelfAttention(nn.Module):\n    def __init__(self, dim, seq_len, k = 256, heads = 8, dim_head = None, one_kv_head = False, share_kv = False, dropout = 0.):\n        super().__init__()\n        assert (dim % heads) == 0, 'dimension must be divisible by the number of heads'\n\n        self.seq_len = seq_len\n        self.k = k\n\n        self.heads = heads\n\n        dim_head = default(dim_head, dim // heads)\n        self.dim_head = dim_head\n\n        self.to_q = nn.Linear(dim, dim_head * heads, bias = False)\n\n        kv_dim = dim_head if one_kv_head else (dim_head * heads)\n        self.to_k = nn.Linear(dim, kv_dim, bias = False)\n        self.proj_k = nn.Parameter(init_(torch.zeros(seq_len, k)))\n\n        self.share_kv = share_kv\n        if not share_kv:\n            self.to_v = nn.Linear(dim, kv_dim, bias = False)\n            self.proj_v = nn.Parameter(init_(torch.zeros(seq_len, k)))\n\n        self.dropout = nn.Dropout(dropout)\n        self.to_out = nn.Linear(dim_head * heads, dim)\n\n    def forward(self, x, context = None, **kwargs):\n        b, n, d, d_h, h, k = *x.shape, self.dim_head, self.heads, self.k\n\n        kv_len = n if context is None else context.shape[1]\n        assert kv_len == self.seq_len, f'the sequence length of the key / values must be {self.seq_len} - {kv_len} given'\n\n        queries = self.to_q(x)\n\n        proj_seq_len = lambda args: torch.einsum('bnd,nk->bkd', *args)\n\n        kv_input = x if context is None else context\n\n        keys = self.to_k(kv_input)\n        values = self.to_v(kv_input) if not self.share_kv else keys\n\n        kv_projs = (self.proj_k, self.proj_v if not self.share_kv else self.proj_k)\n\n        # project keys and values along the sequence length dimension to k\n\n        keys, values = map(proj_seq_len, zip((keys, values), kv_projs))\n\n        # merge head into batch for queries and key / values\n\n        queries = queries.reshape(b, n, h, -1).transpose(1, 2)\n\n        merge_key_values = lambda t: t.reshape(b, k, -1, d_h).transpose(1, 2).expand(-1, h, -1, -1)\n        keys, values = map(merge_key_values, (keys, values))\n        \n        # attention\n\n        dots = torch.einsum('bhnd,bhkd->bhnk', queries, keys) * (d_h ** -0.5)\n        attn = dots.softmax(dim=-1)\n        attn = self.dropout(attn)\n        out = torch.einsum('bhnk,bhkd->bhnd', attn, values)\n\n        # split heads\n        out = out.transpose(1, 2).reshape(b, n, -1)\n        return self.to_out(out)","metadata":{"id":"FOiFLNi6FfaR","execution":{"iopub.status.busy":"2021-07-28T20:43:25.314810Z","iopub.execute_input":"2021-07-28T20:43:25.315173Z","iopub.status.idle":"2021-07-28T20:43:25.337175Z","shell.execute_reply.started":"2021-07-28T20:43:25.315134Z","shell.execute_reply":"2021-07-28T20:43:25.336232Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn, einsum\nimport torch.nn.functional as F\n\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\n\n# helpers\n\ndef pair(t):\n    return t if isinstance(t, tuple) else (t, t)\n\n# classes\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs)\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout = 0.):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass Transformer(nn.Module):\n    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        k = 64\n        seq_len = 1025\n        one_kv_head = False #use False for RoPE\n        share_kv = True\n        reversible = False\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PreNorm(dim, LinformerSelfAttention(dim, seq_len, k = k, heads = heads, dim_head = dim_head, one_kv_head = one_kv_head, share_kv = share_kv, dropout = dropout)),\n                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n            ]))\n    def forward(self, x):\n        for attn, ff in self.layers:\n            x = attn(x) + x\n            x = ff(x) + x\n        return x\n\nclass ViT(nn.Module):\n    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n        super().__init__()\n        image_height, image_width = pair(image_size)\n        patch_height, patch_width = pair(patch_size)\n\n        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n\n        num_patches = (image_height // patch_height) * (image_width // patch_width)\n        patch_dim = channels * patch_height * patch_width\n        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n\n        self.to_patch_embedding = nn.Sequential(\n            nn.Conv2d(3, 32, 3, stride = 1, padding = 1),\n            nn.Conv2d(32, 64, 3, stride = 1, padding = 1),\n            nn.Conv2d(64, dim, 3, stride = 1, padding = 1),\n            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width)\n            \n        )\n\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n        self.dropout = nn.Dropout(emb_dropout)\n\n        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n\n        self.pool = pool\n        self.to_latent = nn.Identity()\n\n        self.mlp_head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_classes)\n        )\n\n\n    def forward(self, img):\n        x = self.to_patch_embedding(img)\n        b, n, _ = x.shape\n\n        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x += self.pos_embedding[:, :(n + 1)] #1D learnable PE\n        x = self.dropout(x)\n\n        x = self.transformer(x)\n\n        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n\n        x = self.to_latent(x)\n        return self.mlp_head(x)","metadata":{"id":"94EEKIehFfaU","execution":{"iopub.status.busy":"2021-07-28T20:43:25.340722Z","iopub.execute_input":"2021-07-28T20:43:25.341029Z","iopub.status.idle":"2021-07-28T20:43:25.365084Z","shell.execute_reply.started":"2021-07-28T20:43:25.341001Z","shell.execute_reply":"2021-07-28T20:43:25.364212Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"model = ViT(\n    image_size = 32,\n    patch_size = 1,\n    num_classes = 10,             # number of stages\n    dim = 128,  # dimensions at each stage\n    depth = 4,              # transformer of depth 4 at each stage\n    heads = 4,      # heads at each stage\n    mlp_dim = 256,\n    dropout = 0.,\n    dim_head = 32\n)\n\n\nmodel.to(device)\nprint(summary(model, (3,32,32)))","metadata":{"id":"WoOl842NFfaX","outputId":"c9630a92-ad98-4274-dd20-0def392d91c9","execution":{"iopub.status.busy":"2021-07-28T20:43:25.366449Z","iopub.execute_input":"2021-07-28T20:43:25.366839Z","iopub.status.idle":"2021-07-28T20:43:30.178646Z","shell.execute_reply.started":"2021-07-28T20:43:25.366801Z","shell.execute_reply":"2021-07-28T20:43:30.177886Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 32, 32, 32]             896\n            Conv2d-2           [-1, 64, 32, 32]          18,496\n            Conv2d-3          [-1, 128, 32, 32]          73,856\n         Rearrange-4            [-1, 1024, 128]               0\n           Dropout-5            [-1, 1025, 128]               0\n         LayerNorm-6            [-1, 1025, 128]             256\n            Linear-7            [-1, 1025, 128]          16,384\n            Linear-8            [-1, 1025, 128]          16,384\n           Dropout-9          [-1, 4, 1025, 64]               0\n           Linear-10            [-1, 1025, 128]          16,512\nLinformerSelfAttention-11            [-1, 1025, 128]               0\n          PreNorm-12            [-1, 1025, 128]               0\n        LayerNorm-13            [-1, 1025, 128]             256\n           Linear-14            [-1, 1025, 256]          33,024\n             GELU-15            [-1, 1025, 256]               0\n          Dropout-16            [-1, 1025, 256]               0\n           Linear-17            [-1, 1025, 128]          32,896\n          Dropout-18            [-1, 1025, 128]               0\n      FeedForward-19            [-1, 1025, 128]               0\n          PreNorm-20            [-1, 1025, 128]               0\n        LayerNorm-21            [-1, 1025, 128]             256\n           Linear-22            [-1, 1025, 128]          16,384\n           Linear-23            [-1, 1025, 128]          16,384\n          Dropout-24          [-1, 4, 1025, 64]               0\n           Linear-25            [-1, 1025, 128]          16,512\nLinformerSelfAttention-26            [-1, 1025, 128]               0\n          PreNorm-27            [-1, 1025, 128]               0\n        LayerNorm-28            [-1, 1025, 128]             256\n           Linear-29            [-1, 1025, 256]          33,024\n             GELU-30            [-1, 1025, 256]               0\n          Dropout-31            [-1, 1025, 256]               0\n           Linear-32            [-1, 1025, 128]          32,896\n          Dropout-33            [-1, 1025, 128]               0\n      FeedForward-34            [-1, 1025, 128]               0\n          PreNorm-35            [-1, 1025, 128]               0\n        LayerNorm-36            [-1, 1025, 128]             256\n           Linear-37            [-1, 1025, 128]          16,384\n           Linear-38            [-1, 1025, 128]          16,384\n          Dropout-39          [-1, 4, 1025, 64]               0\n           Linear-40            [-1, 1025, 128]          16,512\nLinformerSelfAttention-41            [-1, 1025, 128]               0\n          PreNorm-42            [-1, 1025, 128]               0\n        LayerNorm-43            [-1, 1025, 128]             256\n           Linear-44            [-1, 1025, 256]          33,024\n             GELU-45            [-1, 1025, 256]               0\n          Dropout-46            [-1, 1025, 256]               0\n           Linear-47            [-1, 1025, 128]          32,896\n          Dropout-48            [-1, 1025, 128]               0\n      FeedForward-49            [-1, 1025, 128]               0\n          PreNorm-50            [-1, 1025, 128]               0\n        LayerNorm-51            [-1, 1025, 128]             256\n           Linear-52            [-1, 1025, 128]          16,384\n           Linear-53            [-1, 1025, 128]          16,384\n          Dropout-54          [-1, 4, 1025, 64]               0\n           Linear-55            [-1, 1025, 128]          16,512\nLinformerSelfAttention-56            [-1, 1025, 128]               0\n          PreNorm-57            [-1, 1025, 128]               0\n        LayerNorm-58            [-1, 1025, 128]             256\n           Linear-59            [-1, 1025, 256]          33,024\n             GELU-60            [-1, 1025, 256]               0\n          Dropout-61            [-1, 1025, 256]               0\n           Linear-62            [-1, 1025, 128]          32,896\n          Dropout-63            [-1, 1025, 128]               0\n      FeedForward-64            [-1, 1025, 128]               0\n          PreNorm-65            [-1, 1025, 128]               0\n      Transformer-66            [-1, 1025, 128]               0\n         Identity-67                  [-1, 128]               0\n        LayerNorm-68                  [-1, 128]             256\n           Linear-69                   [-1, 10]           1,290\n================================================================\nTotal params: 557,642\nTrainable params: 557,642\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.01\nForward/backward pass size (MB): 80.83\nParams size (MB): 2.13\nEstimated Total Size (MB): 82.97\n----------------------------------------------------------------\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\nscaler = torch.cuda.amp.GradScaler()\n# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\ntop1 = []\ntop5 = []\noptimizer = optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\nfor epoch in range(40):  # loop over the dataset multiple times\n    t0 = time.time()\n    epoch_accuracy = 0\n    epoch_loss = 0\n    running_loss = 0.0\n\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data[0].to(device), data[1].to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        with torch.cuda.amp.autocast():\n            loss = criterion(outputs, labels)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        acc = (outputs.argmax(dim=1) == labels).float().mean()\n        epoch_accuracy += acc / len(trainloader)\n        epoch_loss += loss / len(trainloader)\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 200 == 199:    # print every 2000 mini-batches\n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss / 2000))\n            running_loss = 0.0\n    correct = 0\n    total = 0\n    correct_1=0\n    correct_5=0\n    c = 0\n    with torch.no_grad():\n        for data in testloader:\n            images, labels = data[0].to(device), data[1].to(device)\n            outputs = model(images)\n#         outputs = net(images)\n\n            _, predicted = torch.max(outputs.data, 1)\n            res = accuracy(outputs, labels)\n            correct_1 += res[0][0].float()\n            correct_5 += res[1][0].float()\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            c += 1\n        \n    print(f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - Top 1: {correct_1/c} - Top 5: {correct_5/c} - Time: {time.time() - t0}\\n\")\n    top1.append(correct_1/c)\n    top5.append(correct_5/c)\n    if float(correct_1/c) >= float(max(top1)):\n        PATH = 'ViLCNNRoPE.pth'\n        torch.save(model.state_dict(), PATH)\n        print(1)\nprint('Finished Training')","metadata":{"id":"yZAB3FhZFfaX","outputId":"c4b7d913-a3fd-4e2a-e84e-c04d92f37817","execution":{"iopub.status.busy":"2021-07-28T20:43:30.180207Z","iopub.execute_input":"2021-07-28T20:43:30.180519Z","iopub.status.idle":"2021-07-28T21:15:18.027625Z","shell.execute_reply.started":"2021-07-28T20:43:30.180489Z","shell.execute_reply":"2021-07-28T21:15:18.025417Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Epoch : 1 - loss : 1.6756 - acc: 0.3936 - Top 1: 48.740234375 - Top 5: 92.4609375 - Time: 109.05505418777466\n\n1\nEpoch : 2 - loss : 1.3719 - acc: 0.5045 - Top 1: 52.783203125 - Top 5: 94.12109375 - Time: 109.13424229621887\n\n1\nEpoch : 3 - loss : 1.2456 - acc: 0.5513 - Top 1: 55.703125 - Top 5: 94.9609375 - Time: 109.21355652809143\n\n1\nEpoch : 4 - loss : 1.1380 - acc: 0.5885 - Top 1: 56.42578125 - Top 5: 95.341796875 - Time: 109.18095469474792\n\n1\nEpoch : 5 - loss : 1.0480 - acc: 0.6244 - Top 1: 59.1796875 - Top 5: 95.986328125 - Time: 109.29091238975525\n\n1\nEpoch : 6 - loss : 0.9641 - acc: 0.6547 - Top 1: 60.5859375 - Top 5: 96.25 - Time: 109.16302633285522\n\n1\nEpoch : 7 - loss : 0.8740 - acc: 0.6882 - Top 1: 62.685546875 - Top 5: 96.40625 - Time: 109.20363998413086\n\n1\nEpoch : 8 - loss : 0.7952 - acc: 0.7174 - Top 1: 62.63671875 - Top 5: 96.62109375 - Time: 109.28722286224365\n\nEpoch : 9 - loss : 0.7238 - acc: 0.7420 - Top 1: 63.3984375 - Top 5: 96.7578125 - Time: 109.1933376789093\n\n1\nEpoch : 10 - loss : 0.6546 - acc: 0.7664 - Top 1: 64.58984375 - Top 5: 96.669921875 - Time: 109.37322330474854\n\n1\nEpoch : 11 - loss : 0.5921 - acc: 0.7888 - Top 1: 63.779296875 - Top 5: 96.328125 - Time: 109.44916677474976\n\nEpoch : 12 - loss : 0.5271 - acc: 0.8145 - Top 1: 63.80859375 - Top 5: 96.826171875 - Time: 109.15524554252625\n\nEpoch : 13 - loss : 0.4771 - acc: 0.8318 - Top 1: 64.7265625 - Top 5: 96.77734375 - Time: 109.18259501457214\n\n1\nEpoch : 14 - loss : 0.4259 - acc: 0.8482 - Top 1: 64.765625 - Top 5: 96.845703125 - Time: 109.13843512535095\n\n1\nEpoch : 15 - loss : 0.3602 - acc: 0.8726 - Top 1: 64.228515625 - Top 5: 96.845703125 - Time: 109.33602046966553\n\nEpoch : 16 - loss : 0.3333 - acc: 0.8809 - Top 1: 64.0234375 - Top 5: 96.708984375 - Time: 109.22596073150635\n\nEpoch : 17 - loss : 0.2906 - acc: 0.8973 - Top 1: 63.486328125 - Top 5: 96.6796875 - Time: 109.28411436080933\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-1faadb3c965d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No inf checks were recorded for this optimizer.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No inf checks were recorded for this optimizer.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"plt.subplot(2, 1, 1)\nplt.plot(top1, '.-')\nplt.title('Accuracy')\nplt.ylabel('Top 1 accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(top5, '.-')\nplt.xlabel('epochs')\nplt.ylabel('Top 5 accuracy')\n\nplt.show()\n","metadata":{"id":"suqt55zpFfaZ","execution":{"iopub.status.busy":"2021-07-28T21:15:23.213375Z","iopub.execute_input":"2021-07-28T21:15:23.213713Z","iopub.status.idle":"2021-07-28T21:15:23.469880Z","shell.execute_reply.started":"2021-07-28T21:15:23.213680Z","shell.execute_reply":"2021-07-28T21:15:23.468805Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]}]}