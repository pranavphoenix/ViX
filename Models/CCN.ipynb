{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "CCN.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "id": "5bFsMrNWYRe5",
        "execution": {
          "iopub.status.busy": "2021-08-12T05:29:32.910969Z",
          "iopub.execute_input": "2021-08-12T05:29:32.911346Z",
          "iopub.status.idle": "2021-08-12T05:30:30.736342Z",
          "shell.execute_reply.started": "2021-08-12T05:29:32.911266Z",
          "shell.execute_reply": "2021-08-12T05:30:30.735288Z"
        },
        "trusted": true,
        "outputId": "c4ae8dd3-f227-4c70-c445-5fea5d164a8b"
      },
      "source": [
        "import torch, math\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torch.optim as optim\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "!pip install einops\n",
        "from math import ceil\n",
        "!pip install performer-pytorch\n",
        "import os, glob\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image \n",
        "from torch import nn, einsum\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "from einops import rearrange, reduce\n",
        "\n",
        "# helpers\n",
        "from einops import reduce\n",
        "\n",
        "batch_size = 160\n",
        "\n",
        "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "!unzip -q ./tiny-imagenet-200.zip\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "transform = transforms.Normalize((122.4786, 114.2755, 101.3963), (70.4924, 68.5679, 71.8127))\n",
        "\n",
        "id_dict = {}\n",
        "for i, line in enumerate(open('./tiny-imagenet-200/wnids.txt', 'r')):\n",
        "    id_dict[line.replace('\\n', '')] = i\n",
        "\n",
        "class TrainTinyImageNetDataset(Dataset):\n",
        "    def __init__(self, id, transform=None):\n",
        "        self.filenames = glob.glob(\"./tiny-imagenet-200/train/*/*/*.JPEG\")\n",
        "        self.transform = transform\n",
        "        self.id_dict = id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.filenames[idx]\n",
        "        image = read_image(img_path)\n",
        "        if image.shape[0] == 1:\n",
        "            image = torch.cat((image,image,image),0)\n",
        "        label = self.id_dict[img_path.split('/')[3]]\n",
        "        if self.transform:\n",
        "            image = self.transform(image.type(torch.FloatTensor))\n",
        "        return image, label\n",
        "    \n",
        "class TestTinyImageNetDataset(Dataset):\n",
        "    def __init__(self, id, transform=None):\n",
        "        self.filenames = glob.glob(\"./tiny-imagenet-200/val/images/*.JPEG\")\n",
        "        self.transform = transform\n",
        "        self.id_dict = id\n",
        "        self.cls_dic = {}\n",
        "        for i, line in enumerate(open('./tiny-imagenet-200/val/val_annotations.txt', 'r')):\n",
        "            a = line.split('\\t')\n",
        "            img, cls_id = a[0],a[1]\n",
        "            self.cls_dic[img] = self.id_dict[cls_id]\n",
        " \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.filenames[idx]\n",
        "        image = read_image(img_path)\n",
        "        if image.shape[0] == 1:\n",
        "            image = torch.cat((image,image,image),0)\n",
        "        label = self.cls_dic[img_path.split('/')[-1]]\n",
        "        if self.transform:\n",
        "            image = self.transform(image.type(torch.FloatTensor))\n",
        "        return image, label\n",
        "    \n",
        "trainset = TrainTinyImageNetDataset(id=id_dict, transform = transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = TestTinyImageNetDataset(id=id_dict, transform = transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,5)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\n",
        "    prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
        "    \"\"\"\n",
        "    maxk = max(topk)\n",
        "         # sizefunction: the number of total elements\n",
        "    batch_size = target.size(0) \n",
        " \n",
        "         # topk function selects the number of k before output\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "         ##########Do not understand t()k\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))   \n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nCollecting einops\n  Downloading einops-0.3.0-py2.py3-none-any.whl (25 kB)\nInstalling collected packages: einops\nSuccessfully installed einops-0.3.0\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nCollecting performer-pytorch\n  Downloading performer_pytorch-1.0.11-py3-none-any.whl (12 kB)\nCollecting axial-positional-embedding>=0.1.0\n  Downloading axial_positional_embedding-0.2.1.tar.gz (2.6 kB)\nRequirement already satisfied: torch>=1.6 in /opt/conda/lib/python3.7/site-packages (from performer-pytorch) (1.7.0)\nRequirement already satisfied: einops>=0.3 in /opt/conda/lib/python3.7/site-packages (from performer-pytorch) (0.3.0)\nCollecting local-attention>=1.1.1\n  Downloading local_attention-1.4.3-py3-none-any.whl (5.0 kB)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.6->performer-pytorch) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.6->performer-pytorch) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch>=1.6->performer-pytorch) (0.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch>=1.6->performer-pytorch) (1.19.5)\nBuilding wheels for collected packages: axial-positional-embedding\n  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-py3-none-any.whl size=2902 sha256=575e60520d9496822632df80ab6646a2e6ccb5c42efa083fa68f2f52e1dc7ad8\n  Stored in directory: /root/.cache/pip/wheels/4a/2c/c3/9a1cb267c0d0d9b6eeba7952addb32b17857d1f799690c27a8\nSuccessfully built axial-positional-embedding\nInstalling collected packages: local-attention, axial-positional-embedding, performer-pytorch\nSuccessfully installed axial-positional-embedding-0.2.1 local-attention-1.4.3 performer-pytorch-1.0.11\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n--2021-08-12 05:29:59--  http://cs231n.stanford.edu/tiny-imagenet-200.zip\nResolving cs231n.stanford.edu (cs231n.stanford.edu)... 171.64.68.10\nConnecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.68.10|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 248100043 (237M) [application/zip]\nSaving to: ‘tiny-imagenet-200.zip’\n\ntiny-imagenet-200.z 100%[===================>] 236.61M  11.0MB/s    in 22s     \n\n2021-08-12 05:30:22 (10.8 MB/s) - ‘tiny-imagenet-200.zip’ saved [248100043/248100043]\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-12T05:30:30.738203Z",
          "iopub.execute_input": "2021-08-12T05:30:30.738566Z",
          "iopub.status.idle": "2021-08-12T05:30:30.764653Z",
          "shell.execute_reply.started": "2021-08-12T05:30:30.738525Z",
          "shell.execute_reply": "2021-08-12T05:30:30.763203Z"
        },
        "trusted": true,
        "id": "iUoMYLgP1hU6"
      },
      "source": [
        "from math import ceil\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from einops import rearrange, reduce\n",
        "\n",
        "# helper functions\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def moore_penrose_iter_pinv(x, iters = 6):\n",
        "    device = x.device\n",
        "\n",
        "    abs_x = torch.abs(x)\n",
        "    col = abs_x.sum(dim = -1)\n",
        "    row = abs_x.sum(dim = -2)\n",
        "    z = rearrange(x, '... i j -> ... j i') / (torch.max(col) * torch.max(row))\n",
        "\n",
        "    I = torch.eye(x.shape[-1], device = device)\n",
        "    I = rearrange(I, 'i j -> () i j')\n",
        "\n",
        "    for _ in range(iters):\n",
        "        xz = x @ z\n",
        "        z = 0.25 * z @ (13 * I - (xz @ (15 * I - (xz @ (7 * I - xz)))))\n",
        "\n",
        "    return z\n",
        "# main attention class\n",
        "\n",
        "class NystromAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        dim_head = 32,\n",
        "        heads = 4,\n",
        "        num_landmarks = 256,\n",
        "        pinv_iterations = 6,\n",
        "        residual = True,\n",
        "        residual_conv_kernel = 33,\n",
        "        eps = 1e-8,\n",
        "        dropout = 0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        inner_dim = heads * dim_head\n",
        "\n",
        "        self.num_landmarks = num_landmarks\n",
        "        self.pinv_iterations = pinv_iterations\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self.residual = residual\n",
        "        if residual:\n",
        "            kernel_size = residual_conv_kernel\n",
        "            padding = residual_conv_kernel // 2\n",
        "            self.res_conv = nn.Conv2d(heads, heads, (kernel_size, 1), padding = (padding, 0), groups = heads, bias = False)\n",
        "\n",
        "    def forward(self, x, mask = None, return_attn = False):\n",
        "        b, n, _, h, m, iters, eps = *x.shape, self.heads, self.num_landmarks, self.pinv_iterations, self.eps\n",
        "\n",
        "        # pad so that sequence can be evenly divided into m landmarks\n",
        "\n",
        "        remainder = n % m\n",
        "        if remainder > 0:\n",
        "            padding = m - (n % m)\n",
        "            x = F.pad(x, (0, 0, padding, 0), value = 0)\n",
        "\n",
        "        # derive query, keys, values\n",
        "\n",
        "        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n",
        "\n",
        "        q = q * self.scale\n",
        "\n",
        "        # generate landmarks by sum reduction, and then calculate mean using the mask\n",
        "\n",
        "        l = ceil(n / m)\n",
        "        landmark_einops_eq = '... (n l) d -> ... n d'\n",
        "        q_landmarks = reduce(q, landmark_einops_eq, 'sum', l = l)\n",
        "        k_landmarks = reduce(k, landmark_einops_eq, 'sum', l = l)\n",
        "\n",
        "        # calculate landmark mask, and also get sum of non-masked elements in preparation for masked mean\n",
        "\n",
        "        divisor = l\n",
        "\n",
        "\n",
        "        # masked mean (if mask exists)\n",
        "\n",
        "        q_landmarks /= divisor\n",
        "        k_landmarks /= divisor\n",
        "\n",
        "        # similarities\n",
        "\n",
        "        einops_eq = '... i d, ... j d -> ... i j'\n",
        "        sim1 = einsum(einops_eq, q, k_landmarks)\n",
        "        sim2 = einsum(einops_eq, q_landmarks, k_landmarks)\n",
        "        sim3 = einsum(einops_eq, q_landmarks, k)\n",
        "\n",
        "        # masking\n",
        "\n",
        "        if exists(mask):\n",
        "            mask_value = -torch.finfo(q.dtype).max\n",
        "            sim1.masked_fill_(~(mask[..., None] * mask_landmarks[..., None, :]), mask_value)\n",
        "            sim2.masked_fill_(~(mask_landmarks[..., None] * mask_landmarks[..., None, :]), mask_value)\n",
        "            sim3.masked_fill_(~(mask_landmarks[..., None] * mask[..., None, :]), mask_value)\n",
        "\n",
        "        # eq (15) in the paper and aggregate values\n",
        "\n",
        "        attn1, attn2, attn3 = map(lambda t: t.softmax(dim = -1), (sim1, sim2, sim3))\n",
        "        attn2_inv = moore_penrose_iter_pinv(attn2, iters)\n",
        "\n",
        "        out = (attn1 @ attn2_inv) @ (attn3 @ v)\n",
        "\n",
        "        # add depth-wise conv residual of values\n",
        "\n",
        "        if self.residual:\n",
        "            out += self.res_conv(v)\n",
        "\n",
        "        # merge and combine heads\n",
        "\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n",
        "        out = self.to_out(out)\n",
        "        out = out[:, -n:]\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-12T05:30:30.766661Z",
          "iopub.execute_input": "2021-08-12T05:30:30.767219Z",
          "iopub.status.idle": "2021-08-12T05:30:30.945575Z",
          "shell.execute_reply.started": "2021-08-12T05:30:30.767183Z",
          "shell.execute_reply": "2021-08-12T05:30:30.944642Z"
        },
        "trusted": true,
        "id": "f-JXAfyt1hU7"
      },
      "source": [
        "def _cct(num_layers, num_heads, mlp_ratio, embedding_dim,\n",
        "         kernel_size=3, stride=None, padding=None,\n",
        "         *args, **kwargs):\n",
        "    stride = stride if stride is not None else max(1, (kernel_size // 2) - 1)\n",
        "    padding = padding if padding is not None else max(1, (kernel_size // 2))\n",
        "    return CCT(num_layers=num_layers,\n",
        "               num_heads=num_heads,\n",
        "               mlp_ratio=mlp_ratio,\n",
        "               embedding_dim=embedding_dim,\n",
        "               kernel_size=kernel_size,\n",
        "               stride=stride,\n",
        "               padding=padding,\n",
        "               *args, **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Inspired by torch.nn.TransformerEncoderLayer and\n",
        "    rwightman's timm package.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
        "                 attention_dropout=0.1, drop_path_rate=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        dim = 128\n",
        "        dim_head = 32\n",
        "        heads = 4\n",
        "        num_landmarks = 64\n",
        "        pinv_iterations = 6\n",
        "        residual = True\n",
        "        residual_conv_kernel = 33\n",
        "        eps = 1e-8\n",
        "        \n",
        "        self.pre_norm = nn.LayerNorm(d_model)\n",
        "        self.self_attn = NystromAttention(dim, dim_head = dim_head, num_landmarks =num_landmarks, pinv_iterations =pinv_iterations, residual = residual, residual_conv_kernel = residual_conv_kernel, eps = eps)\n",
        "        self.linear1  = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.norm1    = nn.LayerNorm(d_model)\n",
        "        self.linear2  = nn.Linear(dim_feedforward, d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()\n",
        "\n",
        "        self.activation = F.gelu\n",
        "\n",
        "    def forward(self, src: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
        "        src = src + self.drop_path(self.self_attn(self.pre_norm(src)))\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout1(self.activation(self.linear1(src))))\n",
        "        src = src + self.drop_path(self.dropout2(src2))\n",
        "        return src\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    \"\"\"\n",
        "    Obtained from: github.com:rwightman/pytorch-image-models\n",
        "    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
        "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
        "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
        "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
        "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
        "    'survival rate' as the argument.\n",
        "    \"\"\"\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"\n",
        "    Obtained from: github.com:rwightman/pytorch-image-models\n",
        "    Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class Tokenizer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 kernel_size, stride, padding,\n",
        "                 pooling_kernel_size=3, pooling_stride=2, pooling_padding=1,\n",
        "                 n_conv_layers=1,\n",
        "                 n_input_channels=3,\n",
        "                 n_output_channels=64,\n",
        "                 in_planes=64,\n",
        "                 activation=None,\n",
        "                 max_pool=True,\n",
        "                 conv_bias=False):\n",
        "        super(Tokenizer, self).__init__()\n",
        "\n",
        "        n_filter_list = [n_input_channels] + \\\n",
        "                        [in_planes for _ in range(n_conv_layers - 1)] + \\\n",
        "                        [n_output_channels]\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            *[nn.Sequential(\n",
        "                nn.Conv2d(n_filter_list[i], n_filter_list[i + 1],\n",
        "                          kernel_size=(kernel_size, kernel_size),\n",
        "                          stride=(stride[i], stride[i]),\n",
        "                          padding=(padding, padding), bias=conv_bias),\n",
        "                nn.Identity() if activation is None else activation(),\n",
        "                nn.MaxPool2d(kernel_size=pooling_kernel_size,\n",
        "                             stride=pooling_stride,\n",
        "                             padding=pooling_padding) if max_pool else nn.Identity()\n",
        "            )\n",
        "                for i in range(n_conv_layers)\n",
        "            ])\n",
        "\n",
        "        self.flattener = nn.Flatten(2, 3)\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def sequence_length(self, n_channels=3, height=224, width=224):\n",
        "        return self.forward(torch.zeros((1, n_channels, height, width))).shape[1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.flattener(self.conv_layers(x)).transpose(-2, -1)\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight)\n",
        "\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 seq_pool=True,\n",
        "                 embedding_dim=768,\n",
        "                 num_layers=12,\n",
        "                 num_heads=12,\n",
        "                 mlp_ratio=4.0,\n",
        "                 num_classes=1000,\n",
        "                 dropout_rate=0.1,\n",
        "                 attention_dropout=0.1,\n",
        "                 stochastic_depth_rate=0.1,\n",
        "                 positional_embedding='sine',\n",
        "                 sequence_length=None,\n",
        "                 *args, **kwargs):\n",
        "        super().__init__()\n",
        "        positional_embedding = positional_embedding if \\\n",
        "            positional_embedding in ['sine', 'learnable', 'none'] else 'sine'\n",
        "        dim_feedforward = int(embedding_dim * mlp_ratio)\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.sequence_length = sequence_length\n",
        "        self.seq_pool = seq_pool\n",
        "\n",
        "        assert sequence_length is not None or positional_embedding == 'none', \\\n",
        "            f\"Positional embedding is set to {positional_embedding} and\" \\\n",
        "            f\" the sequence length was not specified.\"\n",
        "\n",
        "        if not seq_pool:\n",
        "            sequence_length += 1\n",
        "            self.class_emb = nn.Parameter(torch.zeros(1, 1, self.embedding_dim),\n",
        "                                          requires_grad=True)\n",
        "        else:\n",
        "            self.attention_pool = nn.Linear(self.embedding_dim, 1)\n",
        "\n",
        "        if positional_embedding != 'none':\n",
        "            if positional_embedding == 'learnable':\n",
        "                self.positional_emb = nn.Parameter(torch.zeros(1, sequence_length, embedding_dim),\n",
        "                                                   requires_grad=True)\n",
        "                nn.init.trunc_normal_(self.positional_emb, std=0.2)\n",
        "            else:\n",
        "                self.positional_emb = nn.Parameter(self.sinusoidal_embedding(sequence_length, embedding_dim),\n",
        "                                                   requires_grad=False)\n",
        "        else:\n",
        "            self.positional_emb = None\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        dpr = [x.item() for x in torch.linspace(0, stochastic_depth_rate, num_layers)]\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads,\n",
        "                                    dim_feedforward=dim_feedforward, dropout=dropout_rate,\n",
        "                                    attention_dropout=attention_dropout, drop_path_rate=dpr[i])\n",
        "            for i in range(num_layers)])\n",
        "        self.norm = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.positional_emb is None and x.size(1) < self.sequence_length:\n",
        "            x = F.pad(x, (0, 0, 0, self.n_channels - x.size(1)), mode='constant', value=0)\n",
        "\n",
        "        if not self.seq_pool:\n",
        "            cls_token = self.class_emb.expand(x.shape[0], -1, -1)\n",
        "            x = torch.cat((cls_token, x), dim=1)\n",
        "\n",
        "        if self.positional_emb is not None:\n",
        "            x += self.positional_emb\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if self.seq_pool:\n",
        "            x = torch.matmul(F.softmax(self.attention_pool(x), dim=1).transpose(-1, -2), x).squeeze(-2)\n",
        "        else:\n",
        "            x = x[:, 0]\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def sinusoidal_embedding(n_channels, dim):\n",
        "        pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]\n",
        "                                for p in range(n_channels)])\n",
        "        pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
        "        pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
        "        return pe.unsqueeze(0)\n",
        "\n",
        "\n",
        "# CCT Main model\n",
        "class CCT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 img_size=224,\n",
        "                 embedding_dim=768,\n",
        "                 n_input_channels=3,\n",
        "                 n_conv_layers=1,\n",
        "                 kernel_size=7,\n",
        "                 stride=(2,1,1),\n",
        "                 padding=3,\n",
        "                 pooling_kernel_size=3,\n",
        "                 pooling_stride=2,\n",
        "                 pooling_padding=1,\n",
        "                 *args, **kwargs):\n",
        "        super(CCT, self).__init__()\n",
        "\n",
        "        self.tokenizer = Tokenizer(n_input_channels=n_input_channels,\n",
        "                                   n_output_channels=embedding_dim,\n",
        "                                   kernel_size=kernel_size,\n",
        "                                   stride=stride,\n",
        "                                   padding=padding,\n",
        "                                   pooling_kernel_size=pooling_kernel_size,\n",
        "                                   pooling_stride=pooling_stride,\n",
        "                                   pooling_padding=pooling_padding,\n",
        "                                   max_pool=True,\n",
        "                                   activation=nn.ReLU,\n",
        "                                   n_conv_layers=n_conv_layers,\n",
        "                                   conv_bias=False)\n",
        "\n",
        "        self.classifier = TransformerClassifier(\n",
        "            sequence_length=self.tokenizer.sequence_length(n_channels=n_input_channels,\n",
        "                                                           height=img_size,\n",
        "                                                           width=img_size),\n",
        "            embedding_dim=embedding_dim,\n",
        "            seq_pool=True,\n",
        "            dropout_rate=0.,\n",
        "            attention_dropout=0.1,\n",
        "            stochastic_depth=0.1,\n",
        "            *args, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.tokenizer(x)\n",
        "        return self.classifier(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je07f7q7YRfV",
        "execution": {
          "iopub.status.busy": "2021-08-12T05:30:30.948815Z",
          "iopub.execute_input": "2021-08-12T05:30:30.949252Z",
          "iopub.status.idle": "2021-08-12T05:30:36.784232Z",
          "shell.execute_reply.started": "2021-08-12T05:30:30.949211Z",
          "shell.execute_reply": "2021-08-12T05:30:36.783426Z"
        },
        "trusted": true,
        "outputId": "086910a6-423e-498d-d26d-f602ae50aba7"
      },
      "source": [
        "model = CCT(\n",
        "        img_size=64,\n",
        "        embedding_dim=128,\n",
        "        n_conv_layers=3,\n",
        "        kernel_size=3,\n",
        "        stride=(2,1,1),\n",
        "        padding=1,\n",
        "        pooling_kernel_size=3,\n",
        "        pooling_stride=1,\n",
        "        pooling_padding=1,\n",
        "        num_layers=4,\n",
        "        num_heads=4,\n",
        "        mlp_radio=1.,\n",
        "        num_classes=200,\n",
        "        positional_embedding='learnable', # ['sine', 'learnable', 'none']\n",
        "        )\n",
        "\n",
        "model.to(device)\n",
        "print(summary(model, (3,64,64)))\n",
        "print(torch.cuda.get_device_properties(device))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 64, 32, 32]           1,728\n              ReLU-2           [-1, 64, 32, 32]               0\n         MaxPool2d-3           [-1, 64, 32, 32]               0\n            Conv2d-4           [-1, 64, 32, 32]          36,864\n              ReLU-5           [-1, 64, 32, 32]               0\n         MaxPool2d-6           [-1, 64, 32, 32]               0\n            Conv2d-7          [-1, 128, 32, 32]          73,728\n              ReLU-8          [-1, 128, 32, 32]               0\n         MaxPool2d-9          [-1, 128, 32, 32]               0\n          Flatten-10            [-1, 128, 1024]               0\n        Tokenizer-11            [-1, 1024, 128]               0\n          Dropout-12            [-1, 1024, 128]               0\n        LayerNorm-13            [-1, 1024, 128]             256\n           Linear-14            [-1, 1024, 384]          49,152\n           Conv2d-15          [-1, 4, 1024, 32]             132\n           Linear-16            [-1, 1024, 128]          16,512\n          Dropout-17            [-1, 1024, 128]               0\n NystromAttention-18            [-1, 1024, 128]               0\n         Identity-19            [-1, 1024, 128]               0\n        LayerNorm-20            [-1, 1024, 128]             256\n           Linear-21            [-1, 1024, 512]          66,048\n          Dropout-22            [-1, 1024, 512]               0\n           Linear-23            [-1, 1024, 128]          65,664\n          Dropout-24            [-1, 1024, 128]               0\n         Identity-25            [-1, 1024, 128]               0\nTransformerEncoderLayer-26            [-1, 1024, 128]               0\n        LayerNorm-27            [-1, 1024, 128]             256\n           Linear-28            [-1, 1024, 384]          49,152\n           Conv2d-29          [-1, 4, 1024, 32]             132\n           Linear-30            [-1, 1024, 128]          16,512\n          Dropout-31            [-1, 1024, 128]               0\n NystromAttention-32            [-1, 1024, 128]               0\n         DropPath-33            [-1, 1024, 128]               0\n        LayerNorm-34            [-1, 1024, 128]             256\n           Linear-35            [-1, 1024, 512]          66,048\n          Dropout-36            [-1, 1024, 512]               0\n           Linear-37            [-1, 1024, 128]          65,664\n          Dropout-38            [-1, 1024, 128]               0\n         DropPath-39            [-1, 1024, 128]               0\nTransformerEncoderLayer-40            [-1, 1024, 128]               0\n        LayerNorm-41            [-1, 1024, 128]             256\n           Linear-42            [-1, 1024, 384]          49,152\n           Conv2d-43          [-1, 4, 1024, 32]             132\n           Linear-44            [-1, 1024, 128]          16,512\n          Dropout-45            [-1, 1024, 128]               0\n NystromAttention-46            [-1, 1024, 128]               0\n         DropPath-47            [-1, 1024, 128]               0\n        LayerNorm-48            [-1, 1024, 128]             256\n           Linear-49            [-1, 1024, 512]          66,048\n          Dropout-50            [-1, 1024, 512]               0\n           Linear-51            [-1, 1024, 128]          65,664\n          Dropout-52            [-1, 1024, 128]               0\n         DropPath-53            [-1, 1024, 128]               0\nTransformerEncoderLayer-54            [-1, 1024, 128]               0\n        LayerNorm-55            [-1, 1024, 128]             256\n           Linear-56            [-1, 1024, 384]          49,152\n           Conv2d-57          [-1, 4, 1024, 32]             132\n           Linear-58            [-1, 1024, 128]          16,512\n          Dropout-59            [-1, 1024, 128]               0\n NystromAttention-60            [-1, 1024, 128]               0\n         DropPath-61            [-1, 1024, 128]               0\n        LayerNorm-62            [-1, 1024, 128]             256\n           Linear-63            [-1, 1024, 512]          66,048\n          Dropout-64            [-1, 1024, 512]               0\n           Linear-65            [-1, 1024, 128]          65,664\n          Dropout-66            [-1, 1024, 128]               0\n         DropPath-67            [-1, 1024, 128]               0\nTransformerEncoderLayer-68            [-1, 1024, 128]               0\n        LayerNorm-69            [-1, 1024, 128]             256\n           Linear-70              [-1, 1024, 1]             129\n           Linear-71                  [-1, 200]          25,800\nTransformerClassifier-72                  [-1, 200]               0\n================================================================\nTotal params: 930,585\nTrainable params: 930,585\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.05\nForward/backward pass size (MB): 98.01\nParams size (MB): 3.55\nEstimated Total Size (MB): 101.61\n----------------------------------------------------------------\nNone\n_CudaDeviceProperties(name='Tesla P100-PCIE-16GB', major=6, minor=0, total_memory=16280MB, multi_processor_count=56)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-12T05:30:36.785569Z",
          "iopub.execute_input": "2021-08-12T05:30:36.785909Z",
          "iopub.status.idle": "2021-08-12T05:30:36.791034Z",
          "shell.execute_reply.started": "2021-08-12T05:30:36.785872Z",
          "shell.execute_reply": "2021-08-12T05:30:36.790278Z"
        },
        "trusted": true,
        "id": "2uGSCEgg1hVA"
      },
      "source": [
        "# model.load_state_dict(torch.load('../input/levin1/LeViN (1).pth'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYztkM0RYRfY",
        "execution": {
          "iopub.status.busy": "2021-08-12T05:30:36.793778Z",
          "iopub.execute_input": "2021-08-12T05:30:36.794033Z"
        },
        "trusted": true,
        "outputId": "a0119dbb-0a35-4543-c68b-c0a7b355d6ab"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "top1 = []\n",
        "top5 = []\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
        "for epoch in range(150):  # loop over the dataset multiple times\n",
        "    t0 = time.time()\n",
        "    epoch_accuracy = 0\n",
        "    epoch_loss = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            loss = criterion(outputs, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        acc = (outputs.argmax(dim=1) == labels).float().mean()\n",
        "        epoch_accuracy += acc / len(trainloader)\n",
        "        epoch_loss += loss / len(trainloader)\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    correct_1=0\n",
        "    correct_5=0\n",
        "    c = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(images)\n",
        "#         outputs = net(images)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            res = accuracy(outputs, labels)\n",
        "            correct_1 += res[0][0].float()\n",
        "            correct_5 += res[1][0].float()\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            c += 1\n",
        "        \n",
        "    print(f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - Top 1: {correct_1/c} - Top 5: {correct_5/c} - Time: {time.time() - t0}\\n\")\n",
        "    top1.append(correct_1/c)\n",
        "    top5.append(correct_5/c)\n",
        "    if float(correct_1/c) >= float(max(top1)):\n",
        "        PATH = 'CCN.pth'\n",
        "        torch.save(model.state_dict(), PATH)\n",
        "        print(1)\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[1,   200] loss: 0.499\n[1,   400] loss: 0.443\n[1,   600] loss: 0.403\nEpoch : 1 - loss : 4.4546 - acc: 0.0823 - Top 1: 15.56547737121582 - Top 5: 39.236114501953125 - Time: 433.0191881656647\n\n1\n[2,   200] loss: 0.371\n[2,   400] loss: 0.352\n[2,   600] loss: 0.340\nEpoch : 2 - loss : 3.5314 - acc: 0.2066 - Top 1: 24.513891220092773 - Top 5: 50.357147216796875 - Time: 433.3909797668457\n\n1\n[3,   200] loss: 0.319\n[3,   400] loss: 0.314\n[3,   600] loss: 0.305\nEpoch : 3 - loss : 3.1243 - acc: 0.2761 - Top 1: 29.583335876464844 - Top 5: 56.6170654296875 - Time: 434.14090156555176\n\n1\n[4,   200] loss: 0.287\n[4,   400] loss: 0.286\n[4,   600] loss: 0.280\nEpoch : 4 - loss : 2.8386 - acc: 0.3298 - Top 1: 32.92658996582031 - Top 5: 60.58531951904297 - Time: 434.6117172241211\n\n1\n[5,   200] loss: 0.263\n[5,   400] loss: 0.264\n[5,   600] loss: 0.260\nEpoch : 5 - loss : 2.6262 - acc: 0.3726 - Top 1: 34.900794982910156 - Top 5: 62.92658996582031 - Time: 434.37584710121155\n\n1\n[6,   200] loss: 0.242\n[6,   400] loss: 0.245\n[6,   600] loss: 0.244\nEpoch : 6 - loss : 2.4386 - acc: 0.4084 - Top 1: 36.865081787109375 - Top 5: 63.84920883178711 - Time: 435.07240533828735\n\n1\n[7,   200] loss: 0.223\n[7,   400] loss: 0.227\n[7,   600] loss: 0.235\nEpoch : 7 - loss : 2.2832 - acc: 0.4396 - Top 1: 38.14484405517578 - Top 5: 65.00992584228516 - Time: 433.60166597366333\n\n1\n[8,   200] loss: 0.209\n[8,   400] loss: 0.216\n[8,   600] loss: 0.218\nEpoch : 8 - loss : 2.1411 - acc: 0.4672 - Top 1: 39.60317611694336 - Top 5: 66.51786041259766 - Time: 434.5579209327698\n\n1\n[9,   200] loss: 0.191\n[9,   400] loss: 0.203\n[9,   600] loss: 0.206\nEpoch : 9 - loss : 1.9992 - acc: 0.4975 - Top 1: 40.218257904052734 - Top 5: 67.3214340209961 - Time: 433.50664949417114\n\n1\n[10,   200] loss: 0.181\n[10,   400] loss: 0.189\n[10,   600] loss: 0.195\nEpoch : 10 - loss : 1.8855 - acc: 0.5202 - Top 1: 39.60317611694336 - Top 5: 66.51786041259766 - Time: 434.36710000038147\n\n[11,   200] loss: 0.168\n[11,   400] loss: 0.179\n[11,   600] loss: 0.180\nEpoch : 11 - loss : 1.7606 - acc: 0.5483 - Top 1: 39.88095474243164 - Top 5: 67.10317993164062 - Time: 434.351176738739\n\n[12,   200] loss: 0.153\n[12,   400] loss: 0.169\n[12,   600] loss: 0.171\nEpoch : 12 - loss : 1.6472 - acc: 0.5704 - Top 1: 40.49603271484375 - Top 5: 66.92460632324219 - Time: 434.33116340637207\n\n1\n[13,   200] loss: 0.143\n[13,   400] loss: 0.156\n[13,   600] loss: 0.162\nEpoch : 13 - loss : 1.5405 - acc: 0.5940 - Top 1: 40.01984405517578 - Top 5: 67.2123031616211 - Time: 434.2463450431824\n\n[14,   200] loss: 0.133\n[14,   400] loss: 0.145\n[14,   600] loss: 0.152\nEpoch : 14 - loss : 1.4393 - acc: 0.6155 - Top 1: 40.80357360839844 - Top 5: 67.81746673583984 - Time: 434.3722906112671\n\n1\n[15,   200] loss: 0.122\n[15,   400] loss: 0.136\n[15,   600] loss: 0.144\nEpoch : 15 - loss : 1.3456 - acc: 0.6381 - Top 1: 39.80158996582031 - Top 5: 67.12301635742188 - Time: 434.11601972579956\n\n[16,   200] loss: 0.114\n[16,   400] loss: 0.124\n[16,   600] loss: 0.137\nEpoch : 16 - loss : 1.2573 - acc: 0.6575 - Top 1: 40.04960632324219 - Top 5: 66.25992584228516 - Time: 433.81041407585144\n\n[17,   200] loss: 0.106\n[17,   400] loss: 0.119\n[17,   600] loss: 0.127\nEpoch : 17 - loss : 1.1756 - acc: 0.6782 - Top 1: 39.47420883178711 - Top 5: 65.77381134033203 - Time: 433.7658624649048\n\n[18,   200] loss: 0.098\n[18,   400] loss: 0.111\n[18,   600] loss: 0.121\nEpoch : 18 - loss : 1.1002 - acc: 0.6951 - Top 1: 38.90873336791992 - Top 5: 66.05158996582031 - Time: 433.19923210144043\n\n[19,   200] loss: 0.092\n[19,   400] loss: 0.104\n[19,   600] loss: 0.113\nEpoch : 19 - loss : 1.0352 - acc: 0.7094 - Top 1: 39.46428680419922 - Top 5: 66.25000762939453 - Time: 433.86824893951416\n\n[20,   200] loss: 0.084\n[20,   400] loss: 0.097\n[20,   600] loss: 0.109\nEpoch : 20 - loss : 0.9712 - acc: 0.7276 - Top 1: 39.06746292114258 - Top 5: 66.04167175292969 - Time: 433.7987926006317\n\n[21,   200] loss: 0.079\n[21,   400] loss: 0.093\n[21,   600] loss: 0.099\nEpoch : 21 - loss : 0.9137 - acc: 0.7397 - Top 1: 38.27381134033203 - Top 5: 64.84127044677734 - Time: 434.1047978401184\n\n[22,   200] loss: 0.074\n[22,   400] loss: 0.086\n[22,   600] loss: 0.097\nEpoch : 22 - loss : 0.8618 - acc: 0.7533 - Top 1: 38.859130859375 - Top 5: 65.734130859375 - Time: 433.67574977874756\n\n[23,   200] loss: 0.071\n[23,   400] loss: 0.082\n[23,   600] loss: 0.093\nEpoch : 23 - loss : 0.8250 - acc: 0.7617 - Top 1: 38.958335876464844 - Top 5: 65.26786041259766 - Time: 433.9364171028137\n\n[24,   200] loss: 0.068\n[24,   400] loss: 0.078\n[24,   600] loss: 0.088\nEpoch : 24 - loss : 0.7873 - acc: 0.7727 - Top 1: 38.92857360839844 - Top 5: 65.17857360839844 - Time: 434.1347577571869\n\n[25,   200] loss: 0.063\n[25,   400] loss: 0.074\n[25,   600] loss: 0.084\nEpoch : 25 - loss : 0.7413 - acc: 0.7838 - Top 1: 38.244049072265625 - Top 5: 65.15873718261719 - Time: 434.2988336086273\n\n[26,   200] loss: 0.060\n[26,   400] loss: 0.074\n[26,   600] loss: 0.080\nEpoch : 26 - loss : 0.7173 - acc: 0.7917 - Top 1: 39.30555725097656 - Top 5: 64.8214340209961 - Time: 434.37314653396606\n\n[27,   200] loss: 0.058\n[27,   400] loss: 0.069\n[27,   600] loss: 0.081\nEpoch : 27 - loss : 0.6971 - acc: 0.7964 - Top 1: 38.68055725097656 - Top 5: 64.61309814453125 - Time: 434.6937801837921\n\n[28,   200] loss: 0.056\n[28,   400] loss: 0.064\n[28,   600] loss: 0.075\nEpoch : 28 - loss : 0.6585 - acc: 0.8076 - Top 1: 38.03571701049805 - Top 5: 64.0873031616211 - Time: 434.3981533050537\n\n[29,   200] loss: 0.055\n[29,   400] loss: 0.063\n[29,   600] loss: 0.072\nEpoch : 29 - loss : 0.6394 - acc: 0.8125 - Top 1: 38.08531951904297 - Top 5: 64.52381134033203 - Time: 433.97711157798767\n\n[30,   200] loss: 0.052\n[30,   400] loss: 0.061\n[30,   600] loss: 0.071\nEpoch : 30 - loss : 0.6204 - acc: 0.8171 - Top 1: 37.986114501953125 - Top 5: 64.33531951904297 - Time: 433.81508135795593\n\n[31,   200] loss: 0.051\n[31,   400] loss: 0.060\n[31,   600] loss: 0.068\nEpoch : 31 - loss : 0.6026 - acc: 0.8224 - Top 1: 38.125003814697266 - Top 5: 64.4543685913086 - Time: 434.6580123901367\n\n[32,   200] loss: 0.050\n[32,   400] loss: 0.058\n[32,   600] loss: 0.068\nEpoch : 32 - loss : 0.5908 - acc: 0.8262 - Top 1: 37.92658996582031 - Top 5: 63.839290618896484 - Time: 434.3936746120453\n\n[33,   200] loss: 0.049\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}