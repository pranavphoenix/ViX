{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "LeViP.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "id": "5bFsMrNWYRe5",
        "execution": {
          "iopub.status.busy": "2021-08-10T21:13:14.947893Z",
          "iopub.execute_input": "2021-08-10T21:13:14.948271Z",
          "iopub.status.idle": "2021-08-10T21:14:14.186082Z",
          "shell.execute_reply.started": "2021-08-10T21:13:14.948191Z",
          "shell.execute_reply": "2021-08-10T21:14:14.185047Z"
        },
        "trusted": true,
        "outputId": "c4ae8dd3-f227-4c70-c445-5fea5d164a8b"
      },
      "source": [
        "import torch, math\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torch.optim as optim\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "!pip install einops\n",
        "from math import ceil\n",
        "!pip install performer-pytorch\n",
        "import os, glob\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image \n",
        "from torch import nn, einsum\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "from einops import rearrange, reduce\n",
        "\n",
        "# helpers\n",
        "from einops import reduce\n",
        "\n",
        "batch_size = 192\n",
        "\n",
        "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "!unzip -q ./tiny-imagenet-200.zip\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "transform = transforms.Normalize((122.4786, 114.2755, 101.3963), (70.4924, 68.5679, 71.8127))\n",
        "\n",
        "id_dict = {}\n",
        "for i, line in enumerate(open('./tiny-imagenet-200/wnids.txt', 'r')):\n",
        "    id_dict[line.replace('\\n', '')] = i\n",
        "\n",
        "class TrainTinyImageNetDataset(Dataset):\n",
        "    def __init__(self, id, transform=None):\n",
        "        self.filenames = glob.glob(\"./tiny-imagenet-200/train/*/*/*.JPEG\")\n",
        "        self.transform = transform\n",
        "        self.id_dict = id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.filenames[idx]\n",
        "        image = read_image(img_path)\n",
        "        if image.shape[0] == 1:\n",
        "            image = torch.cat((image,image,image),0)\n",
        "        label = self.id_dict[img_path.split('/')[3]]\n",
        "        if self.transform:\n",
        "            image = self.transform(image.type(torch.FloatTensor))\n",
        "        return image, label\n",
        "    \n",
        "class TestTinyImageNetDataset(Dataset):\n",
        "    def __init__(self, id, transform=None):\n",
        "        self.filenames = glob.glob(\"./tiny-imagenet-200/val/images/*.JPEG\")\n",
        "        self.transform = transform\n",
        "        self.id_dict = id\n",
        "        self.cls_dic = {}\n",
        "        for i, line in enumerate(open('./tiny-imagenet-200/val/val_annotations.txt', 'r')):\n",
        "            a = line.split('\\t')\n",
        "            img, cls_id = a[0],a[1]\n",
        "            self.cls_dic[img] = self.id_dict[cls_id]\n",
        " \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.filenames[idx]\n",
        "        image = read_image(img_path)\n",
        "        if image.shape[0] == 1:\n",
        "            image = torch.cat((image,image,image),0)\n",
        "        label = self.cls_dic[img_path.split('/')[-1]]\n",
        "        if self.transform:\n",
        "            image = self.transform(image.type(torch.FloatTensor))\n",
        "        return image, label\n",
        "    \n",
        "trainset = TrainTinyImageNetDataset(id=id_dict, transform = transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = TestTinyImageNetDataset(id=id_dict, transform = transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,5)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\n",
        "    prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
        "    \"\"\"\n",
        "    maxk = max(topk)\n",
        "         # sizefunction: the number of total elements\n",
        "    batch_size = target.size(0) \n",
        " \n",
        "         # topk function selects the number of k before output\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "         ##########Do not understand t()k\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))   \n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchsummary\n",
            "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
            "Installing collected packages: torchsummary\n",
            "Successfully installed torchsummary-1.5.1\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: torchsummary in /opt/conda/lib/python3.7/site-packages (1.5.1)\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Collecting einops\n",
            "  Downloading einops-0.3.0-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.3.0\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Collecting performer-pytorch\n",
            "  Downloading performer_pytorch-1.0.11-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: einops>=0.3 in /opt/conda/lib/python3.7/site-packages (from performer-pytorch) (0.3.0)\n",
            "Collecting axial-positional-embedding>=0.1.0\n",
            "  Downloading axial_positional_embedding-0.2.1.tar.gz (2.6 kB)\n",
            "Collecting local-attention>=1.1.1\n",
            "  Downloading local_attention-1.4.3-py3-none-any.whl (5.0 kB)\n",
            "Requirement already satisfied: torch>=1.6 in /opt/conda/lib/python3.7/site-packages (from performer-pytorch) (1.7.0)\n",
            "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.6->performer-pytorch) (0.18.2)\n",
            "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.6->performer-pytorch) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch>=1.6->performer-pytorch) (0.6)\n",
            "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch>=1.6->performer-pytorch) (1.19.5)\n",
            "Building wheels for collected packages: axial-positional-embedding\n",
            "  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-py3-none-any.whl size=2902 sha256=5bf29864cff867ff2c47f2eb241ee6c55760a172377104c011f692030ac9df47\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/2c/c3/9a1cb267c0d0d9b6eeba7952addb32b17857d1f799690c27a8\n",
            "Successfully built axial-positional-embedding\n",
            "Installing collected packages: local-attention, axial-positional-embedding, performer-pytorch\n",
            "Successfully installed axial-positional-embedding-0.2.1 local-attention-1.4.3 performer-pytorch-1.0.11\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "--2021-08-10 21:13:48--  http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
            "Resolving cs231n.stanford.edu (cs231n.stanford.edu)... 171.64.68.10\n",
            "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248100043 (237M) [application/zip]\n",
            "Saving to: ‘tiny-imagenet-200.zip’\n",
            "\n",
            "tiny-imagenet-200.z 100%[===================>] 236.61M  13.3MB/s    in 17s     \n",
            "\n",
            "2021-08-10 21:14:05 (13.7 MB/s) - ‘tiny-imagenet-200.zip’ saved [248100043/248100043]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-10T21:14:14.189015Z",
          "iopub.execute_input": "2021-08-10T21:14:14.189364Z",
          "iopub.status.idle": "2021-08-10T21:14:14.246555Z",
          "shell.execute_reply.started": "2021-08-10T21:14:14.189332Z",
          "shell.execute_reply": "2021-08-10T21:14:14.245737Z"
        },
        "trusted": true,
        "id": "uIpEZrIlVPw-"
      },
      "source": [
        "from torch.cuda.amp import autocast\n",
        "from functools import partial\n",
        "from contextlib import contextmanager\n",
        "from local_attention import LocalAttention\n",
        "from axial_positional_embedding import AxialPositionalEmbedding\n",
        "from performer_pytorch.reversible import ReversibleSequence, SequentialSequence\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def empty(tensor):\n",
        "    return tensor.numel() == 0\n",
        "\n",
        "def default(val, d):\n",
        "    return val if exists(val) else d\n",
        "\n",
        "@contextmanager\n",
        "def null_context():\n",
        "    yield\n",
        "\n",
        "# def cast_tuple(val):\n",
        "#     return (val,) if not isinstance(val, tuple) else val\n",
        "\n",
        "def get_module_device(module):\n",
        "    return next(module.parameters()).device\n",
        "\n",
        "def find_modules(nn_module, type):\n",
        "    return [module for module in nn_module.modules() if isinstance(module, type)]\n",
        "\n",
        "class Always(nn.Module):\n",
        "    def __init__(self, val):\n",
        "        super().__init__()\n",
        "        self.val = val\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        return self.val\n",
        "\n",
        "# kernel functions\n",
        "\n",
        "# transcribed from jax to pytorch from\n",
        "# https://github.com/google-research/google-research/blob/master/performer/fast_attention/jax/fast_attention.py\n",
        "\n",
        "def softmax_kernel(data, *, projection_matrix, is_query, normalize_data=True, eps=1e-4, device = None):\n",
        "    b, h, *_ = data.shape\n",
        "\n",
        "    data_normalizer = (data.shape[-1] ** -0.25) if normalize_data else 1.\n",
        "\n",
        "    ratio = (projection_matrix.shape[0] ** -0.5)\n",
        "\n",
        "    projection = repeat(projection_matrix, 'j d -> b h j d', b = b, h = h)\n",
        "    projection = projection.type_as(data)\n",
        "\n",
        "    data_dash = torch.einsum('...id,...jd->...ij', (data_normalizer * data), projection)\n",
        "\n",
        "    diag_data = data ** 2\n",
        "    diag_data = torch.sum(diag_data, dim=-1)\n",
        "    diag_data = (diag_data / 2.0) * (data_normalizer ** 2)\n",
        "    diag_data = diag_data.unsqueeze(dim=-1)\n",
        "\n",
        "    if is_query:\n",
        "        data_dash = ratio * (\n",
        "            torch.exp(data_dash - diag_data -\n",
        "                    torch.max(data_dash, dim=-1, keepdim=True).values) + eps)\n",
        "    else:\n",
        "        data_dash = ratio * (\n",
        "            torch.exp(data_dash - diag_data - torch.max(data_dash)) + eps)\n",
        "\n",
        "    return data_dash.type_as(data)\n",
        "\n",
        "def generalized_kernel(data, *, projection_matrix, kernel_fn = nn.ReLU(), kernel_epsilon = 0.001, normalize_data = True, device = None):\n",
        "    b, h, *_ = data.shape\n",
        "\n",
        "    data_normalizer = (data.shape[-1] ** -0.25) if normalize_data else 1.\n",
        "\n",
        "    if projection_matrix is None:\n",
        "        return kernel_fn(data_normalizer * data) + kernel_epsilon\n",
        "\n",
        "    projection = repeat(projection_matrix, 'j d -> b h j d', b = b, h = h)\n",
        "    projection = projection.type_as(data)\n",
        "\n",
        "    data_dash = torch.einsum('...id,...jd->...ij', (data_normalizer * data), projection)\n",
        "\n",
        "    data_prime = kernel_fn(data_dash) + kernel_epsilon\n",
        "    return data_prime.type_as(data)\n",
        "\n",
        "def orthogonal_matrix_chunk(cols, device = None):\n",
        "    unstructured_block = torch.randn((cols, cols), device = device)\n",
        "    q, r = torch.qr(unstructured_block.cpu(), some = True)\n",
        "    q, r = map(lambda t: t.to(device), (q, r))\n",
        "    return q.t()\n",
        "\n",
        "def gaussian_orthogonal_random_matrix(nb_rows, nb_columns, scaling = 0, device = None):\n",
        "    nb_full_blocks = int(nb_rows / nb_columns)\n",
        "\n",
        "    block_list = []\n",
        "\n",
        "    for _ in range(nb_full_blocks):\n",
        "        q = orthogonal_matrix_chunk(nb_columns, device = device)\n",
        "        block_list.append(q)\n",
        "\n",
        "    remaining_rows = nb_rows - nb_full_blocks * nb_columns\n",
        "    if remaining_rows > 0:\n",
        "        q = orthogonal_matrix_chunk(nb_columns, device = device)\n",
        "        block_list.append(q[:remaining_rows])\n",
        "\n",
        "    final_matrix = torch.cat(block_list)\n",
        "\n",
        "    if scaling == 0:\n",
        "        multiplier = torch.randn((nb_rows, nb_columns), device = device).norm(dim = 1)\n",
        "    elif scaling == 1:\n",
        "        multiplier = math.sqrt((float(nb_columns))) * torch.ones((nb_rows,), device = device)\n",
        "    else:\n",
        "        raise ValueError(f'Invalid scaling {scaling}')\n",
        "\n",
        "    return torch.diag(multiplier) @ final_matrix\n",
        "\n",
        "# linear attention classes with softmax kernel\n",
        "\n",
        "# non-causal linear attention\n",
        "def linear_attention(q, k, v):\n",
        "    k_cumsum = k.sum(dim = -2)\n",
        "    D_inv = 1. / torch.einsum('...nd,...d->...n', q, k_cumsum.type_as(q))\n",
        "    context = torch.einsum('...nd,...ne->...de', k, v)\n",
        "    out = torch.einsum('...de,...nd,...n->...ne', context, q, D_inv)\n",
        "#     print(\"linear attention\", out.size)\n",
        "    return out\n",
        "\n",
        "class FastAttention(nn.Module):\n",
        "    def __init__(self, dim_heads, nb_features = None, ortho_scaling = 0, causal = False, generalized_attention = False, kernel_fn = nn.ReLU(), no_projection = False):\n",
        "        super().__init__()\n",
        "        nb_features = default(nb_features, int(dim_heads * math.log(dim_heads)))\n",
        "\n",
        "        self.dim_heads = dim_heads\n",
        "        self.nb_features = nb_features\n",
        "        self.ortho_scaling = ortho_scaling\n",
        "\n",
        "        self.create_projection = partial(gaussian_orthogonal_random_matrix, nb_rows = self.nb_features, nb_columns = dim_heads, scaling = ortho_scaling)\n",
        "        projection_matrix = self.create_projection()\n",
        "        self.register_buffer('projection_matrix', projection_matrix)\n",
        "\n",
        "        self.generalized_attention = generalized_attention\n",
        "        self.kernel_fn = kernel_fn\n",
        "\n",
        "        # if this is turned on, no projection will be used\n",
        "        # queries and keys will be softmax-ed as in the original efficient attention paper\n",
        "        self.no_projection = no_projection\n",
        "\n",
        "        self.causal = causal\n",
        "        \n",
        "    @torch.no_grad()\n",
        "    def redraw_projection_matrix(self, device):\n",
        "        projections = self.create_projection(device = device)\n",
        "        self.projection_matrix.copy_(projections)\n",
        "        del projections\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "        device = q.device\n",
        "        \n",
        "        if self.no_projection:\n",
        "            q = q.softmax(dim = -1)\n",
        "            k = torch.exp(k) if self.causal else k.softmax(dim = -2)\n",
        "\n",
        "        elif self.generalized_attention:\n",
        "            create_kernel = partial(generalized_kernel, kernel_fn = self.kernel_fn, projection_matrix = self.projection_matrix, device = device)\n",
        "            q, k = map(create_kernel, (q, k))\n",
        "\n",
        "        else:\n",
        "            create_kernel = partial(softmax_kernel, projection_matrix = self.projection_matrix, device = device)\n",
        "            q = create_kernel(q, is_query = True)\n",
        "            k = create_kernel(k, is_query = False)\n",
        "\n",
        "        attn_fn = linear_attention if not self.causal else self.causal_linear_fn\n",
        "        out = attn_fn(q, k, v)\n",
        "#         print('fastattention', out.size())\n",
        "        return out\n",
        "\n",
        "# a module for keeping track of when to update the projections\n",
        "\n",
        "class ProjectionUpdater(nn.Module):\n",
        "    def __init__(self, instance, feature_redraw_interval):\n",
        "        super().__init__()\n",
        "        self.instance = instance\n",
        "        self.feature_redraw_interval = feature_redraw_interval\n",
        "        self.register_buffer('calls_since_last_redraw', torch.tensor(0))\n",
        "\n",
        "    def fix_projections_(self):\n",
        "        self.feature_redraw_interval = None\n",
        "\n",
        "    def redraw_projections(self):\n",
        "        model = self.instance\n",
        "\n",
        "        if not self.training:\n",
        "            return\n",
        "\n",
        "        if exists(self.feature_redraw_interval) and self.calls_since_last_redraw >= self.feature_redraw_interval:\n",
        "            device = get_module_device(model)\n",
        "\n",
        "            fast_attentions = find_modules(model, FastAttention)\n",
        "            for fast_attention in fast_attentions:\n",
        "                fast_attention.redraw_projection_matrix(device)\n",
        "\n",
        "            self.calls_since_last_redraw.zero_()\n",
        "            return\n",
        "\n",
        "        self.calls_since_last_redraw += 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        raise NotImplemented\n",
        "\n",
        "# classes\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        causal = False,\n",
        "        heads = 4,\n",
        "        dim_head = 32,\n",
        "        local_heads = 0,\n",
        "        local_window_size = 256,\n",
        "        nb_features = None,\n",
        "        feature_redraw_interval = 1000,\n",
        "        generalized_attention = False,\n",
        "        kernel_fn = nn.ReLU(),\n",
        "        dropout = 0.,\n",
        "        no_projection = False,\n",
        "        qkv_bias = False,\n",
        "        attn_out_bias = True\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert dim % heads == 0, 'dimension must be divisible by number of heads'\n",
        "        dim_head = default(dim_head, dim // heads)\n",
        "        inner_dim = dim_head * heads\n",
        "        self.fast_attention = FastAttention(dim_head, nb_features, causal = causal, generalized_attention = generalized_attention, kernel_fn = kernel_fn, no_projection = no_projection)\n",
        "\n",
        "        self.heads = heads\n",
        "        self.global_heads = heads - local_heads\n",
        "        self.local_attn = LocalAttention(window_size = local_window_size, causal = causal, autopad = True, dropout = dropout, look_forward = int(not causal), rel_pos_emb_config = (dim_head, local_heads)) if local_heads > 0 else None\n",
        "\n",
        "        self.to_q = nn.Linear(dim, inner_dim, bias = qkv_bias)\n",
        "        self.to_k = nn.Linear(dim, inner_dim, bias = qkv_bias)\n",
        "        self.to_v = nn.Linear(dim, inner_dim, bias = qkv_bias)\n",
        "        self.to_out = nn.Linear(inner_dim, dim, bias = attn_out_bias)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.convert = nn.Sequential(\n",
        "            Rearrange('b (h w) (p1 p2 c) -> b c (h p1) (w p2)',h = 32, w = 32, p1 = 1, p2 = 1)\n",
        " \n",
        "        )\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = 1, p2 = 1)\n",
        " \n",
        "        )\n",
        "        \n",
        "\n",
        "    def forward(self, x, pos_emb = None, context = None, mask = None, context_mask = None, **kwargs):\n",
        "        x = self.to_patch_embedding(x)\n",
        "        b, n, _, h, gh = *x.shape, self.heads, self.global_heads\n",
        "\n",
        "        cross_attend = exists(context)\n",
        "\n",
        "        context = default(context, x)\n",
        "        context_mask = default(context_mask, mask) if not cross_attend else context_mask\n",
        "\n",
        "        q, k, v = self.to_q(x), self.to_k(context), self.to_v(context)\n",
        "\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n",
        "        (q, lq), (k, lk), (v, lv) = map(lambda t: (t[:, :gh], t[:, gh:]), (q, k, v))\n",
        "\n",
        "        attn_outs = []\n",
        "\n",
        "        if not empty(q):\n",
        "            if exists(context_mask):\n",
        "                global_mask = context_mask[:, None, :, None]\n",
        "                v.masked_fill_(~global_mask, 0.)\n",
        "\n",
        "            if exists(pos_emb) and not cross_attend:\n",
        "                q, k = apply_rotary_pos_emb(q, k, pos_emb)\n",
        "\n",
        "            out = self.fast_attention(q, k, v)\n",
        "            attn_outs.append(out)\n",
        "\n",
        "        if not empty(lq):\n",
        "            assert not cross_attend, 'local attention is not compatible with cross attention'\n",
        "            out = self.local_attn(lq, lk, lv, input_mask = mask)\n",
        "            attn_outs.append(out)\n",
        "\n",
        "        out = torch.cat(attn_outs, dim = 1)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "#         print(\"Attention\", out.size())\n",
        "        out =  self.to_out(out)\n",
        "        out = self.dropout(out)\n",
        "        return self.convert(out)\n",
        "\n",
        "\n",
        "class SelfAttention(Attention):\n",
        "    def forward(self, *args, context = None, **kwargs):\n",
        "        assert not exists(context), 'self attention should not receive context'\n",
        "#         print(1, \"self attention module\")\n",
        "        return super().forward(*args, **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-10T21:15:56.259209Z",
          "iopub.execute_input": "2021-08-10T21:15:56.259720Z",
          "iopub.status.idle": "2021-08-10T21:15:56.305930Z",
          "shell.execute_reply.started": "2021-08-10T21:15:56.259675Z",
          "shell.execute_reply": "2021-08-10T21:15:56.304732Z"
        },
        "trusted": true,
        "id": "QXjsEIXkVPxQ"
      },
      "source": [
        "from math import ceil\n",
        "\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "# helpers\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def default(val, d):\n",
        "    return val if exists(val) else d\n",
        "\n",
        "def cast_tuple(val, l = 3):\n",
        "    val = val if isinstance(val, tuple) else (val,)\n",
        "    return (*val, *((val[-1],) * max(l - len(val), 0)))\n",
        "\n",
        "def always(val):\n",
        "    return lambda *args, **kwargs: val\n",
        "\n",
        "# classes\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, mult, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(dim, dim * mult, 1),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Conv2d(dim * mult, dim, 1),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, fmap_size, depth, heads, dim_key, dim_value, mlp_mult = 2, dropout = 0., dim_out = None, downsample = False):\n",
        "        super().__init__()\n",
        "        dim_out = default(dim_out, dim)\n",
        "        self.layers = nn.ModuleList([])\n",
        "        self.attn_residual = (not downsample) and dim == dim_out\n",
        "        local_attn_heads = 0\n",
        "        dim_head = 32\n",
        "        local_window_size = 256\n",
        "        causal = False\n",
        "        nb_features = None\n",
        "        generalized_attention = False\n",
        "        kernel_fn = nn.ReLU()\n",
        "        attn_dropout = 0.\n",
        "        no_projection = False\n",
        "        qkv_bias = True\n",
        "        attn_out_bias = True\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                SelfAttention(dim, causal = causal, heads = heads, dim_head = dim_head, local_heads = local_attn_heads, local_window_size = local_window_size, nb_features = nb_features, generalized_attention = generalized_attention, kernel_fn = kernel_fn, dropout = attn_dropout, no_projection = no_projection, qkv_bias = qkv_bias, attn_out_bias = attn_out_bias),\n",
        "                FeedForward(dim_out, mlp_mult, dropout = dropout)\n",
        "            ]))\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            attn_res = (x if self.attn_residual else 0)\n",
        "            x = attn(x) + attn_res\n",
        "            x = ff(x) + x\n",
        "        return x\n",
        "\n",
        "class LeViT(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        image_size,\n",
        "        num_classes,\n",
        "        dim,\n",
        "        depth,\n",
        "        heads,\n",
        "        mlp_mult,\n",
        "        stages = 3,\n",
        "        dim_key = 32,\n",
        "        dim_value = 32,\n",
        "        dropout = 0.,\n",
        "        num_distill_classes = None\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        dims = cast_tuple(dim, stages)\n",
        "        depths = cast_tuple(depth, stages)\n",
        "        layer_heads = cast_tuple(heads, stages)\n",
        "\n",
        "        assert all(map(lambda t: len(t) == stages, (dims, depths, layer_heads))), 'dimensions, depths, and heads must be a tuple that is less than the designated number of stages'\n",
        "\n",
        "        self.conv_embedding = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, stride = 2, padding = 1),\n",
        "            nn.Conv2d(32, 64, 3, stride = 1, padding = 1),\n",
        "            nn.Conv2d(64, dims[0], 3, stride = 1, padding = 1),\n",
        "        )\n",
        "\n",
        "        fmap_size = image_size \n",
        "        layers = []\n",
        "\n",
        "        for ind, dim, depth, heads in zip(range(stages), dims, depths, layer_heads):\n",
        "            is_last = ind == (stages - 1)\n",
        "            layers.append(Transformer(dim, fmap_size, depth, heads, dim_key, dim_value, mlp_mult, dropout))\n",
        "\n",
        "            if not is_last:\n",
        "                next_dim = dims[ind + 1]\n",
        "                layers.append(Transformer(dim, fmap_size, 1, heads * 2, dim_key, dim_value, dim_out = next_dim, downsample = True))\n",
        "                fmap_size = ceil(fmap_size / 2)\n",
        "\n",
        "        self.backbone = nn.Sequential(*layers)\n",
        "\n",
        "        self.pool = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            Rearrange('... () () -> ...')\n",
        "        )\n",
        "\n",
        "        self.distill_head = nn.Linear(dim, num_distill_classes) if exists(num_distill_classes) else always(None)\n",
        "        self.mlp_head = nn.Linear(dim, num_classes)\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.conv_embedding(img)\n",
        "\n",
        "        x = self.backbone(x)        \n",
        "\n",
        "        x = self.pool(x)\n",
        "\n",
        "        out = self.mlp_head(x)\n",
        "        distill = self.distill_head(x)\n",
        "\n",
        "        if exists(distill):\n",
        "            return out, distill\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je07f7q7YRfV",
        "execution": {
          "iopub.status.busy": "2021-08-10T21:16:00.093935Z",
          "iopub.execute_input": "2021-08-10T21:16:00.094282Z",
          "iopub.status.idle": "2021-08-10T21:16:05.634978Z",
          "shell.execute_reply.started": "2021-08-10T21:16:00.094240Z",
          "shell.execute_reply": "2021-08-10T21:16:05.634038Z"
        },
        "trusted": true,
        "outputId": "086910a6-423e-498d-d26d-f602ae50aba7"
      },
      "source": [
        "model = LeViT(\n",
        "    image_size = 32,\n",
        "    num_classes = 200,\n",
        "    stages = 1,             # number of stages\n",
        "    dim = (128),  # dimensions at each stage\n",
        "    depth = 4,              # transformer of depth 4 at each stage\n",
        "    heads = (4),      # heads at each stage\n",
        "    mlp_mult = 2,\n",
        "    dropout = 0.,\n",
        ")\n",
        "\n",
        "model.to(device)\n",
        "print(summary(model, (3,64,64)))\n",
        "print(torch.cuda.get_device_properties(device))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "            Conv2d-2           [-1, 64, 32, 32]          18,496\n",
            "            Conv2d-3          [-1, 128, 32, 32]          73,856\n",
            "         Rearrange-4            [-1, 1024, 128]               0\n",
            "            Linear-5            [-1, 1024, 128]          16,512\n",
            "            Linear-6            [-1, 1024, 128]          16,512\n",
            "            Linear-7            [-1, 1024, 128]          16,512\n",
            "     FastAttention-8          [-1, 4, 1024, 32]               0\n",
            "            Linear-9            [-1, 1024, 128]          16,512\n",
            "          Dropout-10            [-1, 1024, 128]               0\n",
            "        Rearrange-11          [-1, 128, 32, 32]               0\n",
            "    SelfAttention-12          [-1, 128, 32, 32]               0\n",
            "           Conv2d-13          [-1, 256, 32, 32]          33,024\n",
            "             GELU-14          [-1, 256, 32, 32]               0\n",
            "          Dropout-15          [-1, 256, 32, 32]               0\n",
            "           Conv2d-16          [-1, 128, 32, 32]          32,896\n",
            "          Dropout-17          [-1, 128, 32, 32]               0\n",
            "      FeedForward-18          [-1, 128, 32, 32]               0\n",
            "        Rearrange-19            [-1, 1024, 128]               0\n",
            "           Linear-20            [-1, 1024, 128]          16,512\n",
            "           Linear-21            [-1, 1024, 128]          16,512\n",
            "           Linear-22            [-1, 1024, 128]          16,512\n",
            "    FastAttention-23          [-1, 4, 1024, 32]               0\n",
            "           Linear-24            [-1, 1024, 128]          16,512\n",
            "          Dropout-25            [-1, 1024, 128]               0\n",
            "        Rearrange-26          [-1, 128, 32, 32]               0\n",
            "    SelfAttention-27          [-1, 128, 32, 32]               0\n",
            "           Conv2d-28          [-1, 256, 32, 32]          33,024\n",
            "             GELU-29          [-1, 256, 32, 32]               0\n",
            "          Dropout-30          [-1, 256, 32, 32]               0\n",
            "           Conv2d-31          [-1, 128, 32, 32]          32,896\n",
            "          Dropout-32          [-1, 128, 32, 32]               0\n",
            "      FeedForward-33          [-1, 128, 32, 32]               0\n",
            "        Rearrange-34            [-1, 1024, 128]               0\n",
            "           Linear-35            [-1, 1024, 128]          16,512\n",
            "           Linear-36            [-1, 1024, 128]          16,512\n",
            "           Linear-37            [-1, 1024, 128]          16,512\n",
            "    FastAttention-38          [-1, 4, 1024, 32]               0\n",
            "           Linear-39            [-1, 1024, 128]          16,512\n",
            "          Dropout-40            [-1, 1024, 128]               0\n",
            "        Rearrange-41          [-1, 128, 32, 32]               0\n",
            "    SelfAttention-42          [-1, 128, 32, 32]               0\n",
            "           Conv2d-43          [-1, 256, 32, 32]          33,024\n",
            "             GELU-44          [-1, 256, 32, 32]               0\n",
            "          Dropout-45          [-1, 256, 32, 32]               0\n",
            "           Conv2d-46          [-1, 128, 32, 32]          32,896\n",
            "          Dropout-47          [-1, 128, 32, 32]               0\n",
            "      FeedForward-48          [-1, 128, 32, 32]               0\n",
            "        Rearrange-49            [-1, 1024, 128]               0\n",
            "           Linear-50            [-1, 1024, 128]          16,512\n",
            "           Linear-51            [-1, 1024, 128]          16,512\n",
            "           Linear-52            [-1, 1024, 128]          16,512\n",
            "    FastAttention-53          [-1, 4, 1024, 32]               0\n",
            "           Linear-54            [-1, 1024, 128]          16,512\n",
            "          Dropout-55            [-1, 1024, 128]               0\n",
            "        Rearrange-56          [-1, 128, 32, 32]               0\n",
            "    SelfAttention-57          [-1, 128, 32, 32]               0\n",
            "           Conv2d-58          [-1, 256, 32, 32]          33,024\n",
            "             GELU-59          [-1, 256, 32, 32]               0\n",
            "          Dropout-60          [-1, 256, 32, 32]               0\n",
            "           Conv2d-61          [-1, 128, 32, 32]          32,896\n",
            "          Dropout-62          [-1, 128, 32, 32]               0\n",
            "      FeedForward-63          [-1, 128, 32, 32]               0\n",
            "      Transformer-64          [-1, 128, 32, 32]               0\n",
            "AdaptiveAvgPool2d-65            [-1, 128, 1, 1]               0\n",
            "        Rearrange-66                  [-1, 128]               0\n",
            "           Linear-67                  [-1, 200]          25,800\n",
            "================================================================\n",
            "Total params: 646,920\n",
            "Trainable params: 646,920\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.05\n",
            "Forward/backward pass size (MB): 74.75\n",
            "Params size (MB): 2.47\n",
            "Estimated Total Size (MB): 77.27\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "_CudaDeviceProperties(name='Tesla P100-PCIE-16GB', major=6, minor=0, total_memory=16280MB, multi_processor_count=56)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-10T21:14:14.411267Z",
          "iopub.status.idle": "2021-08-10T21:14:14.411652Z"
        },
        "trusted": true,
        "id": "PewJRsdrVPxV"
      },
      "source": [
        "# model.load_state_dict(torch.load('../input/levin1/LeViN (1).pth'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYztkM0RYRfY",
        "execution": {
          "iopub.status.busy": "2021-08-10T21:16:12.609134Z",
          "iopub.execute_input": "2021-08-10T21:16:12.609512Z"
        },
        "trusted": true,
        "outputId": "a0119dbb-0a35-4543-c68b-c0a7b355d6ab"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "top1 = []\n",
        "top5 = []\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
        "for epoch in range(150):  # loop over the dataset multiple times\n",
        "    t0 = time.time()\n",
        "    epoch_accuracy = 0\n",
        "    epoch_loss = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            loss = criterion(outputs, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        acc = (outputs.argmax(dim=1) == labels).float().mean()\n",
        "        epoch_accuracy += acc / len(trainloader)\n",
        "        epoch_loss += loss / len(trainloader)\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    correct_1=0\n",
        "    correct_5=0\n",
        "    c = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(images)\n",
        "#         outputs = net(images)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            res = accuracy(outputs, labels)\n",
        "            correct_1 += res[0][0].float()\n",
        "            correct_5 += res[1][0].float()\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            c += 1\n",
        "        \n",
        "    print(f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - Top 1: {correct_1/c} - Top 5: {correct_5/c} - Time: {time.time() - t0}\\n\")\n",
        "    top1.append(correct_1/c)\n",
        "    top5.append(correct_5/c)\n",
        "    if float(correct_1/c) >= float(max(top1)):\n",
        "        PATH = 'LeViP.pth'\n",
        "        torch.save(model.state_dict(), PATH)\n",
        "        print(1)\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   200] loss: 0.505\n",
            "[1,   400] loss: 0.481\n",
            "Epoch : 1 - loss : 4.8460 - acc: 0.0419 - Top 1: 7.46855354309082 - Top 5: 23.103382110595703 - Time: 400.36791157722473\n",
            "\n",
            "1\n",
            "[2,   200] loss: 0.432\n",
            "[2,   400] loss: 0.410\n",
            "Epoch : 2 - loss : 4.1510 - acc: 0.1151 - Top 1: 14.789700508117676 - Top 5: 37.21501159667969 - Time: 400.57597637176514\n",
            "\n",
            "1\n",
            "[3,   200] loss: 0.384\n",
            "[3,   400] loss: 0.371\n",
            "Epoch : 3 - loss : 3.7418 - acc: 0.1757 - Top 1: 20.46973419189453 - Top 5: 45.07664108276367 - Time: 400.8025758266449\n",
            "\n",
            "1\n",
            "[4,   200] loss: 0.354\n",
            "[4,   400] loss: 0.349\n",
            "Epoch : 4 - loss : 3.4972 - acc: 0.2162 - Top 1: 22.84787940979004 - Top 5: 48.21147537231445 - Time: 400.84987807273865\n",
            "\n",
            "1\n",
            "[5,   200] loss: 0.337\n",
            "[5,   400] loss: 0.331\n",
            "Epoch : 5 - loss : 3.3208 - acc: 0.2459 - Top 1: 24.852596282958984 - Top 5: 51.7000732421875 - Time: 400.9460246562958\n",
            "\n",
            "1\n",
            "[6,   200] loss: 0.320\n",
            "[6,   400] loss: 0.317\n",
            "Epoch : 6 - loss : 3.1791 - acc: 0.2709 - Top 1: 26.76886749267578 - Top 5: 53.78340530395508 - Time: 401.09588527679443\n",
            "\n",
            "1\n",
            "[7,   200] loss: 0.307\n",
            "[7,   400] loss: 0.306\n",
            "Epoch : 7 - loss : 3.0585 - acc: 0.2920 - Top 1: 28.87185287475586 - Top 5: 55.92570877075195 - Time: 400.8051028251648\n",
            "\n",
            "1\n",
            "[8,   200] loss: 0.295\n",
            "[8,   400] loss: 0.295\n",
            "Epoch : 8 - loss : 2.9592 - acc: 0.3096 - Top 1: 30.434354782104492 - Top 5: 57.7535400390625 - Time: 400.66990208625793\n",
            "\n",
            "1\n",
            "[9,   200] loss: 0.286\n",
            "[9,   400] loss: 0.288\n",
            "Epoch : 9 - loss : 2.8710 - acc: 0.3254 - Top 1: 30.99449348449707 - Top 5: 58.579010009765625 - Time: 401.13301038742065\n",
            "\n",
            "1\n",
            "[10,   200] loss: 0.280\n",
            "[10,   400] loss: 0.281\n",
            "Epoch : 10 - loss : 2.7956 - acc: 0.3406 - Top 1: 33.25471496582031 - Top 5: 60.66234588623047 - Time: 400.74020528793335\n",
            "\n",
            "1\n",
            "[11,   200] loss: 0.271\n",
            "[11,   400] loss: 0.273\n",
            "Epoch : 11 - loss : 2.7214 - acc: 0.3541 - Top 1: 32.665096282958984 - Top 5: 60.976806640625 - Time: 400.74107360839844\n",
            "\n",
            "[12,   200] loss: 0.264\n",
            "[12,   400] loss: 0.265\n",
            "Epoch : 12 - loss : 2.6528 - acc: 0.3674 - Top 1: 33.69693374633789 - Top 5: 62.00864791870117 - Time: 400.7685327529907\n",
            "\n",
            "1\n",
            "[13,   200] loss: 0.259\n",
            "[13,   400] loss: 0.260\n",
            "Epoch : 13 - loss : 2.5991 - acc: 0.3773 - Top 1: 34.99410629272461 - Top 5: 62.067604064941406 - Time: 401.061963558197\n",
            "\n",
            "1\n",
            "[14,   200] loss: 0.252\n",
            "[14,   400] loss: 0.255\n",
            "Epoch : 14 - loss : 2.5379 - acc: 0.3893 - Top 1: 35.74095916748047 - Top 5: 63.404090881347656 - Time: 400.96408438682556\n",
            "\n",
            "1\n",
            "[15,   200] loss: 0.247\n",
            "[15,   400] loss: 0.250\n",
            "Epoch : 15 - loss : 2.4932 - acc: 0.3976 - Top 1: 36.20283126831055 - Top 5: 64.16077423095703 - Time: 401.0418653488159\n",
            "\n",
            "1\n",
            "[16,   200] loss: 0.242\n",
            "[16,   400] loss: 0.245\n",
            "Epoch : 16 - loss : 2.4422 - acc: 0.4072 - Top 1: 36.88089370727539 - Top 5: 64.4948959350586 - Time: 400.74107217788696\n",
            "\n",
            "1\n",
            "[17,   200] loss: 0.238\n",
            "[17,   400] loss: 0.241\n",
            "Epoch : 17 - loss : 2.3963 - acc: 0.4175 - Top 1: 37.05778503417969 - Top 5: 64.62265014648438 - Time: 401.076518535614\n",
            "\n",
            "1\n",
            "[18,   200] loss: 0.235\n",
            "[18,   400] loss: 0.237\n",
            "Epoch : 18 - loss : 2.3576 - acc: 0.4234 - Top 1: 37.8144645690918 - Top 5: 65.21227264404297 - Time: 400.9083776473999\n",
            "\n",
            "1\n",
            "[19,   200] loss: 0.229\n",
            "[19,   400] loss: 0.232\n",
            "Epoch : 19 - loss : 2.3150 - acc: 0.4324 - Top 1: 37.92256546020508 - Top 5: 65.29088592529297 - Time: 400.9578824043274\n",
            "\n",
            "1\n",
            "[20,   200] loss: 0.225\n",
            "[20,   400] loss: 0.229\n",
            "Epoch : 20 - loss : 2.2774 - acc: 0.4398 - Top 1: 38.276336669921875 - Top 5: 65.1926040649414 - Time: 401.13170862197876\n",
            "\n",
            "1\n",
            "[21,   200] loss: 0.222\n",
            "[21,   400] loss: 0.224\n",
            "Epoch : 21 - loss : 2.2389 - acc: 0.4465 - Top 1: 38.61045837402344 - Top 5: 66.4013442993164 - Time: 400.98894715309143\n",
            "\n",
            "1\n",
            "[22,   200] loss: 0.219\n",
            "[22,   400] loss: 0.221\n",
            "Epoch : 22 - loss : 2.2106 - acc: 0.4540 - Top 1: 38.87578201293945 - Top 5: 66.21461486816406 - Time: 401.3410849571228\n",
            "\n",
            "1\n",
            "[23,   200] loss: 0.213\n",
            "[23,   400] loss: 0.220\n",
            "Epoch : 23 - loss : 2.1743 - acc: 0.4614 - Top 1: 38.39426803588867 - Top 5: 66.30307006835938 - Time: 401.1954731941223\n",
            "\n",
            "[24,   200] loss: 0.209\n",
            "[24,   400] loss: 0.216\n",
            "Epoch : 24 - loss : 2.1304 - acc: 0.4691 - Top 1: 39.00353240966797 - Top 5: 67.10887908935547 - Time: 401.2396273612976\n",
            "\n",
            "1\n",
            "[25,   200] loss: 0.206\n",
            "[25,   400] loss: 0.213\n",
            "Epoch : 25 - loss : 2.1054 - acc: 0.4754 - Top 1: 39.80935287475586 - Top 5: 66.912353515625 - Time: 400.9893853664398\n",
            "\n",
            "1\n",
            "[26,   200] loss: 0.206\n",
            "[26,   400] loss: 0.208\n",
            "Epoch : 26 - loss : 2.0770 - acc: 0.4790 - Top 1: 39.40644836425781 - Top 5: 66.48978424072266 - Time: 401.0688464641571\n",
            "\n",
            "[27,   200] loss: 0.202\n",
            "[27,   400] loss: 0.206\n",
            "Epoch : 27 - loss : 2.0462 - acc: 0.4865 - Top 1: 39.11164093017578 - Top 5: 67.26612091064453 - Time: 400.9890775680542\n",
            "\n",
            "[28,   200] loss: 0.198\n",
            "[28,   400] loss: 0.202\n",
            "Epoch : 28 - loss : 2.0130 - acc: 0.4928 - Top 1: 39.897796630859375 - Top 5: 67.62971496582031 - Time: 401.0259735584259\n",
            "\n",
            "1\n",
            "[29,   200] loss: 0.194\n",
            "[29,   400] loss: 0.201\n",
            "Epoch : 29 - loss : 1.9838 - acc: 0.4998 - Top 1: 39.48506546020508 - Top 5: 66.90251159667969 - Time: 401.20905351638794\n",
            "\n",
            "[30,   200] loss: 0.192\n",
            "[30,   400] loss: 0.198\n",
            "Epoch : 30 - loss : 1.9589 - acc: 0.5039 - Top 1: 39.69142532348633 - Top 5: 67.30542755126953 - Time: 400.8744423389435\n",
            "\n",
            "[31,   200] loss: 0.188\n",
            "[31,   400] loss: 0.195\n",
            "Epoch : 31 - loss : 1.9293 - acc: 0.5106 - Top 1: 40.63482666015625 - Top 5: 67.78695678710938 - Time: 401.1708674430847\n",
            "\n",
            "1\n",
            "[32,   200] loss: 0.185\n",
            "[32,   400] loss: 0.193\n",
            "Epoch : 32 - loss : 1.9052 - acc: 0.5168 - Top 1: 40.25157928466797 - Top 5: 67.00078582763672 - Time: 401.183390378952\n",
            "\n",
            "[33,   200] loss: 0.183\n",
            "[33,   400] loss: 0.189\n",
            "Epoch : 33 - loss : 1.8749 - acc: 0.5223 - Top 1: 40.497249603271484 - Top 5: 68.28813171386719 - Time: 401.38354110717773\n",
            "\n",
            "[34,   200] loss: 0.181\n",
            "[34,   400] loss: 0.187\n",
            "Epoch : 34 - loss : 1.8578 - acc: 0.5271 - Top 1: 41.04755783081055 - Top 5: 68.26847839355469 - Time: 401.0337061882019\n",
            "\n",
            "1\n",
            "[35,   200] loss: 0.178\n",
            "[35,   400] loss: 0.185\n",
            "Epoch : 35 - loss : 1.8290 - acc: 0.5314 - Top 1: 39.86832046508789 - Top 5: 67.62972259521484 - Time: 401.10456919670105\n",
            "\n",
            "[36,   200] loss: 0.175\n",
            "[36,   400] loss: 0.183\n",
            "Epoch : 36 - loss : 1.8064 - acc: 0.5347 - Top 1: 40.7429313659668 - Top 5: 67.7967758178711 - Time: 401.22045063972473\n",
            "\n",
            "[37,   200] loss: 0.173\n",
            "[37,   400] loss: 0.181\n",
            "Epoch : 37 - loss : 1.7846 - acc: 0.5408 - Top 1: 40.064857482910156 - Top 5: 67.97366333007812 - Time: 401.30860567092896\n",
            "\n",
            "[38,   200] loss: 0.172\n",
            "[38,   400] loss: 0.177\n",
            "Epoch : 38 - loss : 1.7612 - acc: 0.5436 - Top 1: 39.868316650390625 - Top 5: 67.09905242919922 - Time: 401.1621446609497\n",
            "\n",
            "[39,   200] loss: 0.168\n",
            "[39,   400] loss: 0.176\n",
            "Epoch : 39 - loss : 1.7358 - acc: 0.5508 - Top 1: 40.34001541137695 - Top 5: 67.53144073486328 - Time: 401.3753068447113\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
